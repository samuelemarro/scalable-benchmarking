[
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "Let $P(x)$ be the predicate \"$x$ is a prime number\" and $Q(x, y)$ be the predicate \"$y$ is the smallest prime greater than $x$.\" For all natural numbers $n$, consider the statement: \"For every natural number $n$, there exists a prime $p$ such that $n < p < 2n$.\" Formalize this statement using the given predicates $P(x)$ and $Q(x, y)$, and determine whether it is true or false. If true, provide a proof; if false, provide a counterexample.",
    "run_id": "1",
    "outer_attempt": 1,
    "topic_slug": "mathematical_logic_and_foundations",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "To solve the problem, we will first formalize the statement using the provided predicates and then determine its truth value by analyzing the properties of natural numbers and prime numbers.\n\n### 1. Formalization of the Statement\n\nWe are given the following:\n*   **Domain**: The set of natural numbers, denoted as $\\mathbb{N}$. Standard definitions of $\\mathbb{N}$ are either $\\{0, 1, 2, \\dots\\}$ or $\\{1, 2, 3, \\dots\\}$.\n*   **Predicate** $P(x)$: \"$x$ is a prime number.\"\n*   **Predicate** $Q(x, y)$: \"$y$ is the smallest prime greater than $x$.\"\n*   **Statement**: \"For every natural number $n$, there exists a prime $p$ such that $n < p < 2n$.\"\n\nWe need to formalize the existence of a prime strictly between $n$ and $2n$. Note that stating \"there exists a prime $p$ such that $n < p < 2n$\" is logically equivalent to stating \"the smallest prime greater than $n$ is strictly less than $2n$.\"\n\n**Reasoning for equivalence:**\n1.  **Forward:** If there exists a prime $p$ such that $n < p < 2n$, let $q$ be the smallest prime greater than $n$. By definition, $n < q \\le p$. Since $p < 2n$, it follows that $n < q < 2n$. Thus, the smallest prime greater than $n$ is strictly less than $2n$.\n2.  **Reverse:** If $y$ is the smallest prime greater than $n$ (i.e., $Q(n, y)$ is true) and $y < 2n$, then $y$ is a prime satisfying $n < y < 2n$.\n\nTherefore, we can formalize the statement compactly using $Q(n, y)$ to represent the prime conditions:\n\n$$ \\forall n \\in \\mathbb{N}, \\exists y \\in \\mathbb{N}, (Q(n, y) \\land y < 2n) $$\n\nAlternatively, using only the basic predicate $P(x)$ inside the quantifier structure explicitly mentioned in the text:\n$$ \\forall n \\in \\mathbb{N}, \\exists p \\in \\mathbb{N}, (P(p) \\land n < p < 2n) $$\n\nGiven the instruction to use the provided predicates (specifically including $Q$), the first formalization is the most precise utilization of the definitions provided.\n\n### 2. Determination of Truth Value\n\nTo determine if the statement is true or false, we examine the statement for specific natural numbers. The statement claims the property holds for **every** natural number $n$.\n\n**Case Analysis:**\n*   **If $n = 1$:**\n    The inequality becomes $1 < p < 2(1)$, which simplifies to $1 < p < 2$.\n    There are no integers strictly between 1 and 2, and consequently, no prime numbers.\n    The smallest prime greater than 1 is 2. Testing the condition $y < 2n$:\n    $$2 < 2(1) \\implies 2 < 2$$\n    This inequality is false.\n\n*   **If $n = 2$:**\n    The inequality is $2 < p < 4$. The prime $p=3$ satisfies this. (True)\n\n*   **If $n = 3$:**\n    The inequality is $3 < p < 6$. The prime $p=5$ satisfies this. (True)\n\n*   **If $n = 0$ (if included in $\\mathbb{N}$):**\n    The inequality is $0 < p < 0$, which is impossible.\n\nSince the statement must hold for **all** natural numbers, a single counterexample is sufficient to disprove it. The case $n=1$ serves as this counterexample.\n\n**Note on Mathematical Context:**\nFor $n > 1$, the statement is true. This is a famous result in number theory known as **Bertrand's Postulate** (or the Bertrand-Chebyshev Theorem), which states that for any integer $n > 1$, there always exists at least one prime $p$ such that $n < p < 2n$. However, because the question specifies \"For every natural number $n$\" and standard definitions of $\\mathbb{N}$ include $1$, the statement is strictly false as written.\n\n### 3. Conclusion and Proof\n\n**Truth Value:** **False**\n\n**Counterexample:**\nLet $n = 1$. The statement asserts the existence of a prime $p$ such that $1 < p < 2$. Since there are no integers between 1 and 2, there is no such prime. Thus, the condition fails for $n=1$.\n\n**Final Formalized Answer:**\nThe formalized statement is:\n$$ \\forall n \\in \\mathbb{N}, \\exists y \\in \\mathbb{N}, (Q(n, y) \\land y < 2n) $$\nThe statement is **false** because for $n=1$, the smallest prime greater than 1 is 2, and $2$ is not strictly less than $2(1)$.",
        "raw_answer": "To solve the problem, we will first formalize the statement using the provided predicates and then determine its truth value by analyzing the properties of natural numbers and prime numbers.\n\n### 1. Formalization of the Statement\n\nWe are given the following:\n*   **Domain**: The set of natural numbers, denoted as $\\mathbb{N}$. Standard definitions of $\\mathbb{N}$ are either $\\{0, 1, 2, \\dots\\}$ or $\\{1, 2, 3, \\dots\\}$.\n*   **Predicate** $P(x)$: \"$x$ is a prime number.\"\n*   **Predicate** $Q(x, y)$: \"$y$ is the smallest prime greater than $x$.\"\n*   **Statement**: \"For every natural number $n$, there exists a prime $p$ such that $n < p < 2n$.\"\n\nWe need to formalize the existence of a prime strictly between $n$ and $2n$. Note that stating \"there exists a prime $p$ such that $n < p < 2n$\" is logically equivalent to stating \"the smallest prime greater than $n$ is strictly less than $2n$.\"\n\n**Reasoning for equivalence:**\n1.  **Forward:** If there exists a prime $p$ such that $n < p < 2n$, let $q$ be the smallest prime greater than $n$. By definition, $n < q \\le p$. Since $p < 2n$, it follows that $n < q < 2n$. Thus, the smallest prime greater than $n$ is strictly less than $2n$.\n2.  **Reverse:** If $y$ is the smallest prime greater than $n$ (i.e., $Q(n, y)$ is true) and $y < 2n$, then $y$ is a prime satisfying $n < y < 2n$.\n\nTherefore, we can formalize the statement compactly using $Q(n, y)$ to represent the prime conditions:\n\n$$ \\forall n \\in \\mathbb{N}, \\exists y \\in \\mathbb{N}, (Q(n, y) \\land y < 2n) $$\n\nAlternatively, using only the basic predicate $P(x)$ inside the quantifier structure explicitly mentioned in the text:\n$$ \\forall n \\in \\mathbb{N}, \\exists p \\in \\mathbb{N}, (P(p) \\land n < p < 2n) $$\n\nGiven the instruction to use the provided predicates (specifically including $Q$), the first formalization is the most precise utilization of the definitions provided.\n\n### 2. Determination of Truth Value\n\nTo determine if the statement is true or false, we examine the statement for specific natural numbers. The statement claims the property holds for **every** natural number $n$.\n\n**Case Analysis:**\n*   **If $n = 1$:**\n    The inequality becomes $1 < p < 2(1)$, which simplifies to $1 < p < 2$.\n    There are no integers strictly between 1 and 2, and consequently, no prime numbers.\n    The smallest prime greater than 1 is 2. Testing the condition $y < 2n$:\n    $$2 < 2(1) \\implies 2 < 2$$\n    This inequality is false.\n\n*   **If $n = 2$:**\n    The inequality is $2 < p < 4$. The prime $p=3$ satisfies this. (True)\n\n*   **If $n = 3$:**\n    The inequality is $3 < p < 6$. The prime $p=5$ satisfies this. (True)\n\n*   **If $n = 0$ (if included in $\\mathbb{N}$):**\n    The inequality is $0 < p < 0$, which is impossible.\n\nSince the statement must hold for **all** natural numbers, a single counterexample is sufficient to disprove it. The case $n=1$ serves as this counterexample.\n\n**Note on Mathematical Context:**\nFor $n > 1$, the statement is true. This is a famous result in number theory known as **Bertrand's Postulate** (or the Bertrand-Chebyshev Theorem), which states that for any integer $n > 1$, there always exists at least one prime $p$ such that $n < p < 2n$. However, because the question specifies \"For every natural number $n$\" and standard definitions of $\\mathbb{N}$ include $1$, the statement is strictly false as written.\n\n### 3. Conclusion and Proof\n\n**Truth Value:** **False**\n\n**Counterexample:**\nLet $n = 1$. The statement asserts the existence of a prime $p$ such that $1 < p < 2$. Since there are no integers between 1 and 2, there is no such prime. Thus, the condition fails for $n=1$.\n\n**Final Formalized Answer:**\nThe formalized statement is:\n$$ \\forall n \\in \\mathbb{N}, \\exists y \\in \\mathbb{N}, (Q(n, y) \\land y < 2n) $$\nThe statement is **false** because for $n=1$, the smallest prime greater than 1 is 2, and $2$ is not strictly less than $2(1)$.",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ]
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "## Step 1: Understand the task\nThe task is to generate a challenging but solvable mathematics problem in the domain of Combinatorics, along with a complete and verifiable solution. The problem must be well-posed, clear, non-trivial, and solvable with established mathematical methods.\n\n## Step 2: Identify the domain\nThe domain specified is Combinatorics, which involves the study of counting, arranging, and optimizing objects in various ways. It encompasses topics such as permutations, combinations, graph theory, and more.\n\n## Step 3: Generate a problem\nTo create a challenging problem, let's consider a topic within Combinatorics that is both interesting and requires substantial reasoning. A potential topic is graph theory, specifically focusing on graph coloring or connectivity.\n\n## Step 4: Formulate the problem\nLet's formulate a problem: \"Prove that in any simple graph with $n$ vertices, if the graph is connected and has no cycles (i.e., it's a tree), then it has exactly $n-1$ edges.\"\n\n## Step 5: Verify the problem against the quality rubric\n- **Well-Posedness and Completeness**: The problem is self-contained, defines the necessary terms (simple graph, connected, no cycles, tree), and asks for a proof.\n- **Clarity**: The terms used are standard in graph theory, and the task is clear.\n- **Non-Triviality**: The problem requires understanding of graph theory and a proof involving induction or another suitable method, making it non-trivial.\n- **Solvability and Verification**: The statement is a known fact in graph theory and can be proven using established methods.\n\n## Step 6: Provide the solution\nTo solve this, we can use induction on $n$. For $n=1$, a tree has 0 edges, and $1-1=0$, so the statement holds. For $n>1$, assume the statement is true for all trees with fewer than $n$ vertices. A tree with $n$ vertices has at least one leaf (a vertex of degree 1); removing this leaf and its edge gives a tree with $n-1$ vertices. By the induction hypothesis, this smaller tree has $(n-1)-1 = n-2$ edges. Adding back the removed edge gives $n-2 + 1 = n-1$ edges, proving the statement for $n$.\n\n## Step 7: Format the output\nLet's format the problem and solution according to the required structure.\n\n\nLet $G$ be a simple graph with $n$ vertices. Prove that if $G$ is a tree (i.e., it is connected and has no cycles), then it has exactly $n-1$ edges.",
    "run_id": "2",
    "outer_attempt": 1,
    "topic_slug": "combinatorics",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "To prove that a simple graph $G$ with $n$ vertices that is a tree (connected and acyclic) has exactly $n-1$ edges, we will use the principle of mathematical induction on the number of vertices $n$.\n\n### Proof\n\nLet $P(n)$ be the statement: \"Any tree with $n$ vertices has exactly $n-1$ edges.\"\n\n**1. Base Case ($n=1$)**\nConsider a graph $G$ with $n=1$ vertex. Since $G$ is a simple graph, it contains no self-loops. Since there is only one vertex, there can be no edges connecting distinct vertices. Therefore, the number of edges is 0.\nThe formula gives $n-1 = 1-1 = 0$.\nThus, $P(1)$ is true.\n\n**2. Inductive Hypothesis**\nAssume that $P(k)$ is true for some integer $k \\geq 1$. That is, assume any tree with $k$ vertices has exactly $k-1$ edges.\n\n**3. Inductive Step**\nWe must show that $P(k+1)$ is true. Let $T$ be an arbitrary tree with $n = k+1$ vertices. We need to show that $T$ has $(k+1)-1 = k$ edges.\n\nTo proceed, we utilize the following lemma:\n\n**Lemma:** Every tree with at least 2 vertices has at least two vertices of degree 1 (leaves).\n\n*Proof of Lemma:*\nConsider a simple path of maximal length in $T$. Let this path be $v_1, v_2, \\dots, v_m$. Since the graph is finite, such a path exists.\nConsider the endpoint $v_1$.\n- $v_1$ is connected to $v_2$.\n- If $v_1$ were connected to any other vertex $v_j$ in the path (where $j > 2$), the sequence $v_1, v_2, \\dots, v_j, v_1$ would form a cycle. This contradicts the assumption that $T$ is acyclic.\n- If $v_1$ were connected to any vertex $u$ not in the path, we could extend the path to $u, v_1, \\dots, v_m$, contradicting the assumption that the path is maximal.\nTherefore, $v_1$ has no neighbors other than $v_2$, so $\\deg(v_1) = 1$. The same argument applies to the other endpoint $v_m$. Thus, $T$ has at least two leaves.\n\n**Continuing the Inductive Step:**\nSince $n = k+1 \\geq 2$, by the Lemma, $T$ contains a leaf vertex $v$ with degree 1. Let $e$ be the unique edge incident to $v$, connecting $v$ to some vertex $u$.\n\nNow, consider the graph $T'$ obtained by removing the vertex $v$ and the edge $e$ from $T$.\n$$ V(T') = V(T) \\setminus \\{v\\} $$\n$$ E(T') = E(T) \\setminus \\{e\\} $$\n\nWe must verify that $T'$ is a tree:\n1.  **$T'$ has $k$ vertices:** Since $T$ had $k+1$ vertices, removing one leaves $k$.\n2.  **$T'$ is acyclic:** $T'$ is a subgraph of $T$. Since $T$ has no cycles, $T'$ cannot have cycles.\n3.  **$T'$ is connected:** Let $x$ and $y$ be any two distinct vertices in $T'$. Since $T$ is a tree, there is a unique path between $x$ and $y$ in $T$. Because $v$ is a leaf (degree 1), $v$ cannot be an intermediate vertex on the path between any two other vertices $x$ and $y$. Therefore, the path connecting $x$ and $y$ in $T$ consists entirely of edges and vertices present in $T'$. Thus, $x$ and $y$ are connected in $T'$.\n\nSince $T'$ is a connected acyclic graph with $k$ vertices, $T'$ is a tree. By the **Inductive Hypothesis**, $T'$ has exactly $k-1$ edges.\n\nThe number of edges in the original tree $T$ is the number of edges in $T'$ plus the removed edge $e$.\n$$ |E(T)| = |E(T')| + 1 = (k-1) + 1 = k $$\n\nThis matches the requirement $n-1 = (k+1)-1 = k$. Thus, $P(k+1)$ is true.\n\n**Conclusion**\nBy the principle of mathematical induction, for every integer $n \\geq 1$, a tree with $n$ vertices has exactly $n-1$ edges.",
        "raw_answer": "To prove that a simple graph $G$ with $n$ vertices that is a tree (connected and acyclic) has exactly $n-1$ edges, we will use the principle of mathematical induction on the number of vertices $n$.\n\n### Proof\n\nLet $P(n)$ be the statement: \"Any tree with $n$ vertices has exactly $n-1$ edges.\"\n\n**1. Base Case ($n=1$)**\nConsider a graph $G$ with $n=1$ vertex. Since $G$ is a simple graph, it contains no self-loops. Since there is only one vertex, there can be no edges connecting distinct vertices. Therefore, the number of edges is 0.\nThe formula gives $n-1 = 1-1 = 0$.\nThus, $P(1)$ is true.\n\n**2. Inductive Hypothesis**\nAssume that $P(k)$ is true for some integer $k \\geq 1$. That is, assume any tree with $k$ vertices has exactly $k-1$ edges.\n\n**3. Inductive Step**\nWe must show that $P(k+1)$ is true. Let $T$ be an arbitrary tree with $n = k+1$ vertices. We need to show that $T$ has $(k+1)-1 = k$ edges.\n\nTo proceed, we utilize the following lemma:\n\n**Lemma:** Every tree with at least 2 vertices has at least two vertices of degree 1 (leaves).\n\n*Proof of Lemma:*\nConsider a simple path of maximal length in $T$. Let this path be $v_1, v_2, \\dots, v_m$. Since the graph is finite, such a path exists.\nConsider the endpoint $v_1$.\n- $v_1$ is connected to $v_2$.\n- If $v_1$ were connected to any other vertex $v_j$ in the path (where $j > 2$), the sequence $v_1, v_2, \\dots, v_j, v_1$ would form a cycle. This contradicts the assumption that $T$ is acyclic.\n- If $v_1$ were connected to any vertex $u$ not in the path, we could extend the path to $u, v_1, \\dots, v_m$, contradicting the assumption that the path is maximal.\nTherefore, $v_1$ has no neighbors other than $v_2$, so $\\deg(v_1) = 1$. The same argument applies to the other endpoint $v_m$. Thus, $T$ has at least two leaves.\n\n**Continuing the Inductive Step:**\nSince $n = k+1 \\geq 2$, by the Lemma, $T$ contains a leaf vertex $v$ with degree 1. Let $e$ be the unique edge incident to $v$, connecting $v$ to some vertex $u$.\n\nNow, consider the graph $T'$ obtained by removing the vertex $v$ and the edge $e$ from $T$.\n$$ V(T') = V(T) \\setminus \\{v\\} $$\n$$ E(T') = E(T) \\setminus \\{e\\} $$\n\nWe must verify that $T'$ is a tree:\n1.  **$T'$ has $k$ vertices:** Since $T$ had $k+1$ vertices, removing one leaves $k$.\n2.  **$T'$ is acyclic:** $T'$ is a subgraph of $T$. Since $T$ has no cycles, $T'$ cannot have cycles.\n3.  **$T'$ is connected:** Let $x$ and $y$ be any two distinct vertices in $T'$. Since $T$ is a tree, there is a unique path between $x$ and $y$ in $T$. Because $v$ is a leaf (degree 1), $v$ cannot be an intermediate vertex on the path between any two other vertices $x$ and $y$. Therefore, the path connecting $x$ and $y$ in $T$ consists entirely of edges and vertices present in $T'$. Thus, $x$ and $y$ are connected in $T'$.\n\nSince $T'$ is a connected acyclic graph with $k$ vertices, $T'$ is a tree. By the **Inductive Hypothesis**, $T'$ has exactly $k-1$ edges.\n\nThe number of edges in the original tree $T$ is the number of edges in $T'$ plus the removed edge $e$.\n$$ |E(T)| = |E(T')| + 1 = (k-1) + 1 = k $$\n\nThis matches the requirement $n-1 = (k+1)-1 = k$. Thus, $P(k+1)$ is true.\n\n**Conclusion**\nBy the principle of mathematical induction, for every integer $n \\geq 1$, a tree with $n$ vertices has exactly $n-1$ edges.",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ]
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "## Step 1: Understand the task and the requirements for the problem.\nThe task is to generate a challenging but solvable mathematics problem in the domain of number theory, along with a complete and verifiable solution. The problem must be well-posed, clear, non-trivial, and solvable with established mathematical methods.\n\n## Step 2: Identify a suitable topic within number theory.\nNumber theory encompasses various topics such as divisibility, prime numbers, congruences, and Diophantine equations. A challenging problem could involve proving a property related to prime numbers or solving a specific type of Diophantine equation.\n\n## Step 3: Formulate a problem that meets the criteria.\nLet's consider a problem involving the sum of divisors of a number and its relation to perfect numbers. A perfect number is a positive integer that is equal to the sum of its proper divisors, excluding the number itself.\n\n## Step 4: Specify the problem.\nConsider the following problem: \"Prove that if $n$ is an even perfect number, then $n = 2^{p-1}(2^p - 1)$, where $2^p - 1$ is a Mersenne prime.\"\n\n## Step 5: Provide the solution to the problem.\nTo solve this, we need to understand the definition of an even perfect number and its relation to Mersenne primes.\n\n## Step 6: Recall the definition and properties of even perfect numbers.\nAn even perfect number $n$ can be written as $n = 2^{k-1}(2^k - 1)$, where $2^k - 1$ is prime. This is known as the Euclid-Euler theorem.\n\n## Step 7: State the Euclid-Euler theorem.\nThe Euclid-Euler theorem states that every even perfect number is of the form $2^{p-1}(2^p - 1)$, where $2^p - 1$ is a Mersenne prime (i.e., $2^p - 1$ is prime).\n\n## Step 8: Provide the proof or reasoning for the given statement.\nThe proof involves two main parts: (1) showing that any number of the form $2^{p-1}(2^p - 1)$, where $2^p - 1$ is prime, is perfect, and (2) showing that every even perfect number is of this form.\n\n## Step 9: Outline the proof that $2^{p-1}(2^p - 1)$ is perfect when $2^p - 1$ is prime.\nLet $n = 2^{p-1}(2^p - 1)$. The sum of divisors of $n$, $\\sigma(n)$, is given by $\\sigma(n) = \\sigma(2^{p-1})\\sigma(2^p - 1)$. Since $2^p - 1$ is prime, $\\sigma(2^p - 1) = 2^p$. Also, $\\sigma(2^{p-1}) = 1 + 2 + 2^2 + \\cdots + 2^{p-1} = 2^p - 1$. Thus, $\\sigma(n) = (2^p - 1) \\cdot 2^p = 2 \\cdot 2^{p-1}(2^p - 1) = 2n$, proving $n$ is perfect.\n\n## Step 10: Outline the proof that every even perfect number is of the form $2^{p-1}(2^p - 1)$.\nLet $n$ be an even perfect number. Then $n = 2^k \\cdot m$, where $m$ is odd. Since $n$ is perfect, $\\sigma(n) = 2n$. Using the multiplicative property of $\\sigma$, we have $\\sigma(n) = \\sigma(2^k)\\sigma(m) = (2^{k+1} - 1)\\sigma(m)$. Thus, $(2^{k+1} - 1)\\sigma(m) = 2^{k+1}m$. This implies $2^{k+1} - 1$ divides $m$, so $m = (2^{k+1} - 1)d$ for some $d$. Substituting back gives $\\sigma(m) = 2^{k+1}d$. For $d = 1$, $m = 2^{k+1} - 1$ is prime, and $\\sigma(m) = m + 1 = 2^{k+1}$, which matches our requirement. For $d > 1$, it leads to a contradiction because $\\sigma(m) \\geq m + d > 2d$, unless $d = 1$ and $m$ is prime.\n\n## Step 11: Conclude the solution.\nThe given statement is a direct consequence of the Euclid-Euler theorem, which characterizes even perfect numbers.\n\nThe final answer is: \n\nProve that if $n$ is an even perfect number, then $n = 2^{p-1}(2^p - 1)$, where $2^p - 1$ is a Mersenne prime.",
    "run_id": "3",
    "outer_attempt": 1,
    "topic_slug": "number_theory",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "Here is the rigorous proof of the statement.\n\n### **Theorem**\nIf $n$ is an even perfect number, then $n$ takes the form $2^{p-1}(2^p - 1)$, where $2^p - 1$ is a Mersenne prime.\n\n---\n\n### **Proof**\n\n**1. Definitions and Setup**\nLet $n$ be an even perfect number.\nSince $n$ is even, we can write $n$ in the form:\n$$n = 2^k \\cdot m$$\nwhere $k \\geq 1$ is an integer and $m$ is an odd positive integer.\n\nSince $n$ is a perfect number, the sum of its positive divisors, denoted by $\\sigma(n)$, satisfies:\n$$\\sigma(n) = 2n$$\n\n**2. Multiplicative Property of $\\sigma$**\nThe function $\\sigma$ is multiplicative, meaning $\\sigma(ab) = \\sigma(a)\\sigma(b)$ if $\\gcd(a, b) = 1$.\nSince $m$ is odd, $\\gcd(2^k, m) = 1$. Therefore:\n$$\\sigma(n) = \\sigma(2^k \\cdot m) = \\sigma(2^k)\\sigma(m)$$\n\nWe calculate $\\sigma(2^k)$ as the sum of the geometric series $1 + 2 + \\dots + 2^k$:\n$$\\sigma(2^k) = \\frac{2^{k+1} - 1}{2 - 1} = 2^{k+1} - 1$$\n\nSubstituting this back into the perfect number equation $\\sigma(n) = 2n$:\n$$(2^{k+1} - 1)\\sigma(m) = 2(2^k \\cdot m)$$\n$$(2^{k+1} - 1)\\sigma(m) = 2^{k+1}m \\quad (*)$$\n\n**3. Divisibility Analysis**\nFrom equation $(*)$, we see that $2^{k+1} - 1$ divides the product $2^{k+1}m$.\nSince $2^{k+1} - 1$ is an odd number, $\\gcd(2^{k+1} - 1, 2^{k+1}) = 1$.\nBy Euclid's Lemma, $2^{k+1} - 1$ must divide $m$.\n\nThus, there exists an integer $q$ such that:\n$$m = (2^{k+1} - 1)q$$\n\n**4. Analyzing $\\sigma(m)$**\nSubstitute $m = (2^{k+1} - 1)q$ back into equation $(*)$:\n$$(2^{k+1} - 1)\\sigma(m) = 2^{k+1}(2^{k+1} - 1)q$$\nDividing both sides by the non-zero term $(2^{k+1} - 1)$, we obtain:\n$$\\sigma(m) = 2^{k+1}q$$\n\nWe also know that $m = (2^{k+1} - 1)q$. Let us consider the divisors of $m$.\nSince $m$ is a multiple of $q$, both $m$ and $q$ are distinct divisors of $m$. (Note: Since $n$ is even, $k \\geq 1$, which implies $2^{k+1} - 1 \\geq 3$, so $m > q$).\n\nThe sum of the divisors of $m$, $\\sigma(m)$, must include at least $m$ and $q$. Therefore:\n$$\\sigma(m) \\geq m + q$$\n\nSubstituting the expressions in terms of $q$:\n$$\\sigma(m) \\geq (2^{k+1} - 1)q + q$$\n$$\\sigma(m) \\geq 2^{k+1}q - q + q$$\n$$\\sigma(m) \\geq 2^{k+1}q$$\n\n**5. Forcing Primality**\nWe have established two facts:\n1. $\\sigma(m) = 2^{k+1}q$ (from the equation)\n2. $\\sigma(m) \\geq m + q = 2^{k+1}q$ (from the property of divisors)\n\nThe equality $\\sigma(m) = m + q$ holds if and only if $m$ and $q$ are the **only** positive divisors of $m$.\nThis implies two conditions:\n1. The divisor $q$ must be $1$ (since $1$ is always a divisor of any integer).\n2. The number $m$ must be prime (since its only divisors are $1$ and itself).\n\nThus, we determine:\n$$q = 1$$\n$$m = (2^{k+1} - 1)(1) = 2^{k+1} - 1$$\nAnd $m$ is a prime number.\n\n**6. Conclusion**\nWe have shown that $m = 2^{k+1} - 1$ is prime. For a number of the form $2^x - 1$ to be prime, the exponent $x$ must be prime. Let $p = k+1$, where $p$ is prime.\nThen:\n$$m = 2^p - 1$$\nis a Mersenne prime.\n\nSubstituting $k = p - 1$ and $m = 2^p - 1$ back into the expression for $n$:\n$$n = 2^{p-1}(2^p - 1)$$\n\n### **Final Answer**\nWe have proven that any even perfect number $n$ must be of the form $n = 2^{p-1}(2^p - 1)$, where $2^p - 1$ is a Mersenne prime.",
        "raw_answer": "Here is the rigorous proof of the statement.\n\n### **Theorem**\nIf $n$ is an even perfect number, then $n$ takes the form $2^{p-1}(2^p - 1)$, where $2^p - 1$ is a Mersenne prime.\n\n---\n\n### **Proof**\n\n**1. Definitions and Setup**\nLet $n$ be an even perfect number.\nSince $n$ is even, we can write $n$ in the form:\n$$n = 2^k \\cdot m$$\nwhere $k \\geq 1$ is an integer and $m$ is an odd positive integer.\n\nSince $n$ is a perfect number, the sum of its positive divisors, denoted by $\\sigma(n)$, satisfies:\n$$\\sigma(n) = 2n$$\n\n**2. Multiplicative Property of $\\sigma$**\nThe function $\\sigma$ is multiplicative, meaning $\\sigma(ab) = \\sigma(a)\\sigma(b)$ if $\\gcd(a, b) = 1$.\nSince $m$ is odd, $\\gcd(2^k, m) = 1$. Therefore:\n$$\\sigma(n) = \\sigma(2^k \\cdot m) = \\sigma(2^k)\\sigma(m)$$\n\nWe calculate $\\sigma(2^k)$ as the sum of the geometric series $1 + 2 + \\dots + 2^k$:\n$$\\sigma(2^k) = \\frac{2^{k+1} - 1}{2 - 1} = 2^{k+1} - 1$$\n\nSubstituting this back into the perfect number equation $\\sigma(n) = 2n$:\n$$(2^{k+1} - 1)\\sigma(m) = 2(2^k \\cdot m)$$\n$$(2^{k+1} - 1)\\sigma(m) = 2^{k+1}m \\quad (*)$$\n\n**3. Divisibility Analysis**\nFrom equation $(*)$, we see that $2^{k+1} - 1$ divides the product $2^{k+1}m$.\nSince $2^{k+1} - 1$ is an odd number, $\\gcd(2^{k+1} - 1, 2^{k+1}) = 1$.\nBy Euclid's Lemma, $2^{k+1} - 1$ must divide $m$.\n\nThus, there exists an integer $q$ such that:\n$$m = (2^{k+1} - 1)q$$\n\n**4. Analyzing $\\sigma(m)$**\nSubstitute $m = (2^{k+1} - 1)q$ back into equation $(*)$:\n$$(2^{k+1} - 1)\\sigma(m) = 2^{k+1}(2^{k+1} - 1)q$$\nDividing both sides by the non-zero term $(2^{k+1} - 1)$, we obtain:\n$$\\sigma(m) = 2^{k+1}q$$\n\nWe also know that $m = (2^{k+1} - 1)q$. Let us consider the divisors of $m$.\nSince $m$ is a multiple of $q$, both $m$ and $q$ are distinct divisors of $m$. (Note: Since $n$ is even, $k \\geq 1$, which implies $2^{k+1} - 1 \\geq 3$, so $m > q$).\n\nThe sum of the divisors of $m$, $\\sigma(m)$, must include at least $m$ and $q$. Therefore:\n$$\\sigma(m) \\geq m + q$$\n\nSubstituting the expressions in terms of $q$:\n$$\\sigma(m) \\geq (2^{k+1} - 1)q + q$$\n$$\\sigma(m) \\geq 2^{k+1}q - q + q$$\n$$\\sigma(m) \\geq 2^{k+1}q$$\n\n**5. Forcing Primality**\nWe have established two facts:\n1. $\\sigma(m) = 2^{k+1}q$ (from the equation)\n2. $\\sigma(m) \\geq m + q = 2^{k+1}q$ (from the property of divisors)\n\nThe equality $\\sigma(m) = m + q$ holds if and only if $m$ and $q$ are the **only** positive divisors of $m$.\nThis implies two conditions:\n1. The divisor $q$ must be $1$ (since $1$ is always a divisor of any integer).\n2. The number $m$ must be prime (since its only divisors are $1$ and itself).\n\nThus, we determine:\n$$q = 1$$\n$$m = (2^{k+1} - 1)(1) = 2^{k+1} - 1$$\nAnd $m$ is a prime number.\n\n**6. Conclusion**\nWe have shown that $m = 2^{k+1} - 1$ is prime. For a number of the form $2^x - 1$ to be prime, the exponent $x$ must be prime. Let $p = k+1$, where $p$ is prime.\nThen:\n$$m = 2^p - 1$$\nis a Mersenne prime.\n\nSubstituting $k = p - 1$ and $m = 2^p - 1$ back into the expression for $n$:\n$$n = 2^{p-1}(2^p - 1)$$\n\n### **Final Answer**\nWe have proven that any even perfect number $n$ must be of the form $n = 2^{p-1}(2^p - 1)$, where $2^p - 1$ is a Mersenne prime.",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ]
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "## Step 1: Understand the task\nThe task is to generate a challenging mathematics problem in the domain of \"Order, lattices, ordered algebraic structures\" along with a complete and verifiable solution.\n\n## Step 2: Identify the key elements of the problem domain\nThe domain involves concepts related to order, lattices, and ordered algebraic structures. This includes partially ordered sets (posets), lattices, Boolean algebras, and other related structures.\n\n## Step 3: Formulate a problem that fits the domain\nTo create a problem, we need to consider what aspects of \"Order, lattices, ordered algebraic structures\" can be explored. A potential problem could involve proving a property about a specific type of lattice or ordered structure.\n\n## Step 4: Generate a specific problem\nLet's consider a problem involving a Boolean algebra, which is a type of lattice. A Boolean algebra is a distributive lattice with the additional property that every element has a complement.\n\n## Step 5: State the problem\n\"Let $B$ be a Boolean algebra. Prove that for any elements $a, b \\in B$, the following identity holds: $a \\vee (a \\wedge b) = a$.\"\n\n## Step 6: Provide a solution to the problem\nTo prove $a \\vee (a \\wedge b) = a$, we can use the properties of a Boolean algebra, specifically the absorption laws.\n\n## Step 7: Write out the solution in detail\nFirst, recall that a Boolean algebra is a distributive lattice with complementation. The absorption laws state that for any $a, b$ in the Boolean algebra, $a \\vee (a \\wedge b) = a$ and $a \\wedge (a \\vee b) = a$. We need to prove the first absorption law.\n\n## Step 8: Apply the properties of a lattice to prove the identity\nBy definition of a lattice, $a \\wedge b \\leq a$ because $a$ is an upper bound of $a$ and $a \\wedge b$, and $a \\wedge b$ is the greatest lower bound of $a$ and $b$. Thus, $a \\vee (a \\wedge b) = a$ because $a \\vee x = a$ whenever $x \\leq a$.\n\n## Step 9: Formalize the solution\nGiven $a, b \\in B$, since $a \\wedge b \\leq a$, we have $a \\vee (a \\wedge b) = a$ due to the property that $x \\leq y$ implies $y \\vee x = y$. Thus, $a \\vee (a \\wedge b) = a$.\n\n## Step 10: Present the problem and solution in the required format\n\nLet $B$ be a Boolean algebra. Prove that for any elements $a, b \\in B$, the following identity holds: $a \\vee (a \\wedge b) = a$.",
    "run_id": "4",
    "topic_slug": "order_lattices_and_ordered_algebraic_structures",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "To provide a rigorous proof of the absorption identity $a \\vee (a \\wedge b) = a$ in a Boolean algebra, we must first explicitly state the definition and axioms of a Boolean algebra being used. We will use the standard algebraic axiomatization.\n\n### Definition and Axioms\n\nA Boolean algebra is a set $B$ equipped with two binary operations $\\vee$ (join/disjunction) and $\\wedge$ (meet/conjunction), a unary operation $'$ (complement), and two distinct elements $0$ (zero) and $1$ (one), satisfying the following axioms for all $x, y, z \\in B$:\n\n1.  **Commutativity:**\n    $x \\vee y = y \\vee x$\n    $x \\wedge y = y \\wedge x$\n\n2.  **Associativity:**\n    $x \\vee (y \\vee z) = (x \\vee y) \\vee z$\n    $x \\wedge (y \\wedge z) = (x \\wedge y) \\wedge z$\n\n3.  **Distributivity:**\n    $x \\vee (y \\wedge z) = (x \\vee y) \\wedge (x \\vee z)$\n    $x \\wedge (y \\vee z) = (x \\wedge y) \\vee (x \\wedge z)$\n\n4.  **Identity:**\n    $x \\vee 0 = x$\n    $x \\wedge 1 = x$\n\n5.  **Complementation:**\n    $x \\vee x' = 1$\n    $x \\wedge x' = 0$\n\n### Proof\n\nWe want to prove the absorption identity: $a \\vee (a \\wedge b) = a$.\n\n**Step 1: Use the Identity Axiom**\nBy the Identity axiom for $\\wedge$, we can rewrite $a$ as:\n$$a = a \\wedge 1$$\n\n**Step 2: Substitute into the Expression**\nSubstitute this into the left-hand side of the target identity:\n$$a \\vee (a \\wedge b) = (a \\wedge 1) \\vee (a \\wedge b)$$\n\n**Step 3: Apply Distributivity**\nWe use the distributive property of $\\wedge$ over $\\vee$ (in reverse):\n$$(a \\wedge 1) \\vee (a \\wedge b) = a \\wedge (1 \\vee b)$$\n\n**Step 4: Establish the Property of the Element 1 (Annihilation)**\nWe need to show that $1 \\vee b = 1$. To prove this, we use the Complementation and Distributivity axioms.\nFirst, recall $1 = b \\vee b'$ by Complementation.\n$$1 \\vee b = (b \\vee b') \\vee b$$\nBy Commutativity and Associativity:\n$$(b \\vee b') \\vee b = b \\vee (b \\vee b') = (b \\vee b) \\vee b'$$\nTo proceed, we need the **Idempotent Law** ($x \\vee x = x$). Let us prove it briefly:\n$$x = x \\vee 0 = x \\vee (x \\wedge x') = (x \\vee x) \\wedge (x \\vee x') = (x \\vee x) \\wedge 1 = x \\vee x$$\nUsing Idempotence ($b \\vee b = b$):\n$$(b \\vee b) \\vee b' = b \\vee b' = 1$$\nThus, $1 \\vee b = 1$. (By commutativity, $b \\vee 1 = 1$ as well).\n\n**Step 5: Complete the Proof**\nSubstitute $1 \\vee b = 1$ back into the result from Step 3:\n$$a \\wedge (1 \\vee b) = a \\wedge 1$$\nBy the Identity axiom ($a \\wedge 1 = a$):\n$$a \\wedge 1 = a$$\n\n### Conclusion\n\nWe have shown a complete chain of equality derived from the axioms:\n$$a \\vee (a \\wedge b) = (a \\wedge 1) \\vee (a \\wedge b) = a \\wedge (1 \\vee b) = a \\wedge 1 = a$$\nTherefore, the identity $a \\vee (a \\wedge b) = a$ holds for any elements $a, b$ in the Boolean algebra $B$.",
        "raw_answer": "To provide a rigorous proof of the absorption identity $a \\vee (a \\wedge b) = a$ in a Boolean algebra, we must first explicitly state the definition and axioms of a Boolean algebra being used. We will use the standard algebraic axiomatization.\n\n### Definition and Axioms\n\nA Boolean algebra is a set $B$ equipped with two binary operations $\\vee$ (join/disjunction) and $\\wedge$ (meet/conjunction), a unary operation $'$ (complement), and two distinct elements $0$ (zero) and $1$ (one), satisfying the following axioms for all $x, y, z \\in B$:\n\n1.  **Commutativity:**\n    $x \\vee y = y \\vee x$\n    $x \\wedge y = y \\wedge x$\n\n2.  **Associativity:**\n    $x \\vee (y \\vee z) = (x \\vee y) \\vee z$\n    $x \\wedge (y \\wedge z) = (x \\wedge y) \\wedge z$\n\n3.  **Distributivity:**\n    $x \\vee (y \\wedge z) = (x \\vee y) \\wedge (x \\vee z)$\n    $x \\wedge (y \\vee z) = (x \\wedge y) \\vee (x \\wedge z)$\n\n4.  **Identity:**\n    $x \\vee 0 = x$\n    $x \\wedge 1 = x$\n\n5.  **Complementation:**\n    $x \\vee x' = 1$\n    $x \\wedge x' = 0$\n\n### Proof\n\nWe want to prove the absorption identity: $a \\vee (a \\wedge b) = a$.\n\n**Step 1: Use the Identity Axiom**\nBy the Identity axiom for $\\wedge$, we can rewrite $a$ as:\n$$a = a \\wedge 1$$\n\n**Step 2: Substitute into the Expression**\nSubstitute this into the left-hand side of the target identity:\n$$a \\vee (a \\wedge b) = (a \\wedge 1) \\vee (a \\wedge b)$$\n\n**Step 3: Apply Distributivity**\nWe use the distributive property of $\\wedge$ over $\\vee$ (in reverse):\n$$(a \\wedge 1) \\vee (a \\wedge b) = a \\wedge (1 \\vee b)$$\n\n**Step 4: Establish the Property of the Element 1 (Annihilation)**\nWe need to show that $1 \\vee b = 1$. To prove this, we use the Complementation and Distributivity axioms.\nFirst, recall $1 = b \\vee b'$ by Complementation.\n$$1 \\vee b = (b \\vee b') \\vee b$$\nBy Commutativity and Associativity:\n$$(b \\vee b') \\vee b = b \\vee (b \\vee b') = (b \\vee b) \\vee b'$$\nTo proceed, we need the **Idempotent Law** ($x \\vee x = x$). Let us prove it briefly:\n$$x = x \\vee 0 = x \\vee (x \\wedge x') = (x \\vee x) \\wedge (x \\vee x') = (x \\vee x) \\wedge 1 = x \\vee x$$\nUsing Idempotence ($b \\vee b = b$):\n$$(b \\vee b) \\vee b' = b \\vee b' = 1$$\nThus, $1 \\vee b = 1$. (By commutativity, $b \\vee 1 = 1$ as well).\n\n**Step 5: Complete the Proof**\nSubstitute $1 \\vee b = 1$ back into the result from Step 3:\n$$a \\wedge (1 \\vee b) = a \\wedge 1$$\nBy the Identity axiom ($a \\wedge 1 = a$):\n$$a \\wedge 1 = a$$\n\n### Conclusion\n\nWe have shown a complete chain of equality derived from the axioms:\n$$a \\vee (a \\wedge b) = (a \\wedge 1) \\vee (a \\wedge b) = a \\wedge (1 \\vee b) = a \\wedge 1 = a$$\nTherefore, the identity $a \\vee (a \\wedge b) = a$ holds for any elements $a, b$ in the Boolean algebra $B$.",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ],
    "outer_attempt": 1
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "## Step 1: Understand the task\nThe task is to generate a challenging but solvable mathematics problem in the domain of general algebraic systems, along with a complete and verifiable solution.\n\n## Step 2: Identify the domain\nThe domain is general algebraic systems, which includes topics like groups, rings, fields, and other algebraic structures.\n\n## Step 3: Formulate a problem\nLet's consider a problem involving a specific algebraic structure. A challenging problem could involve proving a property about a certain type of algebraic system.\n\n## Step 4: Create a specific problem\nConsider the following problem: \"Let $G$ be a finite group and $H$ be a subgroup of $G$. Suppose that $|G| = 2|H|$. Prove that $H$ is a normal subgroup of $G$.\"\n\n## Step 5: Verify the problem meets the quality rubric\n- **Well-Posedness and Completeness**: The problem is fully specified with $G$ being a finite group and $H$ a subgroup of $G$, and the condition $|G| = 2|H|$ is given.\n- **Clarity**: The terms are standard in group theory, and the task is clear.\n- **Non-Triviality**: The problem requires understanding of group theory and the concept of normal subgroups, making it non-trivial.\n- **Solvability and Verification**: The problem is solvable using established methods in group theory.\n\n## Step 6: Provide the solution\nTo prove that $H$ is a normal subgroup of $G$, we need to show that for any $g \\in G$ and $h \\in H$, $g^{-1}hg \\in H$.\n\n## Step 7: Break down the solution\nFirst, note that since $|G| = 2|H|$, $H$ has index 2 in $G$. This implies there are exactly two cosets of $H$ in $G$: $H$ itself and $G \\setminus H$.\n\n## Step 8: Analyze cosets\nFor any $g \\in G$, if $g \\in H$, then $gH = H$ and $Hg = H$, so $gH = Hg$. If $g \\notin H$, then $gH = G \\setminus H$ because there are only two cosets and $gH$ cannot be $H$. Similarly, $Hg = G \\setminus H$, so again $gH = Hg$.\n\n## Step 9: Conclude normality\nSince $gH = Hg$ for all $g \\in G$, $H$ is normal in $G$.\n\n## Step 10: Write the problem and solution in the required format\n\nLet $G$ be a finite group and $H$ be a subgroup of $G$. Suppose that $|G| = 2|H|$. Prove that $H$ is a normal subgroup of $G$.",
    "run_id": "5",
    "topic_slug": "general_algebraic_systems",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "**Solution to the Mathematics Question**\n\n**Problem:**\nLet $G$ be a finite group and $H$ be a subgroup of $G$. Suppose that $|G| = 2|H|$. Prove that $H$ is a normal subgroup of $G$.\n\n**Proof:**\n\n**1. Understanding the Index of $H$ in $G$**\nBy Lagrange's Theorem, for a finite group $G$ and a subgroup $H$, the order of the group is the product of the order of the subgroup and the index of the subgroup. That is:\n$$|G| = [G : H] \\cdot |H|$$\nGiven that $|G| = 2|H|$, we can determine the index of $H$ in $G$:\n$$[G : H] = \\frac{|G|}{|H|} = 2$$\nThe index $[G : H]$ represents the number of distinct left cosets of $H$ in $G$. Since the index is 2, there are exactly two distinct left cosets. Similarly, the number of distinct right cosets is also $[G : H] = 2$.\n\n**2. Analysis of Left Cosets**\nThe left cosets of $H$ in $G$ form a partition of $G$. Let the set of left cosets be $\\mathcal{L} = \\{gH \\mid g \\in G\\}$.\nSince $H$ is a subgroup, $e \\in H$ (where $e$ is the identity element of $G$), and thus $H = eH$ is itself one of the left cosets.\nSince there are exactly two left cosets and they partition $G$, the other left coset must consist of all elements in $G$ that are not in $H$.\nTherefore, the two left cosets are:\n1.  $H$\n2.  $G \\setminus H$ (the set complement of $H$ in $G$)\n\nFor any element $g \\in G$, the left coset $gH$ is determined as follows:\n*   If $g \\in H$, then $gH = H$.\n*   If $g \\notin H$, then $gH \\cap H = \\emptyset$. Since $gH$ is a coset and cannot be $H$, it must be the other coset, so $gH = G \\setminus H$.\n\n**3. Analysis of Right Cosets**\nSimilarly, the right cosets of $H$ in $G$ form a partition of $G$. Let the set of right cosets be $\\mathcal{R} = \\{Hg \\mid g \\in G\\}$.\nSince there are two right cosets, one is $H$ (since $He = H$). By the same logic as above, the right cosets partition $G$ into:\n1.  $H$\n2.  $G \\setminus H$\n\nFor any element $g \\in G$, the right coset $Hg$ is determined as follows:\n*   If $g \\in H$, then $Hg = H$.\n*   If $g \\notin H$, then $Hg \\cap H = \\emptyset$. Since $Hg$ cannot be $H$, it must be $Hg = G \\setminus H$.\n\n**4. Comparing Left and Right Cosets**\nTo prove that $H$ is a normal subgroup of $G$, we must show that $gH = Hg$ for all $g \\in G$.\n\nWe analyze this in two cases based on the membership of $g$:\n\n*   **Case 1: $g \\in H$**\n    From our analysis above:\n    $$gH = H \\quad \\text{and} \\quad Hg = H$$\n    Therefore, $gH = Hg$.\n\n*   **Case 2: $g \\notin H$**\n    From our analysis above:\n    $$gH = G \\setminus H \\quad \\text{and} \\quad Hg = G \\setminus H$$\n    Therefore, $gH = Hg$.\n\n**Conclusion**\nSince $gH = Hg$ for all $g \\in G$, $H$ satisfies the definition of a normal subgroup.\n\nTherefore, $H$ is a normal subgroup of $G$.",
        "raw_answer": "**Solution to the Mathematics Question**\n\n**Problem:**\nLet $G$ be a finite group and $H$ be a subgroup of $G$. Suppose that $|G| = 2|H|$. Prove that $H$ is a normal subgroup of $G$.\n\n**Proof:**\n\n**1. Understanding the Index of $H$ in $G$**\nBy Lagrange's Theorem, for a finite group $G$ and a subgroup $H$, the order of the group is the product of the order of the subgroup and the index of the subgroup. That is:\n$$|G| = [G : H] \\cdot |H|$$\nGiven that $|G| = 2|H|$, we can determine the index of $H$ in $G$:\n$$[G : H] = \\frac{|G|}{|H|} = 2$$\nThe index $[G : H]$ represents the number of distinct left cosets of $H$ in $G$. Since the index is 2, there are exactly two distinct left cosets. Similarly, the number of distinct right cosets is also $[G : H] = 2$.\n\n**2. Analysis of Left Cosets**\nThe left cosets of $H$ in $G$ form a partition of $G$. Let the set of left cosets be $\\mathcal{L} = \\{gH \\mid g \\in G\\}$.\nSince $H$ is a subgroup, $e \\in H$ (where $e$ is the identity element of $G$), and thus $H = eH$ is itself one of the left cosets.\nSince there are exactly two left cosets and they partition $G$, the other left coset must consist of all elements in $G$ that are not in $H$.\nTherefore, the two left cosets are:\n1.  $H$\n2.  $G \\setminus H$ (the set complement of $H$ in $G$)\n\nFor any element $g \\in G$, the left coset $gH$ is determined as follows:\n*   If $g \\in H$, then $gH = H$.\n*   If $g \\notin H$, then $gH \\cap H = \\emptyset$. Since $gH$ is a coset and cannot be $H$, it must be the other coset, so $gH = G \\setminus H$.\n\n**3. Analysis of Right Cosets**\nSimilarly, the right cosets of $H$ in $G$ form a partition of $G$. Let the set of right cosets be $\\mathcal{R} = \\{Hg \\mid g \\in G\\}$.\nSince there are two right cosets, one is $H$ (since $He = H$). By the same logic as above, the right cosets partition $G$ into:\n1.  $H$\n2.  $G \\setminus H$\n\nFor any element $g \\in G$, the right coset $Hg$ is determined as follows:\n*   If $g \\in H$, then $Hg = H$.\n*   If $g \\notin H$, then $Hg \\cap H = \\emptyset$. Since $Hg$ cannot be $H$, it must be $Hg = G \\setminus H$.\n\n**4. Comparing Left and Right Cosets**\nTo prove that $H$ is a normal subgroup of $G$, we must show that $gH = Hg$ for all $g \\in G$.\n\nWe analyze this in two cases based on the membership of $g$:\n\n*   **Case 1: $g \\in H$**\n    From our analysis above:\n    $$gH = H \\quad \\text{and} \\quad Hg = H$$\n    Therefore, $gH = Hg$.\n\n*   **Case 2: $g \\notin H$**\n    From our analysis above:\n    $$gH = G \\setminus H \\quad \\text{and} \\quad Hg = G \\setminus H$$\n    Therefore, $gH = Hg$.\n\n**Conclusion**\nSince $gH = Hg$ for all $g \\in G$, $H$ satisfies the definition of a normal subgroup.\n\nTherefore, $H$ is a normal subgroup of $G$.",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ],
    "outer_attempt": 1
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "Let $F$ be a field and let $p(x) = x^3 + ax + b \\in F[x]$ be a cubic polynomial with distinct roots in some extension field of $F$. Suppose that the discriminant of $p(x)$ is a square in $F$. Prove that the Galois group of $p(x)$ over $F$ is isomorphic to $\\mathbb{Z}/3\\mathbb{Z}$ if and only if $p(x)$ is irreducible over $F$.",
    "run_id": "6",
    "outer_attempt": 1,
    "topic_slug": "field_theory_and_polynomials",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "To provide a rigorous proof, we must interpret the properties of the polynomial and its Galois group within the context of Field Theory.\n\n### Preliminaries and Assumptions\n\nLet $F$ be a field and $p(x) = x^3 + ax + b \\in F[x]$.\nLet $K$ be a splitting field of $p(x)$ over $F$.\nLet the roots of $p(x)$ in $K$ be $\\alpha_1, \\alpha_2, \\alpha_3$. The problem states these roots are distinct, so $p(x)$ is separable and the discriminant $\\Delta \\neq 0$.\nThe Galois group of $p(x)$ over $F$ is $G = \\text{Gal}(K/F)$. Since $G$ permutes the three roots, $G$ is isomorphic to a subgroup of the symmetric group $S_3$.\n\n**Assumption regarding Characteristic:**\nWe assume that the characteristic of $F$ is not 2.\n*Reasoning:* The relationship between the discriminant being a square and the Galois group being a subgroup of the alternating group $A_3$ relies on the property that for distinct roots, the square root of the discriminant $\\delta = \\prod_{i<j}(\\alpha_i - \\alpha_j)$ satisfies $\\delta \\neq -\\delta$. In characteristic 2, this distinction vanishes, and the discriminant is always a square in $F$, rendering the condition in the problem statement trivial and the equivalence false (e.g., an irreducible cubic in characteristic 2 can have Galois group $S_3$).\n\n### Step 1: The Discriminant and the Alternating Group\n\nThe discriminant of $p(x)$ is given by $\\Delta = \\prod_{1 \\le i < j \\le 3} (\\alpha_i - \\alpha_j)^2$.\nLet $\\delta = \\prod_{1 \\le i < j \\le 3} (\\alpha_i - \\alpha_j)$. Then $\\delta^2 = \\Delta$.\nFor any $\\sigma \\in G$, the action of $\\sigma$ on $\\delta$ is given by:\n$$ \\sigma(\\delta) = \\text{sgn}(\\sigma) \\delta $$\nwhere $\\text{sgn}(\\sigma)$ is the sign of the permutation ($1$ for even permutations, $-1$ for odd permutations).\n\n**Hypothesis:** The discriminant $\\Delta$ is a square in $F$.\nThis implies $\\delta \\in F$.\nSince $\\delta \\in F$, it is fixed by all automorphisms in the Galois group:\n$$ \\forall \\sigma \\in G, \\quad \\sigma(\\delta) = \\delta $$\nCombining this with the sign property:\n$$ \\text{sgn}(\\sigma) \\delta = \\delta $$\nSince roots are distinct, $\\Delta \\neq 0$, so $\\delta \\neq 0$. In characteristic $\\neq 2$, $\\delta \\neq -\\delta$. Therefore:\n$$ \\text{sgn}(\\sigma) = 1 \\quad \\forall \\sigma \\in G $$\nThis implies that $G$ consists only of even permutations. Thus, $G$ is a subgroup of the alternating group $A_3$.\n\nThe subgroups of $S_3$ contained in $A_3$ are:\n1.  The trivial group $\\{e\\}$ (Order 1).\n2.  The alternating group $A_3 = \\{e, (123), (132)\\}$ (Order 3). Note that $A_3 \\cong \\mathbb{Z}/3\\mathbb{Z}$.\n\nThus, given the discriminant condition, we have established that $G \\cong \\{e\\}$ or $G \\cong \\mathbb{Z}/3\\mathbb{Z}$.\n\n### Step 2: Proof of Equivalence\n\nWe must prove: $G \\cong \\mathbb{Z}/3\\mathbb{Z} \\iff p(x)$ is irreducible over $F$.\n\n#### Part A: ($p(x)$ is irreducible $\\implies G \\cong \\mathbb{Z}/3\\mathbb{Z}$)\n\n1.  Assume $p(x)$ is irreducible over $F$.\n2.  Let $\\alpha_1$ be a root of $p(x)$. Since $p(x)$ is irreducible, the degree of the extension $F(\\alpha_1)/F$ is equal to the degree of the polynomial:\n    $$ [F(\\alpha_1) : F] = 3 $$\n3.  Since $K$ is the splitting field, $F \\subseteq F(\\alpha_1) \\subseteq K$. By the tower law, $[K:F] = [K:F(\\alpha_1)][F(\\alpha_1):F]$.\n4.  Therefore, $[F(\\alpha_1):F]$ divides $[K:F]$. Since $|G| = [K:F]$, we have $3 \\mid |G|$.\n5.  From Step 1, we know $G \\subseteq A_3$, so $|G|$ must divide $|A_3| = 3$.\n6.  The only positive integer that divides 3 and is divisible by 3 is 3.\n7.  Thus, $|G| = 3$. The only group of order 3 is the cyclic group $\\mathbb{Z}/3\\mathbb{Z}$.\n    $$ G \\cong \\mathbb{Z}/3\\mathbb{Z} $$\n\n#### Part B: ($G \\cong \\mathbb{Z}/3\\mathbb{Z} \\implies p(x)$ is irreducible)\n\n1.  Assume $G \\cong \\mathbb{Z}/3\\mathbb{Z}$. The elements of $G$ viewed as permutations of roots $\\{\\alpha_1, \\alpha_2, \\alpha_3\\}$ are $\\{e, (123), (132)\\}$.\n2.  The group $G$ acts transitively on the set of roots. (The orbit of any root $\\alpha_i$ is the entire set $\\{\\alpha_1, \\alpha_2, \\alpha_3\\}$).\n3.  **Transitivity implies irreducibility:**\n    If $p(x)$ were reducible over $F$, it would factor into polynomials of lower degree in $F[x]$.\n    Let $p(x) = f(x)g(x)$ with $\\deg(f) \\ge 1, \\deg(g) \\ge 1$.\n    The roots of $f(x)$ would form a subset of $\\{\\alpha_1, \\alpha_2, \\alpha_3\\}$ that is mapped to itself by all $\\sigma \\in G$ (since $\\sigma$ fixes coefficients in $F$, it maps roots of $f$ to roots of $f$).\n    This would imply the action of $G$ on the roots is not transitive (there would be an orbit of size $\\deg(f) < 3$).\n4.  Since the action is transitive, $p(x)$ cannot satisfy the condition of reducibility.\n    (Alternatively: The orbit of $\\alpha_1$ is the set of roots of its minimal polynomial. The size of the orbit is 3, so the minimal polynomial has degree 3. Since $p(x)$ is monic of degree 3 and shares a root with this minimal polynomial, $p(x)$ is the minimal polynomial, hence irreducible).\n5.  Therefore, $p(x)$ is irreducible over $F$.\n\n### Conclusion\n\nWe have shown that if the discriminant of the cubic polynomial $p(x)$ is a square in $F$ (and char $F \\neq 2$), then $G$ is a subgroup of $A_3$. Under this condition:\n- If $p(x)$ is irreducible, $|G|=3$, so $G \\cong \\mathbb{Z}/3\\mathbb{Z}$.\n- If $G \\cong \\mathbb{Z}/3\\mathbb{Z}$, the transitive action on roots implies $p(x)$ is irreducible.\n\nThus, $\\text{Gal}(p(x)/F) \\cong \\mathbb{Z}/3\\mathbb{Z}$ if and only if $p(x)$ is irreducible over $F$.",
        "raw_answer": "To provide a rigorous proof, we must interpret the properties of the polynomial and its Galois group within the context of Field Theory.\n\n### Preliminaries and Assumptions\n\nLet $F$ be a field and $p(x) = x^3 + ax + b \\in F[x]$.\nLet $K$ be a splitting field of $p(x)$ over $F$.\nLet the roots of $p(x)$ in $K$ be $\\alpha_1, \\alpha_2, \\alpha_3$. The problem states these roots are distinct, so $p(x)$ is separable and the discriminant $\\Delta \\neq 0$.\nThe Galois group of $p(x)$ over $F$ is $G = \\text{Gal}(K/F)$. Since $G$ permutes the three roots, $G$ is isomorphic to a subgroup of the symmetric group $S_3$.\n\n**Assumption regarding Characteristic:**\nWe assume that the characteristic of $F$ is not 2.\n*Reasoning:* The relationship between the discriminant being a square and the Galois group being a subgroup of the alternating group $A_3$ relies on the property that for distinct roots, the square root of the discriminant $\\delta = \\prod_{i<j}(\\alpha_i - \\alpha_j)$ satisfies $\\delta \\neq -\\delta$. In characteristic 2, this distinction vanishes, and the discriminant is always a square in $F$, rendering the condition in the problem statement trivial and the equivalence false (e.g., an irreducible cubic in characteristic 2 can have Galois group $S_3$).\n\n### Step 1: The Discriminant and the Alternating Group\n\nThe discriminant of $p(x)$ is given by $\\Delta = \\prod_{1 \\le i < j \\le 3} (\\alpha_i - \\alpha_j)^2$.\nLet $\\delta = \\prod_{1 \\le i < j \\le 3} (\\alpha_i - \\alpha_j)$. Then $\\delta^2 = \\Delta$.\nFor any $\\sigma \\in G$, the action of $\\sigma$ on $\\delta$ is given by:\n$$ \\sigma(\\delta) = \\text{sgn}(\\sigma) \\delta $$\nwhere $\\text{sgn}(\\sigma)$ is the sign of the permutation ($1$ for even permutations, $-1$ for odd permutations).\n\n**Hypothesis:** The discriminant $\\Delta$ is a square in $F$.\nThis implies $\\delta \\in F$.\nSince $\\delta \\in F$, it is fixed by all automorphisms in the Galois group:\n$$ \\forall \\sigma \\in G, \\quad \\sigma(\\delta) = \\delta $$\nCombining this with the sign property:\n$$ \\text{sgn}(\\sigma) \\delta = \\delta $$\nSince roots are distinct, $\\Delta \\neq 0$, so $\\delta \\neq 0$. In characteristic $\\neq 2$, $\\delta \\neq -\\delta$. Therefore:\n$$ \\text{sgn}(\\sigma) = 1 \\quad \\forall \\sigma \\in G $$\nThis implies that $G$ consists only of even permutations. Thus, $G$ is a subgroup of the alternating group $A_3$.\n\nThe subgroups of $S_3$ contained in $A_3$ are:\n1.  The trivial group $\\{e\\}$ (Order 1).\n2.  The alternating group $A_3 = \\{e, (123), (132)\\}$ (Order 3). Note that $A_3 \\cong \\mathbb{Z}/3\\mathbb{Z}$.\n\nThus, given the discriminant condition, we have established that $G \\cong \\{e\\}$ or $G \\cong \\mathbb{Z}/3\\mathbb{Z}$.\n\n### Step 2: Proof of Equivalence\n\nWe must prove: $G \\cong \\mathbb{Z}/3\\mathbb{Z} \\iff p(x)$ is irreducible over $F$.\n\n#### Part A: ($p(x)$ is irreducible $\\implies G \\cong \\mathbb{Z}/3\\mathbb{Z}$)\n\n1.  Assume $p(x)$ is irreducible over $F$.\n2.  Let $\\alpha_1$ be a root of $p(x)$. Since $p(x)$ is irreducible, the degree of the extension $F(\\alpha_1)/F$ is equal to the degree of the polynomial:\n    $$ [F(\\alpha_1) : F] = 3 $$\n3.  Since $K$ is the splitting field, $F \\subseteq F(\\alpha_1) \\subseteq K$. By the tower law, $[K:F] = [K:F(\\alpha_1)][F(\\alpha_1):F]$.\n4.  Therefore, $[F(\\alpha_1):F]$ divides $[K:F]$. Since $|G| = [K:F]$, we have $3 \\mid |G|$.\n5.  From Step 1, we know $G \\subseteq A_3$, so $|G|$ must divide $|A_3| = 3$.\n6.  The only positive integer that divides 3 and is divisible by 3 is 3.\n7.  Thus, $|G| = 3$. The only group of order 3 is the cyclic group $\\mathbb{Z}/3\\mathbb{Z}$.\n    $$ G \\cong \\mathbb{Z}/3\\mathbb{Z} $$\n\n#### Part B: ($G \\cong \\mathbb{Z}/3\\mathbb{Z} \\implies p(x)$ is irreducible)\n\n1.  Assume $G \\cong \\mathbb{Z}/3\\mathbb{Z}$. The elements of $G$ viewed as permutations of roots $\\{\\alpha_1, \\alpha_2, \\alpha_3\\}$ are $\\{e, (123), (132)\\}$.\n2.  The group $G$ acts transitively on the set of roots. (The orbit of any root $\\alpha_i$ is the entire set $\\{\\alpha_1, \\alpha_2, \\alpha_3\\}$).\n3.  **Transitivity implies irreducibility:**\n    If $p(x)$ were reducible over $F$, it would factor into polynomials of lower degree in $F[x]$.\n    Let $p(x) = f(x)g(x)$ with $\\deg(f) \\ge 1, \\deg(g) \\ge 1$.\n    The roots of $f(x)$ would form a subset of $\\{\\alpha_1, \\alpha_2, \\alpha_3\\}$ that is mapped to itself by all $\\sigma \\in G$ (since $\\sigma$ fixes coefficients in $F$, it maps roots of $f$ to roots of $f$).\n    This would imply the action of $G$ on the roots is not transitive (there would be an orbit of size $\\deg(f) < 3$).\n4.  Since the action is transitive, $p(x)$ cannot satisfy the condition of reducibility.\n    (Alternatively: The orbit of $\\alpha_1$ is the set of roots of its minimal polynomial. The size of the orbit is 3, so the minimal polynomial has degree 3. Since $p(x)$ is monic of degree 3 and shares a root with this minimal polynomial, $p(x)$ is the minimal polynomial, hence irreducible).\n5.  Therefore, $p(x)$ is irreducible over $F$.\n\n### Conclusion\n\nWe have shown that if the discriminant of the cubic polynomial $p(x)$ is a square in $F$ (and char $F \\neq 2$), then $G$ is a subgroup of $A_3$. Under this condition:\n- If $p(x)$ is irreducible, $|G|=3$, so $G \\cong \\mathbb{Z}/3\\mathbb{Z}$.\n- If $G \\cong \\mathbb{Z}/3\\mathbb{Z}$, the transitive action on roots implies $p(x)$ is irreducible.\n\nThus, $\\text{Gal}(p(x)/F) \\cong \\mathbb{Z}/3\\mathbb{Z}$ if and only if $p(x)$ is irreducible over $F$.",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ]
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "## Step 1: Understand the task\nThe task is to generate a challenging mathematics problem in the domain of commutative algebra, along with a complete and verifiable solution. The problem must be well-posed, clear, non-trivial, and solvable with established mathematical methods.\n\n## Step 2: Identify a suitable topic in commutative algebra\nCommutative algebra involves the study of commutative rings and their ideals, modules, and algebras. A challenging problem could involve understanding the properties of ideals, quotient rings, or the behavior of polynomials over certain rings.\n\n## Step 3: Formulate a specific problem\nLet's consider a problem involving the properties of ideals in a polynomial ring. Specifically, we can examine the ideal generated by a set of polynomials and ask questions about its properties, such as whether it is prime or maximal.\n\n## Step 4: Construct the problem\nConsider the polynomial ring $\\mathbb{Q}[x, y]$ and the ideal $I = (x^2 + y^2, xy)$. We can ask: Is the ideal $I$ prime? To make it more challenging and ensure it's well-posed, we can ask to determine whether $I$ is radical, i.e., whether it is equal to its radical $\\sqrt{I}$.\n\n## Step 5: Write down the problem statement\nLet $I = (x^2 + y^2, xy)$ be an ideal in $\\mathbb{Q}[x, y]$. Determine whether $I = \\sqrt{I}$, where $\\sqrt{I} = \\{f \\in \\mathbb{Q}[x, y] \\mid f^n \\in I \\text{ for some } n \\in \\mathbb{N}\\}$.\n\n## Step 6: Provide the solution\nTo determine if $I = \\sqrt{I}$, we first need to understand what $\\sqrt{I}$ is. An element $f \\in \\mathbb{Q}[x, y]$ is in $\\sqrt{I}$ if $f^n \\in I$ for some $n$. We need to check if there are elements in $\\sqrt{I}$ that are not in $I$.\n\n## Step 7: Analyze the ideal $I$\nTo check if $I = \\sqrt{I}$, let's first examine the quotient ring $\\mathbb{Q}[x, y]/I$. The ideal $I$ contains $x^2 + y^2$ and $xy$, which implies that $x^2 + y^2$ and $xy$ are zero in $\\mathbb{Q}[x, y]/I$. We can simplify this by observing that $x^2y + y^3 = 0$ and $x^2y = 0$ in $\\mathbb{Q}[x, y]/I$, implying $y^3 = 0$ in $\\mathbb{Q}[x, y]/I$. Similarly, $x^3 = 0$.\n\n## Step 8: Check if $I$ is radical\nSince $x^3 = 0$ and $y^3 = 0$ in $\\mathbb{Q}[x, y]/I$, it follows that $x, y \\in \\sqrt{I}$ because $x^3, y^3 \\in I$. We need to verify if $x$ and $y$ are in $I$. If not, then $I \\neq \\sqrt{I}$.\n\n## Step 9: Verify if $x$ and $y$ are in $I$\nTo check if $x \\in I$, we need to see if there exist $f, g \\in \\mathbb{Q}[x, y]$ such that $x = f(x^2 + y^2) + g(xy)$. This simplifies to checking if $x$ can be expressed as a combination of $x^2 + y^2$ and $xy$, which is not possible because any element in $I$ is of degree at least 2.\n\n## Step 10: Conclude on $I = \\sqrt{I}$\nSince $x, y \\in \\sqrt{I}$ but $x, y \\notin I$, it follows that $I \\neq \\sqrt{I}$. Thus, $I$ is not radical.\n\nThe final answer is: \n\nLet $I = (x^2 + y^2, xy)$ be an ideal in $\\mathbb{Q}[x, y]$. Determine whether $I = \\sqrt{I}$, where $\\sqrt{I} = \\{f \\in \\mathbb{Q}[x, y] \\mid f^n \\in I \\text{ for some } n \\in \\mathbb{N}\\}$.",
    "run_id": "7",
    "topic_slug": "commutative_algebra",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "To determine whether the ideal $I = (x^2 + y^2, xy)$ in the polynomial ring $R = \\mathbb{Q}[x, y]$ is equal to its radical $\\sqrt{I}$, we must verify if the property $f^n \\in I \\implies f \\in I$ holds for all $f \\in R$.\n\nIf we can find a polynomial $f$ such that $f^n \\in I$ for some integer $n \\geq 1$ but $f \\notin I$, then $I \\neq \\sqrt{I}$.\n\n### Step 1: Analyze the Generators and Degree Structure of $I$\n\nThe ideal $I$ is generated by two polynomials:\n1. $g_1 = x^2 + y^2$\n2. $g_2 = xy$\n\nBoth generators are homogeneous polynomials of degree 2. Let $f$ be an arbitrary non-zero element of $I$. By definition, there exist polynomials $A, B \\in \\mathbb{Q}[x, y]$ such that:\n$$f(x, y) = A(x, y)(x^2 + y^2) + B(x, y)(xy)$$\n\nConsider the degree of any non-zero term in $f$.\n- The degree of any term in $A(x, y)(x^2 + y^2)$ is at least $0 + 2 = 2$.\n- The degree of any term in $B(x, y)(xy)$ is at least $0 + 2 = 2$.\n\nConsequently, any non-zero polynomial in $I$ must have a total degree of at least 2. In particular, $I$ contains no linear polynomials (degree 1) and no non-zero constants (degree 0).\n\n### Step 2: Identify a Candidate for the Radical\n\nLet us test the polynomial $h(x, y) = x + y$.\nIf $I$ were a radical ideal, then $h^2 \\in I$ would imply $h \\in I$.\n\nFirst, we calculate $h^2$:\n$$h^2 = (x + y)^2 = x^2 + 2xy + y^2$$\nWe can regroup the terms as follows:\n$$h^2 = (x^2 + y^2) + 2(xy)$$\n\nObserve the components of this expression:\n1. $x^2 + y^2$ is the first generator of $I$, so $x^2 + y^2 \\in I$.\n2. $xy$ is the second generator of $I$, and since $I$ is an ideal, $2(xy) \\in I$.\n\nSince $I$ is closed under addition, their sum must be in $I$:\n$$(x^2 + y^2) + 2(xy) \\in I \\implies (x + y)^2 \\in I$$\n\n### Step 3: Verify if the Candidate is in $I$\n\nWe have established that $(x + y)^2 \\in I$, which implies $x + y \\in \\sqrt{I}$.\nNow we check if $x + y \\in I$.\n\nThe polynomial $x + y$ has a total degree of 1. As established in Step 1, every non-zero polynomial in $I$ has a degree of at least 2. Therefore:\n$$x + y \\notin I$$\n\n### Step 4: Conclusion\n\nWe have found an element $h = x + y$ such that $h^2 \\in I$ but $h \\notin I$. By definition, a radical ideal must contain all its roots (i.e., if a power of an element is in the ideal, the element itself must be in the ideal). Since this condition fails for $I$, $I$ is not a radical ideal.\n\n**Final Answer:**\nNo, $I \\neq \\sqrt{I}$. The ideal $I = (x^2 + y^2, xy)$ is not radical because the element $x + y$ belongs to $\\sqrt{I}$ (as $(x+y)^2 \\in I$) but does not belong to $I$ (as it has degree 1, while all non-zero elements of $I$ have degree $\\geq 2$).",
        "raw_answer": "To determine whether the ideal $I = (x^2 + y^2, xy)$ in the polynomial ring $R = \\mathbb{Q}[x, y]$ is equal to its radical $\\sqrt{I}$, we must verify if the property $f^n \\in I \\implies f \\in I$ holds for all $f \\in R$.\n\nIf we can find a polynomial $f$ such that $f^n \\in I$ for some integer $n \\geq 1$ but $f \\notin I$, then $I \\neq \\sqrt{I}$.\n\n### Step 1: Analyze the Generators and Degree Structure of $I$\n\nThe ideal $I$ is generated by two polynomials:\n1. $g_1 = x^2 + y^2$\n2. $g_2 = xy$\n\nBoth generators are homogeneous polynomials of degree 2. Let $f$ be an arbitrary non-zero element of $I$. By definition, there exist polynomials $A, B \\in \\mathbb{Q}[x, y]$ such that:\n$$f(x, y) = A(x, y)(x^2 + y^2) + B(x, y)(xy)$$\n\nConsider the degree of any non-zero term in $f$.\n- The degree of any term in $A(x, y)(x^2 + y^2)$ is at least $0 + 2 = 2$.\n- The degree of any term in $B(x, y)(xy)$ is at least $0 + 2 = 2$.\n\nConsequently, any non-zero polynomial in $I$ must have a total degree of at least 2. In particular, $I$ contains no linear polynomials (degree 1) and no non-zero constants (degree 0).\n\n### Step 2: Identify a Candidate for the Radical\n\nLet us test the polynomial $h(x, y) = x + y$.\nIf $I$ were a radical ideal, then $h^2 \\in I$ would imply $h \\in I$.\n\nFirst, we calculate $h^2$:\n$$h^2 = (x + y)^2 = x^2 + 2xy + y^2$$\nWe can regroup the terms as follows:\n$$h^2 = (x^2 + y^2) + 2(xy)$$\n\nObserve the components of this expression:\n1. $x^2 + y^2$ is the first generator of $I$, so $x^2 + y^2 \\in I$.\n2. $xy$ is the second generator of $I$, and since $I$ is an ideal, $2(xy) \\in I$.\n\nSince $I$ is closed under addition, their sum must be in $I$:\n$$(x^2 + y^2) + 2(xy) \\in I \\implies (x + y)^2 \\in I$$\n\n### Step 3: Verify if the Candidate is in $I$\n\nWe have established that $(x + y)^2 \\in I$, which implies $x + y \\in \\sqrt{I}$.\nNow we check if $x + y \\in I$.\n\nThe polynomial $x + y$ has a total degree of 1. As established in Step 1, every non-zero polynomial in $I$ has a degree of at least 2. Therefore:\n$$x + y \\notin I$$\n\n### Step 4: Conclusion\n\nWe have found an element $h = x + y$ such that $h^2 \\in I$ but $h \\notin I$. By definition, a radical ideal must contain all its roots (i.e., if a power of an element is in the ideal, the element itself must be in the ideal). Since this condition fails for $I$, $I$ is not a radical ideal.\n\n**Final Answer:**\nNo, $I \\neq \\sqrt{I}$. The ideal $I = (x^2 + y^2, xy)$ is not radical because the element $x + y$ belongs to $\\sqrt{I}$ (as $(x+y)^2 \\in I$) but does not belong to $I$ (as it has degree 1, while all non-zero elements of $I$ have degree $\\geq 2$).",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ],
    "outer_attempt": 1
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "## Step 1: Understand the task and the domain\nThe task is to generate a challenging but solvable mathematics problem in the domain of algebraic geometry, along with a complete and verifiable solution. The problem should be well-posed, clear, non-trivial, and solvable with established mathematical methods.\n\n## Step 2: Identify a suitable topic within algebraic geometry\nAlgebraic geometry is a broad field that involves the study of geometric objects, such as varieties and schemes, using tools from abstract algebra. A suitable topic could involve the study of curves, surfaces, or higher-dimensional varieties, and their properties.\n\n## Step 3: Formulate a specific problem\nLet's consider a problem involving the intersection of curves on a surface. A classic result in algebraic geometry is B\u00e9zout's theorem, which describes the intersection of two curves on a projective plane. We can formulate a problem related to this.\n\n## Step 4: State the problem\nConsider two curves $C_1$ and $C_2$ on $\\mathbb{P}^2$, defined by homogeneous polynomials $f(x,y,z) = x^2 + y^2 - z^2$ and $g(x,y,z) = xy - z^2$, respectively. Find the number of intersection points of $C_1$ and $C_2$ in $\\mathbb{P}^2$, counted with multiplicity.\n\n## Step 5: Provide the solution\nTo find the intersection points, we need to solve the system of equations given by $f(x,y,z) = 0$ and $g(x,y,z) = 0$. This means we have to solve $x^2 + y^2 - z^2 = 0$ and $xy - z^2 = 0$ simultaneously.\n\n## Step 6: Solve the system of equations\nFirst, from $xy - z^2 = 0$, we can express $z^2 = xy$. Substituting $z^2$ into the first equation gives $x^2 + y^2 - xy = 0$. This is a homogeneous equation in $x$ and $y$, which can be solved by considering it as a quadratic form.\n\n## Step 7: Analyze the quadratic form\nThe equation $x^2 - xy + y^2 = 0$ can be analyzed by finding its roots. This is equivalent to solving $x^2 - xy + y^2 = 0$ for $x$ in terms of $y$ (or vice versa), treating it as a quadratic equation.\n\n## Step 8: Solve for $x$ in terms of $y$\nUsing the quadratic formula, $x = \\frac{y \\pm \\sqrt{y^2 - 4y^2}}{2} = \\frac{y \\pm \\sqrt{-3y^2}}{2}$. For $y \\neq 0$, this simplifies to $x = \\frac{y(1 \\pm i\\sqrt{3})}{2}$. Thus, $x = y\\omega$ or $x = y\\omega^2$, where $\\omega$ is a primitive third root of unity.\n\n## Step 9: Determine the intersection points\nFor $x = y\\omega$ and $x = y\\omega^2$, substituting back into $xy = z^2$ gives $y^2\\omega = z^2$ and $y^2\\omega^2 = z^2$, respectively. This implies $z = \\pm y\\sqrt{\\omega}$ and $z = \\pm y\\sqrt{\\omega^2}$. However, we must consider the projective nature and the fact that we're working with homogeneous coordinates.\n\n## Step 10: Count the intersection points with multiplicity\nSince $\\omega$ and $\\omega^2$ are not real, and we're working over $\\mathbb{C}$, we need to consider the solutions in $\\mathbb{P}^2(\\mathbb{C})$. The solutions are given by the pairs $(x:y:z)$ satisfying both equations. We need to check if there are any solutions where $y = 0$, which would imply $x = 0$ and thus $z = 0$, but $(0,0,0)$ is not a valid point in $\\mathbb{P}^2$.\n\n## Step 11: Apply B\u00e9zout's theorem for a simpler solution\nB\u00e9zout's theorem states that two curves of degrees $d_1$ and $d_2$ intersect in $d_1d_2$ points, counted with multiplicity, if they have no common components. Here, $C_1$ and $C_2$ are both of degree 2, so they should intersect in $2 \\times 2 = 4$ points.\n\nThe final answer is: \n\nConsider two curves $C_1$ and $C_2$ on $\\mathbb{P}^2$, defined by homogeneous polynomials $f(x,y,z) = x^2 + y^2 - z^2$ and $g(x,y,z) = xy - z^2$, respectively. Find the number of intersection points of $C_1$ and $C_2$ in $\\mathbb{P}^2$, counted with multiplicity.",
    "run_id": "8",
    "outer_attempt": 1,
    "topic_slug": "algebraic_geometry",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "To find the number of intersection points of the curves $C_1$ and $C_2$ in the complex projective plane $\\mathbb{P}^2$, counted with multiplicity, we will analyze the system of homogeneous equations defined by the polynomials $f$ and $g$.\n\n### 1. Problem Statement\nLet the two curves be defined by the homogeneous polynomials:\n$$f(x,y,z) = x^2 + y^2 - z^2 = 0$$\n$$g(x,y,z) = xy - z^2 = 0$$\nWe seek the number of solutions to this system in $\\mathbb{P}^2$, counted with intersection multiplicity. We assume the ground field is the field of complex numbers $\\mathbb{C}$, as is standard for intersection theory in algebraic geometry.\n\n### 2. Application of B\u00e9zout's Theorem\nB\u00e9zout's Theorem states that for two plane projective curves of degrees $d_1$ and $d_2$ defined over an algebraically closed field, if they do not share a common component, the number of intersection points counted with multiplicity is exactly $d_1 \\cdot d_2$.\n\n**Checking Degrees:**\n- $f(x,y,z)$ is a polynomial of degree $d_1 = 2$.\n- $g(x,y,z)$ is a polynomial of degree $d_2 = 2$.\n\n**Checking for Common Components:**\nBoth $f$ and $g$ define conics.\n- $g(x,y,z) = xy - z^2$ is an irreducible quadratic form (it corresponds to a non-degenerate conic, specifically identifiable as the Veronese embedding of $\\mathbb{P}^1$ or simply by noting it cannot be factored into linear terms).\n- $f(x,y,z) = x^2 + y^2 - z^2$ is also irreducible.\n- For them to share a common component, since they are both irreducible and of the same degree, they would have to be proportional. That is, $f = k \\cdot g$ for some constant $k$. Clearly, $x^2 + y^2 - z^2$ is not a scalar multiple of $xy - z^2$.\n\nThus, the curves have no common components. By B\u00e9zout's Theorem, the number of intersection points is:\n$$N = d_1 \\cdot d_2 = 2 \\cdot 2 = 4$$\n\n### 3. Explicit Verification of Intersection Points\nTo ensure rigor and verify that the multiplicities behave as expected (i.e., we don't have tangencies summing to 4, but rather 4 distinct points), we solve the system explicitly.\n\nThe system is:\n1. $x^2 + y^2 = z^2$\n2. $xy = z^2$\n\nSubstitute equation (2) into equation (1):\n$$x^2 + y^2 = xy$$\n$$x^2 - xy + y^2 = 0$$\n\nWe look for solutions $[x:y:z]$ in $\\mathbb{P}^2$. Note that $(0,0,0)$ is not a valid projective point.\nIf $y = 0$, the equation $x^2 - x(0) + 0 = 0$ implies $x = 0$. Then $z^2 = xy = 0 \\implies z=0$. Since $[0:0:0]$ is not valid, we must have $y \\neq 0$.\n\nSince $y \\neq 0$, we can dehomogenize by setting $y = 1$. The equation becomes:\n$$x^2 - x + 1 = 0$$\n\nThe roots of this quadratic equation are:\n$$x = \\frac{1 \\pm \\sqrt{1 - 4}}{2} = \\frac{1 \\pm i\\sqrt{3}}{2}$$\nLet $\\omega = e^{i\\pi/3} = \\frac{1 + i\\sqrt{3}}{2}$ and $\\bar{\\omega} = e^{-i\\pi/3} = \\frac{1 - i\\sqrt{3}}{2}$.\nSo, $x \\in \\{\\omega, \\bar{\\omega}\\}$.\n\nNow we solve for $z$ using $z^2 = xy$. Since $y=1$, $z^2 = x$.\n\n**Case 1: $x = \\omega$**\n$$z^2 = \\omega$$\nThis yields two distinct solutions for $z$: $z_1 = \\sqrt{\\omega}$ and $z_2 = -\\sqrt{\\omega}$.\nThis gives two intersection points:\n$$P_1 = [\\omega : 1 : \\sqrt{\\omega}]$$\n$$P_2 = [\\omega : 1 : -\\sqrt{\\omega}]$$\n\n**Case 2: $x = \\bar{\\omega}$**\n$$z^2 = \\bar{\\omega}$$\nThis yields two distinct solutions for $z$: $z_3 = \\sqrt{\\bar{\\omega}}$ and $z_4 = -\\sqrt{\\bar{\\omega}}$.\nThis gives two intersection points:\n$$P_3 = [\\bar{\\omega} : 1 : \\sqrt{\\bar{\\omega}}]$$\n$$P_4 = [\\bar{\\omega} : 1 : -\\sqrt{\\bar{\\omega}}]$$\n\n**Analysis of Multiplicity:**\nWe have found 4 distinct points ($P_1, P_2, P_3, P_4$).\nSince the sum of multiplicities must be 4 (by B\u00e9zout's Theorem), and there are 4 distinct solutions, each solution must have multiplicity exactly 1. This means the curves intersect transversally at all 4 points.\n\n### 4. Conclusion\nThe curves $C_1$ and $C_2$ intersect at exactly 4 distinct points. Summing their multiplicities:\n\n$$ \\sum_{P \\in C_1 \\cap C_2} m_P(C_1, C_2) = 4 $$\n\n**Answer:**\nThe number of intersection points of $C_1$ and $C_2$ in $\\mathbb{P}^2$, counted with multiplicity, is **4**.",
        "raw_answer": "To find the number of intersection points of the curves $C_1$ and $C_2$ in the complex projective plane $\\mathbb{P}^2$, counted with multiplicity, we will analyze the system of homogeneous equations defined by the polynomials $f$ and $g$.\n\n### 1. Problem Statement\nLet the two curves be defined by the homogeneous polynomials:\n$$f(x,y,z) = x^2 + y^2 - z^2 = 0$$\n$$g(x,y,z) = xy - z^2 = 0$$\nWe seek the number of solutions to this system in $\\mathbb{P}^2$, counted with intersection multiplicity. We assume the ground field is the field of complex numbers $\\mathbb{C}$, as is standard for intersection theory in algebraic geometry.\n\n### 2. Application of B\u00e9zout's Theorem\nB\u00e9zout's Theorem states that for two plane projective curves of degrees $d_1$ and $d_2$ defined over an algebraically closed field, if they do not share a common component, the number of intersection points counted with multiplicity is exactly $d_1 \\cdot d_2$.\n\n**Checking Degrees:**\n- $f(x,y,z)$ is a polynomial of degree $d_1 = 2$.\n- $g(x,y,z)$ is a polynomial of degree $d_2 = 2$.\n\n**Checking for Common Components:**\nBoth $f$ and $g$ define conics.\n- $g(x,y,z) = xy - z^2$ is an irreducible quadratic form (it corresponds to a non-degenerate conic, specifically identifiable as the Veronese embedding of $\\mathbb{P}^1$ or simply by noting it cannot be factored into linear terms).\n- $f(x,y,z) = x^2 + y^2 - z^2$ is also irreducible.\n- For them to share a common component, since they are both irreducible and of the same degree, they would have to be proportional. That is, $f = k \\cdot g$ for some constant $k$. Clearly, $x^2 + y^2 - z^2$ is not a scalar multiple of $xy - z^2$.\n\nThus, the curves have no common components. By B\u00e9zout's Theorem, the number of intersection points is:\n$$N = d_1 \\cdot d_2 = 2 \\cdot 2 = 4$$\n\n### 3. Explicit Verification of Intersection Points\nTo ensure rigor and verify that the multiplicities behave as expected (i.e., we don't have tangencies summing to 4, but rather 4 distinct points), we solve the system explicitly.\n\nThe system is:\n1. $x^2 + y^2 = z^2$\n2. $xy = z^2$\n\nSubstitute equation (2) into equation (1):\n$$x^2 + y^2 = xy$$\n$$x^2 - xy + y^2 = 0$$\n\nWe look for solutions $[x:y:z]$ in $\\mathbb{P}^2$. Note that $(0,0,0)$ is not a valid projective point.\nIf $y = 0$, the equation $x^2 - x(0) + 0 = 0$ implies $x = 0$. Then $z^2 = xy = 0 \\implies z=0$. Since $[0:0:0]$ is not valid, we must have $y \\neq 0$.\n\nSince $y \\neq 0$, we can dehomogenize by setting $y = 1$. The equation becomes:\n$$x^2 - x + 1 = 0$$\n\nThe roots of this quadratic equation are:\n$$x = \\frac{1 \\pm \\sqrt{1 - 4}}{2} = \\frac{1 \\pm i\\sqrt{3}}{2}$$\nLet $\\omega = e^{i\\pi/3} = \\frac{1 + i\\sqrt{3}}{2}$ and $\\bar{\\omega} = e^{-i\\pi/3} = \\frac{1 - i\\sqrt{3}}{2}$.\nSo, $x \\in \\{\\omega, \\bar{\\omega}\\}$.\n\nNow we solve for $z$ using $z^2 = xy$. Since $y=1$, $z^2 = x$.\n\n**Case 1: $x = \\omega$**\n$$z^2 = \\omega$$\nThis yields two distinct solutions for $z$: $z_1 = \\sqrt{\\omega}$ and $z_2 = -\\sqrt{\\omega}$.\nThis gives two intersection points:\n$$P_1 = [\\omega : 1 : \\sqrt{\\omega}]$$\n$$P_2 = [\\omega : 1 : -\\sqrt{\\omega}]$$\n\n**Case 2: $x = \\bar{\\omega}$**\n$$z^2 = \\bar{\\omega}$$\nThis yields two distinct solutions for $z$: $z_3 = \\sqrt{\\bar{\\omega}}$ and $z_4 = -\\sqrt{\\bar{\\omega}}$.\nThis gives two intersection points:\n$$P_3 = [\\bar{\\omega} : 1 : \\sqrt{\\bar{\\omega}}]$$\n$$P_4 = [\\bar{\\omega} : 1 : -\\sqrt{\\bar{\\omega}}]$$\n\n**Analysis of Multiplicity:**\nWe have found 4 distinct points ($P_1, P_2, P_3, P_4$).\nSince the sum of multiplicities must be 4 (by B\u00e9zout's Theorem), and there are 4 distinct solutions, each solution must have multiplicity exactly 1. This means the curves intersect transversally at all 4 points.\n\n### 4. Conclusion\nThe curves $C_1$ and $C_2$ intersect at exactly 4 distinct points. Summing their multiplicities:\n\n$$ \\sum_{P \\in C_1 \\cap C_2} m_P(C_1, C_2) = 4 $$\n\n**Answer:**\nThe number of intersection points of $C_1$ and $C_2$ in $\\mathbb{P}^2$, counted with multiplicity, is **4**.",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ]
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "## Step 1: Understand the task and the domain\nThe task is to generate a challenging but solvable mathematics problem in the domain of Linear and multilinear algebra; matrix theory, along with a complete and verifiable solution.\n\n## Step 2: Identify the key elements of a well-posed problem\nThe problem must be fully self-contained, clear, non-trivial, solvable, and verifiable. It should involve sophisticated techniques from linear and multilinear algebra or matrix theory.\n\n## Step 3: Generate a problem that fits the criteria\nLet's consider a problem that involves matrix theory and requires a good understanding of linear algebra concepts. A potential problem could involve proving a property of matrices or finding a specific matrix that satisfies certain conditions.\n\n## Step 4: Formulate the problem\nConsider the following problem: \"Let $A$ be a $3 \\times 3$ real matrix such that $A^3 = I$, where $I$ is the $3 \\times 3$ identity matrix. Prove that $A$ is diagonalizable over $\\mathbb{C}$.\"\n\n## Step 5: Verify that the problem is well-posed and solvable\nThe problem is well-posed because it specifies the domain of $A$ (real $3 \\times 3$ matrices), gives a clear condition ($A^3 = I$), and asks for a specific proof (that $A$ is diagonalizable over $\\mathbb{C}$). It is solvable using established methods in linear algebra.\n\n## Step 6: Provide a complete solution to the problem\nTo solve this, we note that $A$ satisfies the polynomial $x^3 - 1 = 0$. This polynomial factors as $(x-1)(x^2 + x + 1) = 0$. The roots are $1$, $e^{2\\pi i/3}$, and $e^{4\\pi i/3}$. Since $A$ satisfies this polynomial, its eigenvalues must be among these roots. Because the minimal polynomial of $A$ divides $x^3 - 1$ and has distinct roots (since $x^3 - 1$ has distinct roots), $A$ is diagonalizable over $\\mathbb{C}$.\n\n## Step 7: Write the problem and solution in the required format\n\nLet $A$ be a $3 \\times 3$ real matrix such that $A^3 = I$, where $I$ is the $3 \\times 3$ identity matrix. Prove that $A$ is diagonalizable over $\\mathbb{C}$.",
    "run_id": "9",
    "topic_slug": "linear_and_multilinear_algebra",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "**Proof**\n\nLet $A$ be a $3 \\times 3$ real matrix satisfying $A^3 = I$. We aim to prove that $A$ is diagonalizable over the complex numbers $\\mathbb{C}$.\n\n**1. Identification of an Annihilating Polynomial**\nSince $A^3 = I$, the matrix $A$ satisfies the polynomial equation:\n$$ A^3 - I = 0 $$\nLet $P(x) = x^3 - 1$. Then $P(A) = 0$. This makes $P(x)$ an annihilating polynomial for $A$.\n\n**2. Factorization of the Polynomial**\nWe factor $P(x)$ over the field $\\mathbb{C}$. The roots of $x^3 - 1 = 0$ are the cube roots of unity.\n$$ P(x) = (x - 1)(x^2 + x + 1) $$\nUsing the quadratic formula on the second factor, the roots are:\n$$ x = 1, \\quad x = \\frac{-1 + i\\sqrt{3}}{2}, \\quad x = \\frac{-1 - i\\sqrt{3}}{2} $$\nLet $\\omega = \\frac{-1 + i\\sqrt{3}}{2}$. Then the roots are $1, \\omega, \\text{ and } \\bar{\\omega}$ (which is $\\omega^2$).\nSince $\\omega \\in \\mathbb{C} \\setminus \\mathbb{R}$, the three roots $\\{1, \\omega, \\omega^2\\}$ are distinct.\nThus, $P(x)$ factors into distinct linear terms over $\\mathbb{C}$:\n$$ P(x) = (x - 1)(x - \\omega)(x - \\omega^2) $$\n\n**3. Analysis of the Minimal Polynomial**\nLet $\\mu_A(x)$ denote the minimal polynomial of $A$ over $\\mathbb{C}$.\nBy the properties of the minimal polynomial:\n1.  $\\mu_A(x)$ must divide any annihilating polynomial of $A$. Therefore, $\\mu_A(x)$ divides $P(x)$.\n2.  Every root of $\\mu_A(x)$ is a root of $P(x)$.\n\nSince $P(x)$ has distinct roots (it is square-free), any polynomial dividing it must also have distinct roots. Consequently, $\\mu_A(x)$ has distinct roots (no repeated factors).\n\n**4. Application of the Diagonalizability Criterion**\nA well-known theorem in linear algebra states:\n> A square matrix $A$ is diagonalizable over a field $F$ if and only if its minimal polynomial splits into distinct linear factors over $F$.\n\nHere, $F = \\mathbb{C}$. We established that $\\mu_A(x)$ divides $P(x) = (x - 1)(x - \\omega)(x - \\omega^2)$. Since the factors of $P(x)$ are distinct and linear, $\\mu_A(x)$ is a product of distinct linear factors.\n\n**Conclusion**\nBecause the minimal polynomial of $A$ has no repeated roots, $A$ is diagonalizable over $\\mathbb{C}$.",
        "raw_answer": "**Proof**\n\nLet $A$ be a $3 \\times 3$ real matrix satisfying $A^3 = I$. We aim to prove that $A$ is diagonalizable over the complex numbers $\\mathbb{C}$.\n\n**1. Identification of an Annihilating Polynomial**\nSince $A^3 = I$, the matrix $A$ satisfies the polynomial equation:\n$$ A^3 - I = 0 $$\nLet $P(x) = x^3 - 1$. Then $P(A) = 0$. This makes $P(x)$ an annihilating polynomial for $A$.\n\n**2. Factorization of the Polynomial**\nWe factor $P(x)$ over the field $\\mathbb{C}$. The roots of $x^3 - 1 = 0$ are the cube roots of unity.\n$$ P(x) = (x - 1)(x^2 + x + 1) $$\nUsing the quadratic formula on the second factor, the roots are:\n$$ x = 1, \\quad x = \\frac{-1 + i\\sqrt{3}}{2}, \\quad x = \\frac{-1 - i\\sqrt{3}}{2} $$\nLet $\\omega = \\frac{-1 + i\\sqrt{3}}{2}$. Then the roots are $1, \\omega, \\text{ and } \\bar{\\omega}$ (which is $\\omega^2$).\nSince $\\omega \\in \\mathbb{C} \\setminus \\mathbb{R}$, the three roots $\\{1, \\omega, \\omega^2\\}$ are distinct.\nThus, $P(x)$ factors into distinct linear terms over $\\mathbb{C}$:\n$$ P(x) = (x - 1)(x - \\omega)(x - \\omega^2) $$\n\n**3. Analysis of the Minimal Polynomial**\nLet $\\mu_A(x)$ denote the minimal polynomial of $A$ over $\\mathbb{C}$.\nBy the properties of the minimal polynomial:\n1.  $\\mu_A(x)$ must divide any annihilating polynomial of $A$. Therefore, $\\mu_A(x)$ divides $P(x)$.\n2.  Every root of $\\mu_A(x)$ is a root of $P(x)$.\n\nSince $P(x)$ has distinct roots (it is square-free), any polynomial dividing it must also have distinct roots. Consequently, $\\mu_A(x)$ has distinct roots (no repeated factors).\n\n**4. Application of the Diagonalizability Criterion**\nA well-known theorem in linear algebra states:\n> A square matrix $A$ is diagonalizable over a field $F$ if and only if its minimal polynomial splits into distinct linear factors over $F$.\n\nHere, $F = \\mathbb{C}$. We established that $\\mu_A(x)$ divides $P(x) = (x - 1)(x - \\omega)(x - \\omega^2)$. Since the factors of $P(x)$ are distinct and linear, $\\mu_A(x)$ is a product of distinct linear factors.\n\n**Conclusion**\nBecause the minimal polynomial of $A$ has no repeated roots, $A$ is diagonalizable over $\\mathbb{C}$.",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ],
    "outer_attempt": 1
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "## Step 1: Understand the task and the domain\nThe task is to generate a challenging but solvable mathematics problem in the domain of associative rings and algebras, along with a complete and verifiable solution. The problem should be well-posed, clear, non-trivial, and solvable with established mathematical methods.\n\n## Step 2: Identify a suitable topic within associative rings and algebras\nAssociative rings and algebras involve the study of algebraic structures with associative multiplication. A suitable topic could involve properties of ideals, quotient rings, or specific types of algebras like matrix algebras.\n\n## Step 3: Formulate a problem\nLet's consider a problem involving the properties of a specific type of associative algebra. For instance, we could explore the characteristics of a matrix algebra over a ring.\n\n## Step 4: Generate a specific problem\nConsider the ring $R = \\mathbb{Z}/4\\mathbb{Z}$ and the matrix algebra $M_2(R)$ of $2 \\times 2$ matrices with entries in $R$. Let's examine the properties of ideals in $M_2(R)$.\n\n## Step 5: Formulate the problem statement\n\"Let $R = \\mathbb{Z}/4\\mathbb{Z}$ and consider the matrix algebra $M_2(R)$. Prove that $M_2(R)$ is not a simple ring, i.e., it has a non-trivial ideal.\"\n\n## Step 6: Provide the solution\nTo solve this, we need to show that there exists a non-trivial ideal in $M_2(R)$. An ideal $I$ in $M_2(R)$ is a subset that is closed under addition and under multiplication by any element of $M_2(R)$ from either side.\n\n## Step 7: Identify a non-trivial ideal\nConsider the set $I$ of matrices in $M_2(R)$ where all entries are even (i.e., $0$ or $2$ mod $4$). This set is non-empty and not equal to $M_2(R)$ because it does not contain matrices with odd entries.\n\n## Step 8: Verify that $I$ is an ideal\nFor $I$ to be an ideal, it must be closed under addition and under multiplication by elements of $M_2(R)$. For any two matrices $A, B \\in I$, $A + B$ has even entries because the sum of even numbers is even, so $A + B \\in I$. For any $A \\in I$ and $C \\in M_2(R)$, the product $CA$ and $AC$ will have entries that are products and sums of products involving elements of $R$. Since $A$ has even entries, $CA$ and $AC$ will also have even entries because multiplying any element of $R$ by an even element results in an even element.\n\n## Step 9: Check if $I$ is non-trivial\n$I$ is non-trivial if it is not the zero ideal and not the entire ring $M_2(R)$. The zero matrix is in $I$, and there are non-zero matrices in $I$ (e.g., $\\begin{pmatrix} 2 & 0 \\\\ 0 & 0 \\end{pmatrix}$), so $I$ is not the zero ideal. $I$ is not $M_2(R)$ because, for example, $\\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} \\notin I$.\n\n## Step 10: Conclude the solution\nSince $I$ is a non-trivial ideal, $M_2(R)$ is not a simple ring.\n\nThe final answer is: \n\nLet $R = \\mathbb{Z}/4\\mathbb{Z}$ and consider the matrix algebra $M_2(R)$. Prove that $M_2(R)$ is not a simple ring, i.e., it has a non-trivial ideal.",
    "run_id": "10",
    "topic_slug": "associative_rings_and_algebras",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "To prove that the matrix algebra $M_2(R)$ over the ring $R = \\mathbb{Z}/4\\mathbb{Z}$ is not a simple ring, we must demonstrate the existence of a two-sided ideal $J \\subseteq M_2(R)$ such that $J \\neq \\{0\\}$ (the zero ideal) and $J \\neq M_2(R)$ (the whole ring).\n\n### Step 1: Analyze the Ideals of the Base Ring $R$\n\nFirst, let us examine the ideal structure of the scalar ring $R = \\mathbb{Z}/4\\mathbb{Z}$. The elements of $R$ are the residue classes $\\{0, 1, 2, 3\\}$ modulo 4.\n\nAn ideal $I$ of $R$ is a subset closed under addition and under multiplication by any element of $R$. The ideals of $\\mathbb{Z}/n\\mathbb{Z}$ correspond to the subgroups generated by the divisors of $n$. For $n=4$, the divisors are 1, 2, and 4.\n\nConsider the principal ideal generated by 2, denoted as $I = (2) = 2R$:\n$$ I = \\{ 2 \\cdot 0, 2 \\cdot 1, 2 \\cdot 2, 2 \\cdot 3 \\} = \\{ 0, 2, 0, 2 \\} = \\{0, 2\\} $$\n\n**Verification that $I$ is a non-trivial ideal of $R$:**\n1.  **Non-triviality:** $I$ contains $2 \\neq 0$, so $I \\neq \\{0\\}$. $I$ does not contain $1$, so $I \\neq R$.\n2.  **Additivity:** $0+0=0$, $0+2=2$, $2+0=2$, $2+2=4 \\equiv 0$. The set is closed under addition.\n3.  **Absorbtion:** For any $r \\in R$ and $x \\in I$:\n    -   If $x=0$, $r \\cdot 0 = 0 \\in I$.\n    -   If $x=2$:\n        -   $0 \\cdot 2 = 0 \\in I$\n        -   $1 \\cdot 2 = 2 \\in I$\n        -   $2 \\cdot 2 = 4 \\equiv 0 \\in I$\n        -   $3 \\cdot 2 = 6 \\equiv 2 \\in I$\n\nThus, $I = \\{0, 2\\}$ is a proper, non-zero ideal of $R$.\n\n### Step 2: Construct the Ideal in $M_2(R)$\n\nThere is a standard correspondence between ideals in a ring $R$ and ideals in the matrix ring $M_n(R)$. Specifically, if $I$ is an ideal of $R$, then the set of matrices with entries in $I$, denoted $M_2(I)$, is an ideal of $M_2(R)$.\n\nLet us define $J$ as the set of all $2 \\times 2$ matrices with entries in $I = \\{0, 2\\}$:\n$$ J = \\left\\{ \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} \\in M_2(R) \\;\\middle|\\; a, b, c, d \\in \\{0, 2\\} \\right\\} $$\n\n### Step 3: Prove $J$ is a Two-Sided Ideal\n\nWe must show that $J$ is closed under addition and absorbs multiplication from $M_2(R)$ on both the left and the right.\n\n**1. Additive Subgroup:**\nLet $A, B \\in J$. Then their entries $A_{ij}, B_{ij} \\in I$. Since $I$ is an additive subgroup of $R$, $A_{ij} + B_{ij} \\in I$ for all $i, j$. Thus, $A+B \\in J$. $J$ is also non-empty (contains the zero matrix).\n\n**2. Multiplicative Absorption (Left Ideal):**\nLet $X \\in M_2(R)$ and $A \\in J$.\nLet $X = \\begin{pmatrix} x_{11} & x_{12} \\\\ x_{21} & x_{22} \\end{pmatrix}$ where $x_{ij} \\in R$.\nLet $A = \\begin{pmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{pmatrix}$ where $a_{ij} \\in I$.\n\nConsider the entry $(XA)_{11}$:\n$$ (XA)_{11} = x_{11}a_{11} + x_{12}a_{21} $$\nSince $I$ is an ideal of $R$, the product of any element in $R$ with an element in $I$ is in $I$.\nTherefore, $x_{11}a_{11} \\in I$ and $x_{12}a_{21} \\in I$.\nSince $I$ is closed under addition, $(XA)_{11} \\in I$.\n\nBy the same logic, every entry $(XA)_{ij} = \\sum_k x_{ik}a_{kj}$ is a sum of elements in $I$, and thus belongs to $I$. Consequently, $XA \\in J$.\n\n**3. Multiplicative Absorption (Right Ideal):**\nLet $A \\in J$ and $X \\in M_2(R)$.\nConsider the entry $(AX)_{ij} = \\sum_k a_{ik}x_{kj}$.\nSince $a_{ik} \\in I$ and $I$ is an ideal, $a_{ik}x_{kj} \\in I$. The sum of these terms is in $I$. Consequently, $AX \\in J$.\n\nThus, $J$ is a two-sided ideal.\n\n### Step 4: Verify $J$ is Non-Trivial\n\nWe must show that $J \\neq \\{0\\}$ and $J \\neq M_2(R)$.\n\n1.  **$J \\neq \\{0\\}$**:\n    Consider the matrix $A = \\begin{pmatrix} 2 & 0 \\\\ 0 & 0 \\end{pmatrix}$.\n    Since $2 \\in I$ and $0 \\in I$, $A \\in J$.\n    Since $2 \\not\\equiv 0 \\pmod 4$, $A$ is not the zero matrix.\n\n2.  **$J \\neq M_2(R)$**:\n    Consider the identity matrix $E = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$.\n    $E \\in M_2(R)$.\n    However, the entry $1 \\notin I$ (since $I=\\{0, 2\\}$).\n    Therefore, $E \\notin J$.\n\n### Conclusion\n\nWe have constructed a set $J \\subset M_2(R)$ and proved that:\n1.  $J$ is a two-sided ideal of $M_2(R)$.\n2.  $J$ contains non-zero elements.\n3.  $J$ is a proper subset of $M_2(R)$.\n\nSince $M_2(R)$ contains a non-trivial two-sided ideal, it is not a simple ring.",
        "raw_answer": "To prove that the matrix algebra $M_2(R)$ over the ring $R = \\mathbb{Z}/4\\mathbb{Z}$ is not a simple ring, we must demonstrate the existence of a two-sided ideal $J \\subseteq M_2(R)$ such that $J \\neq \\{0\\}$ (the zero ideal) and $J \\neq M_2(R)$ (the whole ring).\n\n### Step 1: Analyze the Ideals of the Base Ring $R$\n\nFirst, let us examine the ideal structure of the scalar ring $R = \\mathbb{Z}/4\\mathbb{Z}$. The elements of $R$ are the residue classes $\\{0, 1, 2, 3\\}$ modulo 4.\n\nAn ideal $I$ of $R$ is a subset closed under addition and under multiplication by any element of $R$. The ideals of $\\mathbb{Z}/n\\mathbb{Z}$ correspond to the subgroups generated by the divisors of $n$. For $n=4$, the divisors are 1, 2, and 4.\n\nConsider the principal ideal generated by 2, denoted as $I = (2) = 2R$:\n$$ I = \\{ 2 \\cdot 0, 2 \\cdot 1, 2 \\cdot 2, 2 \\cdot 3 \\} = \\{ 0, 2, 0, 2 \\} = \\{0, 2\\} $$\n\n**Verification that $I$ is a non-trivial ideal of $R$:**\n1.  **Non-triviality:** $I$ contains $2 \\neq 0$, so $I \\neq \\{0\\}$. $I$ does not contain $1$, so $I \\neq R$.\n2.  **Additivity:** $0+0=0$, $0+2=2$, $2+0=2$, $2+2=4 \\equiv 0$. The set is closed under addition.\n3.  **Absorbtion:** For any $r \\in R$ and $x \\in I$:\n    -   If $x=0$, $r \\cdot 0 = 0 \\in I$.\n    -   If $x=2$:\n        -   $0 \\cdot 2 = 0 \\in I$\n        -   $1 \\cdot 2 = 2 \\in I$\n        -   $2 \\cdot 2 = 4 \\equiv 0 \\in I$\n        -   $3 \\cdot 2 = 6 \\equiv 2 \\in I$\n\nThus, $I = \\{0, 2\\}$ is a proper, non-zero ideal of $R$.\n\n### Step 2: Construct the Ideal in $M_2(R)$\n\nThere is a standard correspondence between ideals in a ring $R$ and ideals in the matrix ring $M_n(R)$. Specifically, if $I$ is an ideal of $R$, then the set of matrices with entries in $I$, denoted $M_2(I)$, is an ideal of $M_2(R)$.\n\nLet us define $J$ as the set of all $2 \\times 2$ matrices with entries in $I = \\{0, 2\\}$:\n$$ J = \\left\\{ \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} \\in M_2(R) \\;\\middle|\\; a, b, c, d \\in \\{0, 2\\} \\right\\} $$\n\n### Step 3: Prove $J$ is a Two-Sided Ideal\n\nWe must show that $J$ is closed under addition and absorbs multiplication from $M_2(R)$ on both the left and the right.\n\n**1. Additive Subgroup:**\nLet $A, B \\in J$. Then their entries $A_{ij}, B_{ij} \\in I$. Since $I$ is an additive subgroup of $R$, $A_{ij} + B_{ij} \\in I$ for all $i, j$. Thus, $A+B \\in J$. $J$ is also non-empty (contains the zero matrix).\n\n**2. Multiplicative Absorption (Left Ideal):**\nLet $X \\in M_2(R)$ and $A \\in J$.\nLet $X = \\begin{pmatrix} x_{11} & x_{12} \\\\ x_{21} & x_{22} \\end{pmatrix}$ where $x_{ij} \\in R$.\nLet $A = \\begin{pmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{pmatrix}$ where $a_{ij} \\in I$.\n\nConsider the entry $(XA)_{11}$:\n$$ (XA)_{11} = x_{11}a_{11} + x_{12}a_{21} $$\nSince $I$ is an ideal of $R$, the product of any element in $R$ with an element in $I$ is in $I$.\nTherefore, $x_{11}a_{11} \\in I$ and $x_{12}a_{21} \\in I$.\nSince $I$ is closed under addition, $(XA)_{11} \\in I$.\n\nBy the same logic, every entry $(XA)_{ij} = \\sum_k x_{ik}a_{kj}$ is a sum of elements in $I$, and thus belongs to $I$. Consequently, $XA \\in J$.\n\n**3. Multiplicative Absorption (Right Ideal):**\nLet $A \\in J$ and $X \\in M_2(R)$.\nConsider the entry $(AX)_{ij} = \\sum_k a_{ik}x_{kj}$.\nSince $a_{ik} \\in I$ and $I$ is an ideal, $a_{ik}x_{kj} \\in I$. The sum of these terms is in $I$. Consequently, $AX \\in J$.\n\nThus, $J$ is a two-sided ideal.\n\n### Step 4: Verify $J$ is Non-Trivial\n\nWe must show that $J \\neq \\{0\\}$ and $J \\neq M_2(R)$.\n\n1.  **$J \\neq \\{0\\}$**:\n    Consider the matrix $A = \\begin{pmatrix} 2 & 0 \\\\ 0 & 0 \\end{pmatrix}$.\n    Since $2 \\in I$ and $0 \\in I$, $A \\in J$.\n    Since $2 \\not\\equiv 0 \\pmod 4$, $A$ is not the zero matrix.\n\n2.  **$J \\neq M_2(R)$**:\n    Consider the identity matrix $E = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$.\n    $E \\in M_2(R)$.\n    However, the entry $1 \\notin I$ (since $I=\\{0, 2\\}$).\n    Therefore, $E \\notin J$.\n\n### Conclusion\n\nWe have constructed a set $J \\subset M_2(R)$ and proved that:\n1.  $J$ is a two-sided ideal of $M_2(R)$.\n2.  $J$ contains non-zero elements.\n3.  $J$ is a proper subset of $M_2(R)$.\n\nSince $M_2(R)$ contains a non-trivial two-sided ideal, it is not a simple ring.",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ],
    "outer_attempt": 1
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "## Step 1: Understand the task and the domain\nThe task is to generate a challenging but solvable mathematics problem in the domain of nonassociative rings and algebras, along with a complete and verifiable solution. Nonassociative rings and algebras are mathematical structures where the multiplication operation is not necessarily associative.\n\n## Step 2: Identify a specific area within nonassociative rings and algebras\nA key area within nonassociative rings and algebras is the study of Lie algebras, which are crucial in various branches of mathematics and physics, particularly in the study of symmetries and conservation laws.\n\n## Step 3: Formulate a problem\nLet's consider a problem involving Lie algebras. A Lie algebra is a vector space $\\mathfrak{g}$ over a field $\\mathbb{F}$, together with a binary operation $[\\cdot, \\cdot]: \\mathfrak{g} \\times \\mathfrak{g} \\to \\mathfrak{g}$, called the Lie bracket, that satisfies bilinearity, skew-symmetry, and the Jacobi identity.\n\n## Step 4: Specify the problem\nConsider the Lie algebra $\\mathfrak{sl}(2, \\mathbb{R})$, which consists of $2 \\times 2$ real matrices with trace zero. The Lie bracket is defined as $[A, B] = AB - BA$. Let $H = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix}$, $X = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}$, and $Y = \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix}$. Show that $H, X, Y$ form a basis for $\\mathfrak{sl}(2, \\mathbb{R})$ and determine the structure constants with respect to this basis.\n\n## Step 5: Provide the problem statement in the required format\n\nLet $\\mathfrak{sl}(2, \\mathbb{R})$ be the Lie algebra of $2 \\times 2$ real matrices with trace zero, with the Lie bracket $[A, B] = AB - BA$. Show that the matrices $H = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix}$, $X = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}$, and $Y = \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix}$ form a basis for $\\mathfrak{sl}(2, \\mathbb{R})$ and compute the structure constants with respect to this basis.",
    "run_id": "11",
    "outer_attempt": 1,
    "topic_slug": "nonassociative_rings_and_algebras",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "Here is the complete solution to the problem concerning the Lie algebra $\\mathfrak{sl}(2, \\mathbb{R})$.\n\n### 1. Definition and Basis Verification\n\nFirst, we define the vector space $\\mathfrak{sl}(2, \\mathbb{R})$. It consists of all $2 \\times 2$ matrices with real entries such that the trace (the sum of diagonal elements) is zero.\n\nLet $A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} \\in M_2(\\mathbb{R})$.\nThe condition $\\text{tr}(A) = 0$ implies $a + d = 0$, or $d = -a$.\nThus, a general element of $\\mathfrak{sl}(2, \\mathbb{R})$ takes the form:\n$$ A = \\begin{pmatrix} a & b \\\\ c & -a \\end{pmatrix} $$\nwhere $a, b, c \\in \\mathbb{R}$. This indicates that the space has dimension 3.\n\nWe are given the matrices:\n$$ H = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix}, \\quad X = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}, \\quad Y = \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix} $$\n\n**Checking Trace Condition:**\n*   $\\text{tr}(H) = 1 + (-1) = 0$\n*   $\\text{tr}(X) = 0 + 0 = 0$\n*   $\\text{tr}(Y) = 0 + 0 = 0$\nAll three matrices belong to $\\mathfrak{sl}(2, \\mathbb{R})$.\n\n**Linear Independence:**\nConsider a linear combination equal to the zero matrix:\n$$ c_1 H + c_2 X + c_3 Y = \\mathbf{0} $$\nSubstituting the matrices:\n$$ c_1 \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix} + c_2 \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} + c_3 \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix} $$\n$$ \\begin{pmatrix} c_1 & c_2 \\\\ c_3 & -c_1 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix} $$\nComparing entries implies $c_1 = 0$, $c_2 = 0$, and $c_3 = 0$. Thus, the set $\\{H, X, Y\\}$ is linearly independent.\n\n**Spanning:**\nAny generic matrix $A = \\begin{pmatrix} a & b \\\\ c & -a \\end{pmatrix} \\in \\mathfrak{sl}(2, \\mathbb{R})$ can be written as:\n$$ A = a \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix} + b \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} + c \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix} = aH + bX + cY $$\nTherefore, $\\{H, X, Y\\}$ spans $\\mathfrak{sl}(2, \\mathbb{R})$.\n\n**Conclusion:**\nSince the set is linearly independent and spans the space, $\\{H, X, Y\\}$ forms a basis for $\\mathfrak{sl}(2, \\mathbb{R})$.\n\n---\n\n### 2. Computation of Lie Brackets\n\nThe Lie bracket is defined as $[A, B] = AB - BA$. We compute the brackets for the pairs of basis vectors. Note that $[A, A] = 0$ and $[B, A] = -[A, B]$, so we only need to compute $[H, X]$, $[H, Y]$, and $[X, Y]$.\n\n**1. Computing $[H, X]$:**\n$$ HX = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} = X $$\n$$ XH = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix} = \\begin{pmatrix} 0 & -1 \\\\ 0 & 0 \\end{pmatrix} = -X $$\n$$ [H, X] = HX - XH = X - (-X) = 2X $$\n\n**2. Computing $[H, Y]$:**\n$$ HY = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix} \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ -1 & 0 \\end{pmatrix} = -Y $$\n$$ YH = \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix} = Y $$\n$$ [H, Y] = HY - YH = -Y - Y = -2Y $$\n\n**3. Computing $[X, Y]$:**\n$$ XY = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} $$\n$$ YX = \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} $$\n$$ [X, Y] = XY - YX = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} - \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix} = H $$\n\n**Summary of Commutation Relations:**\n$$ [H, X] = 2X $$\n$$ [H, Y] = -2Y $$\n$$ [X, Y] = H $$\n\n---\n\n### 3. Structure Constants\n\nThe structure constants $C_{ij}^k$ with respect to a basis $\\{e_1, e_2, e_3\\}$ are the coefficients defined by the relation $[e_i, e_j] = \\sum_{k=1}^3 C_{ij}^k e_k$.\n\nWe identify the ordered basis as:\n$e_1 = H$, $e_2 = X$, $e_3 = Y$.\n\nFrom the commutation relations derived above:\n\n1.  $[e_1, e_2] = [H, X] = 2X = 0e_1 + 2e_2 + 0e_3$\n    *   $C_{12}^1 = 0$\n    *   $\\mathbf{C_{12}^2 = 2}$\n    *   $C_{12}^3 = 0$\n\n2.  $[e_1, e_3] = [H, Y] = -2Y = 0e_1 + 0e_2 - 2e_3$\n    *   $C_{13}^1 = 0$\n    *   $C_{13}^2 = 0$\n    *   $\\mathbf{C_{13}^3 = -2}$\n\n3.  $[e_2, e_3] = [X, Y] = H = 1e_1 + 0e_2 + 0e_3$\n    *   $\\mathbf{C_{23}^1 = 1}$\n    *   $C_{23}^2 = 0$\n    *   $C_{23}^3 = 0$\n\nDue to the skew-symmetry of the Lie bracket ($[e_j, e_i] = -[e_i, e_j]$), we also have:\n*   $C_{21}^2 = -2$\n*   $C_{31}^3 = 2$\n*   $C_{32}^1 = -1$\n\nAll other structure constants where $i=j$ are zero because $[e_i, e_i] = 0$.\n\n**Final Result:**\n\nThe matrices $H, X, Y$ form a basis for $\\mathfrak{sl}(2, \\mathbb{R})$. The non-zero structure constants for the ordered basis $(H, X, Y)$ are:\n$$ C_{HX}^X = 2, \\quad C_{HY}^Y = -2, \\quad C_{XY}^H = 1 $$\nand their skew-symmetric counterparts:\n$$ C_{XH}^X = -2, \\quad C_{YH}^Y = 2, \\quad C_{YX}^H = -1 $$",
        "raw_answer": "Here is the complete solution to the problem concerning the Lie algebra $\\mathfrak{sl}(2, \\mathbb{R})$.\n\n### 1. Definition and Basis Verification\n\nFirst, we define the vector space $\\mathfrak{sl}(2, \\mathbb{R})$. It consists of all $2 \\times 2$ matrices with real entries such that the trace (the sum of diagonal elements) is zero.\n\nLet $A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} \\in M_2(\\mathbb{R})$.\nThe condition $\\text{tr}(A) = 0$ implies $a + d = 0$, or $d = -a$.\nThus, a general element of $\\mathfrak{sl}(2, \\mathbb{R})$ takes the form:\n$$ A = \\begin{pmatrix} a & b \\\\ c & -a \\end{pmatrix} $$\nwhere $a, b, c \\in \\mathbb{R}$. This indicates that the space has dimension 3.\n\nWe are given the matrices:\n$$ H = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix}, \\quad X = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}, \\quad Y = \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix} $$\n\n**Checking Trace Condition:**\n*   $\\text{tr}(H) = 1 + (-1) = 0$\n*   $\\text{tr}(X) = 0 + 0 = 0$\n*   $\\text{tr}(Y) = 0 + 0 = 0$\nAll three matrices belong to $\\mathfrak{sl}(2, \\mathbb{R})$.\n\n**Linear Independence:**\nConsider a linear combination equal to the zero matrix:\n$$ c_1 H + c_2 X + c_3 Y = \\mathbf{0} $$\nSubstituting the matrices:\n$$ c_1 \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix} + c_2 \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} + c_3 \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix} $$\n$$ \\begin{pmatrix} c_1 & c_2 \\\\ c_3 & -c_1 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix} $$\nComparing entries implies $c_1 = 0$, $c_2 = 0$, and $c_3 = 0$. Thus, the set $\\{H, X, Y\\}$ is linearly independent.\n\n**Spanning:**\nAny generic matrix $A = \\begin{pmatrix} a & b \\\\ c & -a \\end{pmatrix} \\in \\mathfrak{sl}(2, \\mathbb{R})$ can be written as:\n$$ A = a \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix} + b \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} + c \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix} = aH + bX + cY $$\nTherefore, $\\{H, X, Y\\}$ spans $\\mathfrak{sl}(2, \\mathbb{R})$.\n\n**Conclusion:**\nSince the set is linearly independent and spans the space, $\\{H, X, Y\\}$ forms a basis for $\\mathfrak{sl}(2, \\mathbb{R})$.\n\n---\n\n### 2. Computation of Lie Brackets\n\nThe Lie bracket is defined as $[A, B] = AB - BA$. We compute the brackets for the pairs of basis vectors. Note that $[A, A] = 0$ and $[B, A] = -[A, B]$, so we only need to compute $[H, X]$, $[H, Y]$, and $[X, Y]$.\n\n**1. Computing $[H, X]$:**\n$$ HX = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} = X $$\n$$ XH = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix} = \\begin{pmatrix} 0 & -1 \\\\ 0 & 0 \\end{pmatrix} = -X $$\n$$ [H, X] = HX - XH = X - (-X) = 2X $$\n\n**2. Computing $[H, Y]$:**\n$$ HY = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix} \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ -1 & 0 \\end{pmatrix} = -Y $$\n$$ YH = \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix} = Y $$\n$$ [H, Y] = HY - YH = -Y - Y = -2Y $$\n\n**3. Computing $[X, Y]$:**\n$$ XY = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} $$\n$$ YX = \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} $$\n$$ [X, Y] = XY - YX = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} - \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix} = H $$\n\n**Summary of Commutation Relations:**\n$$ [H, X] = 2X $$\n$$ [H, Y] = -2Y $$\n$$ [X, Y] = H $$\n\n---\n\n### 3. Structure Constants\n\nThe structure constants $C_{ij}^k$ with respect to a basis $\\{e_1, e_2, e_3\\}$ are the coefficients defined by the relation $[e_i, e_j] = \\sum_{k=1}^3 C_{ij}^k e_k$.\n\nWe identify the ordered basis as:\n$e_1 = H$, $e_2 = X$, $e_3 = Y$.\n\nFrom the commutation relations derived above:\n\n1.  $[e_1, e_2] = [H, X] = 2X = 0e_1 + 2e_2 + 0e_3$\n    *   $C_{12}^1 = 0$\n    *   $\\mathbf{C_{12}^2 = 2}$\n    *   $C_{12}^3 = 0$\n\n2.  $[e_1, e_3] = [H, Y] = -2Y = 0e_1 + 0e_2 - 2e_3$\n    *   $C_{13}^1 = 0$\n    *   $C_{13}^2 = 0$\n    *   $\\mathbf{C_{13}^3 = -2}$\n\n3.  $[e_2, e_3] = [X, Y] = H = 1e_1 + 0e_2 + 0e_3$\n    *   $\\mathbf{C_{23}^1 = 1}$\n    *   $C_{23}^2 = 0$\n    *   $C_{23}^3 = 0$\n\nDue to the skew-symmetry of the Lie bracket ($[e_j, e_i] = -[e_i, e_j]$), we also have:\n*   $C_{21}^2 = -2$\n*   $C_{31}^3 = 2$\n*   $C_{32}^1 = -1$\n\nAll other structure constants where $i=j$ are zero because $[e_i, e_i] = 0$.\n\n**Final Result:**\n\nThe matrices $H, X, Y$ form a basis for $\\mathfrak{sl}(2, \\mathbb{R})$. The non-zero structure constants for the ordered basis $(H, X, Y)$ are:\n$$ C_{HX}^X = 2, \\quad C_{HY}^Y = -2, \\quad C_{XY}^H = 1 $$\nand their skew-symmetric counterparts:\n$$ C_{XH}^X = -2, \\quad C_{YH}^Y = 2, \\quad C_{YX}^H = -1 $$",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ]
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "## Step 1: Understand the task\nThe task is to generate a challenging mathematical problem in the domain of category theory or homological algebra, along with a solution that is verifiable and follows the given guidelines.\n\n## Step 2: Identify the domain\nThe problem should be related to category theory or homological algebra.\n\n## Step 3: Formulate a challenging problem\nTo create a challenging problem, we need to ensure it is well-posed, non-trivial, and solvable with the given constraints. Let's consider a problem that involves understanding and applying concepts from category theory.\n\n## Step 4: Generate the problem\nLet's formulate a problem that involves proving a property related to categories and functors.\n\n## Step 5: Write the problem\nProve that the functor $F: \\mathbf{Ab} \\to \\mathbf{Ab}$ defined by $F(G) = G \\otimes_{\\mathbb{Z}} \\mathbb{Q}$ is exact.\n[/QUESTION]",
    "run_id": "12",
    "topic_slug": "category_theory_and_homological_algebra",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "To prove that the functor $F: \\mathbf{Ab} \\to \\mathbf{Ab}$ defined by $F(G) = G \\otimes_{\\mathbb{Z}} \\mathbb{Q}$ is exact, we must show that it preserves short exact sequences.\n\n### 1. Definitions and Setup\n\nLet $\\mathbf{Ab}$ denote the category of abelian groups (equivalently, $\\mathbb{Z}$-modules).\nLet $0 \\to A \\xrightarrow{f} B \\xrightarrow{g} C \\to 0$ be a short exact sequence of abelian groups. This implies:\n1.  $f$ is injective ($\\ker(f) = \\{0\\}$).\n2.  $g$ is surjective ($\\text{im}(g) = C$).\n3.  $\\text{im}(f) = \\ker(g)$.\n\nWe apply the functor $F(G) = G \\otimes_{\\mathbb{Z}} \\mathbb{Q}$ to this sequence to obtain:\n$$0 \\to A \\otimes_{\\mathbb{Z}} \\mathbb{Q} \\xrightarrow{f \\otimes 1} B \\otimes_{\\mathbb{Z}} \\mathbb{Q} \\xrightarrow{g \\otimes 1} C \\otimes_{\\mathbb{Z}} \\mathbb{Q} \\to 0$$\nTo prove $F$ is exact, we must verify that this induced sequence is also exact.\n\n### 2. Right Exactness\n\nIt is a standard result in homological algebra that the tensor product functor $- \\otimes_R M$ is right exact for any $R$-module $M$. Since $\\mathbb{Q}$ is a $\\mathbb{Z}$-module, the sequence:\n$$A \\otimes_{\\mathbb{Z}} \\mathbb{Q} \\xrightarrow{f \\otimes 1} B \\otimes_{\\mathbb{Z}} \\mathbb{Q} \\xrightarrow{g \\otimes 1} C \\otimes_{\\mathbb{Z}} \\mathbb{Q} \\to 0$$\nis exact. This establishes that:\n1.  $g \\otimes 1$ is surjective.\n2.  $\\text{im}(f \\otimes 1) = \\ker(g \\otimes 1)$.\n\nThus, to complete the proof of exactness, we only need to show **Left Exactness**. Specifically, we must prove that $f \\otimes 1$ is injective.\n\n### 3. Left Exactness (Injectivity)\n\nWe need to show that if $f: A \\to B$ is injective, then $f \\otimes 1: A \\otimes_{\\mathbb{Z}} \\mathbb{Q} \\to B \\otimes_{\\mathbb{Z}} \\mathbb{Q}$ is injective.\n\n**Lemma 1 (Structure of the Tensor Product):**\nFor any abelian group $G$, there is a natural isomorphism $\\eta: G \\otimes_{\\mathbb{Z}} \\mathbb{Q} \\xrightarrow{\\cong} S^{-1}G$, where $S = \\mathbb{Z} \\setminus \\{0\\}$ is the multiplicative set of non-zero integers, and $S^{-1}G$ is the localization of $G$ at $S$.\nThe isomorphism is given by $\\eta(g \\otimes \\frac{p}{q}) = \\frac{p \\cdot g}{q}$.\n\n**Proof of Lemma 1 (Sketch):**\nThe map $\\phi: G \\times \\mathbb{Q} \\to S^{-1}G$ defined by $(g, \\frac{p}{q}) \\mapsto \\frac{p \\cdot g}{q}$ is $\\mathbb{Z}$-bilinear. By the universal property of tensor products, this induces a homomorphism $\\eta: G \\otimes \\mathbb{Q} \\to S^{-1}G$.\nConversely, define $\\psi: S^{-1}G \\to G \\otimes \\mathbb{Q}$ by $\\frac{g}{q} \\mapsto g \\otimes \\frac{1}{q}$. One can check this is well-defined and inverse to $\\eta$.\n\n**Using the Lemma to prove Injectivity:**\nUsing the isomorphism $G \\otimes_{\\mathbb{Z}} \\mathbb{Q} \\cong S^{-1}G$, checking the injectivity of $f \\otimes 1$ is equivalent to checking the injectivity of the induced map on localizations $S^{-1}f: S^{-1}A \\to S^{-1}B$, defined by $S^{-1}f(\\frac{a}{s}) = \\frac{f(a)}{s}$.\n\nLet $\\frac{a}{s} \\in S^{-1}A$ be an element in the kernel of $S^{-1}f$.\n$$S^{-1}f\\left(\\frac{a}{s}\\right) = \\frac{f(a)}{s} = 0 \\quad \\text{in } S^{-1}B$$\n\nBy the definition of equality in a localization, an element $\\frac{x}{y}$ is zero in $S^{-1}B$ if and only if there exists a $t \\in S$ such that $t \\cdot x = 0$ in $B$.\nApplying this here, there exists an integer $t \\in \\mathbb{Z} \\setminus \\{0\\}$ such that:\n$$t \\cdot f(a) = 0$$\n\nSince $f$ is a homomorphism, $t \\cdot f(a) = f(t \\cdot a)$. Thus:\n$$f(t \\cdot a) = 0$$\n\nSince $f$ is injective (by the hypothesis that the original sequence is exact), the kernel of $f$ is trivial. Therefore:\n$$t \\cdot a = 0$$\n\nNow we return to the element $\\frac{a}{s}$ in $S^{-1}A$. By the definition of localization, $\\frac{a}{s}$ is equal to $\\frac{t \\cdot a}{t \\cdot s}$. Substituting $t \\cdot a = 0$:\n$$\\frac{a}{s} = \\frac{0}{t \\cdot s} = 0 \\quad \\text{in } S^{-1}A$$\n\nSince the only element in the kernel of $S^{-1}f$ is $0$, the map is injective.\n\n### 4. Conclusion\n\nWe have shown:\n1.  The functor preserves surjectivity and exactness at the middle term (Right Exactness) due to general properties of the tensor product.\n2.  The functor preserves injectivity (Left Exactness) due to the properties of localization by non-zero integers (specifically, that $\\mathbb{Q}$ is torsion-free and thus flat over $\\mathbb{Z}$).\n\nSince $F$ preserves short exact sequences, $F$ is an **exact functor**.",
        "raw_answer": "To prove that the functor $F: \\mathbf{Ab} \\to \\mathbf{Ab}$ defined by $F(G) = G \\otimes_{\\mathbb{Z}} \\mathbb{Q}$ is exact, we must show that it preserves short exact sequences.\n\n### 1. Definitions and Setup\n\nLet $\\mathbf{Ab}$ denote the category of abelian groups (equivalently, $\\mathbb{Z}$-modules).\nLet $0 \\to A \\xrightarrow{f} B \\xrightarrow{g} C \\to 0$ be a short exact sequence of abelian groups. This implies:\n1.  $f$ is injective ($\\ker(f) = \\{0\\}$).\n2.  $g$ is surjective ($\\text{im}(g) = C$).\n3.  $\\text{im}(f) = \\ker(g)$.\n\nWe apply the functor $F(G) = G \\otimes_{\\mathbb{Z}} \\mathbb{Q}$ to this sequence to obtain:\n$$0 \\to A \\otimes_{\\mathbb{Z}} \\mathbb{Q} \\xrightarrow{f \\otimes 1} B \\otimes_{\\mathbb{Z}} \\mathbb{Q} \\xrightarrow{g \\otimes 1} C \\otimes_{\\mathbb{Z}} \\mathbb{Q} \\to 0$$\nTo prove $F$ is exact, we must verify that this induced sequence is also exact.\n\n### 2. Right Exactness\n\nIt is a standard result in homological algebra that the tensor product functor $- \\otimes_R M$ is right exact for any $R$-module $M$. Since $\\mathbb{Q}$ is a $\\mathbb{Z}$-module, the sequence:\n$$A \\otimes_{\\mathbb{Z}} \\mathbb{Q} \\xrightarrow{f \\otimes 1} B \\otimes_{\\mathbb{Z}} \\mathbb{Q} \\xrightarrow{g \\otimes 1} C \\otimes_{\\mathbb{Z}} \\mathbb{Q} \\to 0$$\nis exact. This establishes that:\n1.  $g \\otimes 1$ is surjective.\n2.  $\\text{im}(f \\otimes 1) = \\ker(g \\otimes 1)$.\n\nThus, to complete the proof of exactness, we only need to show **Left Exactness**. Specifically, we must prove that $f \\otimes 1$ is injective.\n\n### 3. Left Exactness (Injectivity)\n\nWe need to show that if $f: A \\to B$ is injective, then $f \\otimes 1: A \\otimes_{\\mathbb{Z}} \\mathbb{Q} \\to B \\otimes_{\\mathbb{Z}} \\mathbb{Q}$ is injective.\n\n**Lemma 1 (Structure of the Tensor Product):**\nFor any abelian group $G$, there is a natural isomorphism $\\eta: G \\otimes_{\\mathbb{Z}} \\mathbb{Q} \\xrightarrow{\\cong} S^{-1}G$, where $S = \\mathbb{Z} \\setminus \\{0\\}$ is the multiplicative set of non-zero integers, and $S^{-1}G$ is the localization of $G$ at $S$.\nThe isomorphism is given by $\\eta(g \\otimes \\frac{p}{q}) = \\frac{p \\cdot g}{q}$.\n\n**Proof of Lemma 1 (Sketch):**\nThe map $\\phi: G \\times \\mathbb{Q} \\to S^{-1}G$ defined by $(g, \\frac{p}{q}) \\mapsto \\frac{p \\cdot g}{q}$ is $\\mathbb{Z}$-bilinear. By the universal property of tensor products, this induces a homomorphism $\\eta: G \\otimes \\mathbb{Q} \\to S^{-1}G$.\nConversely, define $\\psi: S^{-1}G \\to G \\otimes \\mathbb{Q}$ by $\\frac{g}{q} \\mapsto g \\otimes \\frac{1}{q}$. One can check this is well-defined and inverse to $\\eta$.\n\n**Using the Lemma to prove Injectivity:**\nUsing the isomorphism $G \\otimes_{\\mathbb{Z}} \\mathbb{Q} \\cong S^{-1}G$, checking the injectivity of $f \\otimes 1$ is equivalent to checking the injectivity of the induced map on localizations $S^{-1}f: S^{-1}A \\to S^{-1}B$, defined by $S^{-1}f(\\frac{a}{s}) = \\frac{f(a)}{s}$.\n\nLet $\\frac{a}{s} \\in S^{-1}A$ be an element in the kernel of $S^{-1}f$.\n$$S^{-1}f\\left(\\frac{a}{s}\\right) = \\frac{f(a)}{s} = 0 \\quad \\text{in } S^{-1}B$$\n\nBy the definition of equality in a localization, an element $\\frac{x}{y}$ is zero in $S^{-1}B$ if and only if there exists a $t \\in S$ such that $t \\cdot x = 0$ in $B$.\nApplying this here, there exists an integer $t \\in \\mathbb{Z} \\setminus \\{0\\}$ such that:\n$$t \\cdot f(a) = 0$$\n\nSince $f$ is a homomorphism, $t \\cdot f(a) = f(t \\cdot a)$. Thus:\n$$f(t \\cdot a) = 0$$\n\nSince $f$ is injective (by the hypothesis that the original sequence is exact), the kernel of $f$ is trivial. Therefore:\n$$t \\cdot a = 0$$\n\nNow we return to the element $\\frac{a}{s}$ in $S^{-1}A$. By the definition of localization, $\\frac{a}{s}$ is equal to $\\frac{t \\cdot a}{t \\cdot s}$. Substituting $t \\cdot a = 0$:\n$$\\frac{a}{s} = \\frac{0}{t \\cdot s} = 0 \\quad \\text{in } S^{-1}A$$\n\nSince the only element in the kernel of $S^{-1}f$ is $0$, the map is injective.\n\n### 4. Conclusion\n\nWe have shown:\n1.  The functor preserves surjectivity and exactness at the middle term (Right Exactness) due to general properties of the tensor product.\n2.  The functor preserves injectivity (Left Exactness) due to the properties of localization by non-zero integers (specifically, that $\\mathbb{Q}$ is torsion-free and thus flat over $\\mathbb{Z}$).\n\nSince $F$ preserves short exact sequences, $F$ is an **exact functor**.",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ],
    "outer_attempt": 1
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "## Step 1: Understand the task\nThe task is to generate a challenging mathematics problem in the domain of K-theory, along with a solution that is verifiable and follows the given format.\n\n## Step 2: Identify the topic and relevant concepts\nThe topic is K-theory, which is a branch of algebraic topology that studies the properties of vector bundles and their relations to the underlying space. K-theory is crucial in various areas of mathematics and physics.\n\n## Step 3: Formulate a problem\nTo create a problem, we need to understand what kind of questions are typically asked in K-theory. A common type of problem involves calculating the K-theory groups of a specific space or understanding the properties of K-theory groups under certain operations.\n\n## Step 4: Generate a specific problem\nLet's generate a problem related to the K-theory of a particular space. Consider the space $S^2$, the 2-sphere. A relevant problem could be to compute $K^0(S^2)$ and $K^1(S^2)$, which are related to the complex vector bundles on $S^2$.\n\n## Step 5: Formulate the problem in the required format\nWe need to provide a problem in the format: \"Compute $K^0(X)$ and $K^1(X)$ for $X = S^2$,\" where $S^2$ is the 2-sphere.\n\n## Step 6: Provide the solution\nTo solve this, we recall that $K^0(X)$ and $K^1(X)$ are related to the Grothendieck groups of complex vector bundles on $X$. For $S^2$, we know that $K^0(S^2)$ is related to the isomorphism classes of complex vector bundles on $S^2$, and $K^1(S^2)$ is related to the stable homotopy group $\\pi_1(S^2)$.\n\n## Step 7: Write down the problem and its solution\n### Problem\nCompute $K^0(S^2)$ and $K^1(S^2)$.",
    "run_id": "13",
    "topic_slug": "k_theory",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "Here is the complete, rigorous solution to the problem identified in the task.\n\n**Problem:** Compute the complex K-theory groups $K^0(S^2)$ and $K^1(S^2)$ for the 2-sphere $S^2$.\n\n### **Solution**\n\nWe aim to compute the unreduced complex K-theory groups $K^0(S^2)$ and $K^1(S^2)$. We assume $K^*(X)$ denotes the standard complex topological K-theory of a compact Hausdorff space $X$.\n\n**1. Preliminaries and Decomposition**\n\nFor a compact Hausdorff space $X$ with a chosen basepoint $x_0$, the unreduced K-theory group $K^0(X)$ splits as the direct sum of the reduced K-theory group $\\tilde{K}^0(X)$ and the K-theory of the point:\n$$ K^0(X) \\cong \\tilde{K}^0(X) \\oplus K^0(\\{x_0\\}). $$\nThe group $K^1(X)$ is defined as the reduced K-theory of the suspension of $X$ (up to a shift), specifically $K^1(X) \\cong \\tilde{K}^0(\\Sigma X)$. Note that for a point, $K^0(pt) \\cong \\mathbb{Z}$ and $K^1(pt) = 0$.\n\nFor the 2-sphere $S^2$, we have:\n1.  $K^0(S^2) \\cong \\tilde{K}^0(S^2) \\oplus \\mathbb{Z}$.\n2.  $K^1(S^2) \\cong \\tilde{K}^0(S^3)$.\n\nThe problem reduces to computing the reduced K-groups $\\tilde{K}^0(S^n)$ for $n=2$ and $n=3$.\n\n**2. Connection to Homotopy Groups of the Unitary Group**\n\nReduced K-theory groups of spheres are isomorphic to the homotopy groups of the classifying space of the unitary group (or the unitary group itself, depending on the index). Specifically, there is a standard isomorphism derived from the fact that vector bundles over $S^n$ are classified by the homotopy group of the transition maps on the equator $S^{n-1}$:\n$$ \\tilde{K}^0(S^n) \\cong \\pi_{n-1}(GL(\\infty, \\mathbb{C})) \\cong \\pi_{n-1}(U(\\infty)). $$\nHere $U(\\infty) = \\varinjlim U(n)$ is the stable unitary group.\n\n**3. Application of Bott Periodicity**\n\nBott Periodicity establishes the values of the homotopy groups $\\pi_k(U(\\infty))$. The periodicity theorem states that:\n$$ \\pi_k(U(\\infty)) \\cong \\pi_{k+2}(U(\\infty)) $$\nThe base cases are:\n*   $\\pi_0(U(\\infty)) = 0$ (since $U(n)$ is path-connected).\n*   $\\pi_1(U(\\infty)) \\cong \\mathbb{Z}$.\n\nUsing this periodicity, we can determine the groups for any $k \\geq 0$:\n*   For even $k$: $\\pi_k(U(\\infty)) \\cong \\pi_0(U(\\infty)) = 0$.\n*   For odd $k$: $\\pi_k(U(\\infty)) \\cong \\pi_1(U(\\infty)) \\cong \\mathbb{Z}$.\n\n**4. Computation of $K^0(S^2)$**\n\nUsing the relation $\\tilde{K}^0(S^n) \\cong \\pi_{n-1}(U(\\infty))$ for $n=2$:\n$$ \\tilde{K}^0(S^2) \\cong \\pi_1(U(\\infty)). $$\nFrom Bott Periodicity, $\\pi_1(U(\\infty)) \\cong \\mathbb{Z}$.\nThus, $\\tilde{K}^0(S^2) \\cong \\mathbb{Z}$.\n\nSubstituting this back into the decomposition formula:\n$$ K^0(S^2) \\cong \\tilde{K}^0(S^2) \\oplus \\mathbb{Z} \\cong \\mathbb{Z} \\oplus \\mathbb{Z}. $$\n\n*Note on Generators:* The group is generated by the class of the trivial line bundle, $[1]$, and the class of the tautological line bundle, $[H]$, via the identification $S^2 \\cong \\mathbb{C}P^1$. Alternatively, it is generated by $[1]$ and the reduced element $([H] - [1])$.\n\n**5. Computation of $K^1(S^2)$**\n\nWe use the definition $K^1(S^2) \\cong \\tilde{K}^0(S^3)$ (or equivalently $K^{-1}(S^2) \\cong [S^2, U(\\infty)]$).\nUsing the relation $\\tilde{K}^0(S^n) \\cong \\pi_{n-1}(U(\\infty))$ for $n=3$:\n$$ K^1(S^2) \\cong \\tilde{K}^0(S^3) \\cong \\pi_2(U(\\infty)). $$\nFrom Bott Periodicity, $\\pi_2(U(\\infty)) \\cong \\pi_0(U(\\infty)) = 0$.\nThus,\n$$ K^1(S^2) \\cong 0. $$\n\n**Conclusion**\n\nThe complex K-theory groups of the 2-sphere are:\n\n$$\nK^0(S^2) \\cong \\mathbb{Z} \\oplus \\mathbb{Z}\n$$\n$$\nK^1(S^2) \\cong 0\n$$",
        "raw_answer": "Here is the complete, rigorous solution to the problem identified in the task.\n\n**Problem:** Compute the complex K-theory groups $K^0(S^2)$ and $K^1(S^2)$ for the 2-sphere $S^2$.\n\n### **Solution**\n\nWe aim to compute the unreduced complex K-theory groups $K^0(S^2)$ and $K^1(S^2)$. We assume $K^*(X)$ denotes the standard complex topological K-theory of a compact Hausdorff space $X$.\n\n**1. Preliminaries and Decomposition**\n\nFor a compact Hausdorff space $X$ with a chosen basepoint $x_0$, the unreduced K-theory group $K^0(X)$ splits as the direct sum of the reduced K-theory group $\\tilde{K}^0(X)$ and the K-theory of the point:\n$$ K^0(X) \\cong \\tilde{K}^0(X) \\oplus K^0(\\{x_0\\}). $$\nThe group $K^1(X)$ is defined as the reduced K-theory of the suspension of $X$ (up to a shift), specifically $K^1(X) \\cong \\tilde{K}^0(\\Sigma X)$. Note that for a point, $K^0(pt) \\cong \\mathbb{Z}$ and $K^1(pt) = 0$.\n\nFor the 2-sphere $S^2$, we have:\n1.  $K^0(S^2) \\cong \\tilde{K}^0(S^2) \\oplus \\mathbb{Z}$.\n2.  $K^1(S^2) \\cong \\tilde{K}^0(S^3)$.\n\nThe problem reduces to computing the reduced K-groups $\\tilde{K}^0(S^n)$ for $n=2$ and $n=3$.\n\n**2. Connection to Homotopy Groups of the Unitary Group**\n\nReduced K-theory groups of spheres are isomorphic to the homotopy groups of the classifying space of the unitary group (or the unitary group itself, depending on the index). Specifically, there is a standard isomorphism derived from the fact that vector bundles over $S^n$ are classified by the homotopy group of the transition maps on the equator $S^{n-1}$:\n$$ \\tilde{K}^0(S^n) \\cong \\pi_{n-1}(GL(\\infty, \\mathbb{C})) \\cong \\pi_{n-1}(U(\\infty)). $$\nHere $U(\\infty) = \\varinjlim U(n)$ is the stable unitary group.\n\n**3. Application of Bott Periodicity**\n\nBott Periodicity establishes the values of the homotopy groups $\\pi_k(U(\\infty))$. The periodicity theorem states that:\n$$ \\pi_k(U(\\infty)) \\cong \\pi_{k+2}(U(\\infty)) $$\nThe base cases are:\n*   $\\pi_0(U(\\infty)) = 0$ (since $U(n)$ is path-connected).\n*   $\\pi_1(U(\\infty)) \\cong \\mathbb{Z}$.\n\nUsing this periodicity, we can determine the groups for any $k \\geq 0$:\n*   For even $k$: $\\pi_k(U(\\infty)) \\cong \\pi_0(U(\\infty)) = 0$.\n*   For odd $k$: $\\pi_k(U(\\infty)) \\cong \\pi_1(U(\\infty)) \\cong \\mathbb{Z}$.\n\n**4. Computation of $K^0(S^2)$**\n\nUsing the relation $\\tilde{K}^0(S^n) \\cong \\pi_{n-1}(U(\\infty))$ for $n=2$:\n$$ \\tilde{K}^0(S^2) \\cong \\pi_1(U(\\infty)). $$\nFrom Bott Periodicity, $\\pi_1(U(\\infty)) \\cong \\mathbb{Z}$.\nThus, $\\tilde{K}^0(S^2) \\cong \\mathbb{Z}$.\n\nSubstituting this back into the decomposition formula:\n$$ K^0(S^2) \\cong \\tilde{K}^0(S^2) \\oplus \\mathbb{Z} \\cong \\mathbb{Z} \\oplus \\mathbb{Z}. $$\n\n*Note on Generators:* The group is generated by the class of the trivial line bundle, $[1]$, and the class of the tautological line bundle, $[H]$, via the identification $S^2 \\cong \\mathbb{C}P^1$. Alternatively, it is generated by $[1]$ and the reduced element $([H] - [1])$.\n\n**5. Computation of $K^1(S^2)$**\n\nWe use the definition $K^1(S^2) \\cong \\tilde{K}^0(S^3)$ (or equivalently $K^{-1}(S^2) \\cong [S^2, U(\\infty)]$).\nUsing the relation $\\tilde{K}^0(S^n) \\cong \\pi_{n-1}(U(\\infty))$ for $n=3$:\n$$ K^1(S^2) \\cong \\tilde{K}^0(S^3) \\cong \\pi_2(U(\\infty)). $$\nFrom Bott Periodicity, $\\pi_2(U(\\infty)) \\cong \\pi_0(U(\\infty)) = 0$.\nThus,\n$$ K^1(S^2) \\cong 0. $$\n\n**Conclusion**\n\nThe complex K-theory groups of the 2-sphere are:\n\n$$\nK^0(S^2) \\cong \\mathbb{Z} \\oplus \\mathbb{Z}\n$$\n$$\nK^1(S^2) \\cong 0\n$$",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ],
    "outer_attempt": 1
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "## Step 1: Understand the task and the domain\nThe task is to generate a challenging but solvable mathematics problem in the domain of \"Group theory and generalizations\" along with a complete and verifiable solution. The problem should be well-posed, clear, non-trivial, and solvable with established mathematical methods.\n\n## Step 2: Identify a suitable topic within Group theory and generalizations\nGroup theory is a branch of abstract algebra that studies the symmetric aspects of objects and the transformations that preserve those symmetries. A suitable topic could involve properties of groups, such as subgroup structures, homomorphisms, or specific types of groups like cyclic or symmetric groups.\n\n## Step 3: Formulate a challenging problem\nTo create a challenging problem, we can consider a topic like the classification of groups or the study of group homomorphisms. A potential problem could involve determining the number of homomorphisms between two given groups or analyzing the structure of a specific group.\n\n## Step 4: Generate a specific problem\nLet's consider the problem of finding the number of homomorphisms from the cyclic group of order 12, $\\mathbb{Z}_{12}$, to the symmetric group $S_3$. This involves understanding the structure of both groups and applying knowledge about homomorphisms.\n\n## Step 5: State the problem formally\nThe problem can be stated as: \"Let $\\mathbb{Z}_{12}$ be the cyclic group of order 12 and $S_3$ be the symmetric group on 3 elements. Determine the number of homomorphisms from $\\mathbb{Z}_{12}$ to $S_3$.\"\n\n## Step 6: Provide the solution\nTo solve this, we need to understand that a homomorphism $f: \\mathbb{Z}_{12} \\to S_3$ is determined by $f(1)$, where $1$ is a generator of $\\mathbb{Z}_{12}$. The element $f(1)$ must have an order that divides 12. $S_3$ has elements of orders 1, 2, and 3. We need to count how many elements in $S_3$ have orders that divide 12.\n\n## Step 7: Analyze the elements of $S_3$\n$S_3 = \\{e, (12), (13), (23), (123), (132)\\}$. The identity $e$ has order 1, the transpositions $(12), (13), (23)$ have order 2, and the 3-cycles $(123), (132)$ have order 3. The orders of elements in $S_3$ that divide 12 are 1, 2, and 3.\n\n## Step 8: Count the possible images of $1 \\in \\mathbb{Z}_{12}$\nThere is 1 element of order 1, 3 elements of order 2, and 2 elements of order 3. So, $f(1)$ can be any of these 6 elements because their orders divide 12.\n\n## Step 9: Conclude the number of homomorphisms\nSince $f(1)$ can be any of the 6 elements (1 of order 1, 3 of order 2, and 2 of order 3), there are 6 possible homomorphisms from $\\mathbb{Z}_{12}$ to $S_3$.\n\n## Step 10: Format the problem and solution according to the required output format\n\nLet $\\mathbb{Z}_{12}$ be the cyclic group of order 12 and $S_3$ be the symmetric group on 3 elements. Determine the number of homomorphisms from $\\mathbb{Z}_{12}$ to $S_3$.",
    "run_id": "14",
    "topic_slug": "group_theory_and_generalizations",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "**Problem Statement**\nLet $\\mathbb{Z}_{12}$ be the cyclic group of order 12 and $S_3$ be the symmetric group on 3 elements. Determine the number of homomorphisms from $\\mathbb{Z}_{12}$ to $S_3$.\n\n**Solution**\n\nTo find the number of group homomorphisms from $\\mathbb{Z}_{12}$ to $S_3$, we analyze the structural properties of homomorphisms originating from a cyclic group.\n\n**1. General Property of Homomorphisms from a Cyclic Group**\nLet $G = \\mathbb{Z}_n$ be a cyclic group of order $n$ generated by an element $g$ (usually denoted as $1$ in additive notation). Let $H$ be any target group. A homomorphism $\\phi: \\mathbb{Z}_n \\to H$ is completely determined by the image of the generator, $\\phi(1)$.\n\nLet $y = \\phi(1) \\in H$. For $\\phi$ to be a well-defined homomorphism, it must respect the group relation of the domain. In $\\mathbb{Z}_n$, the defining relation is $n \\cdot 1 = 0$ (the identity element of $\\mathbb{Z}_n$). Applying $\\phi$ to this relation requires:\n$$ \\phi(n \\cdot 1) = \\phi(0) $$\n$$ (\\phi(1))^n = e_H $$\n$$ y^n = e_H $$\nwhere $e_H$ is the identity element of $H$.\n\nThis condition implies that the order of the element $y$ in $H$, denoted $|y|$, must divide $n$. Conversely, if we choose any element $y \\in H$ such that $|y|$ divides $n$, the map $\\phi(k) = y^k$ defines a unique homomorphism.\n\nThus, the number of homomorphisms from $\\mathbb{Z}_{12}$ to $S_3$ is equal to the number of elements $y \\in S_3$ such that $|y|$ divides 12.\n\n**2. Analyzing the Elements of $S_3$**\nThe symmetric group $S_3$ consists of permutations of the set $\\{1, 2, 3\\}$. The order of the group is $|S_3| = 3! = 6$. We list the elements and determine their orders:\n\n*   **Identity:**\n    *   $e = (1)(2)(3)$\n    *   Order: 1\n\n*   **Transpositions (2-cycles):**\n    *   $(1 \\ 2)$, $(1 \\ 3)$, $(2 \\ 3)$\n    *   Order: 2 (since $(a \\ b)^2 = e$)\n\n*   **3-cycles:**\n    *   $(1 \\ 2 \\ 3)$, $(1 \\ 3 \\ 2)$\n    *   Order: 3 (since $(a \\ b \\ c)^3 = e$)\n\n**3. Checking the Divisibility Condition**\nWe require $|y|$ to be a divisor of 12. We check each order found in $S_3$:\n\n*   **Elements of order 1:**\n    *   $1$ divides $12$.\n    *   Count: 1 element ($e$).\n\n*   **Elements of order 2:**\n    *   $2$ divides $12$.\n    *   Count: 3 elements ($(1 \\ 2), (1 \\ 3), (2 \\ 3)$).\n\n*   **Elements of order 3:**\n    *   $3$ divides $12$.\n    *   Count: 2 elements ($(1 \\ 2 \\ 3), (1 \\ 3 \\ 2)$).\n\n**4. Calculation and Conclusion**\nEvery element in $S_3$ has an order (1, 2, or 3) that divides 12. Therefore, every element in $S_3$ can serve as the image of the generator $1 \\in \\mathbb{Z}_{12}$ to define a valid homomorphism.\n\nTotal number of homomorphisms = (Number of elements of order 1) + (Number of elements of order 2) + (Number of elements of order 3)\n$$ \\text{Total} = 1 + 3 + 2 = 6 $$\n\nThe set of homomorphisms consists of:\n1.  The trivial homomorphism mapping everything to $e$.\n2.  Three homomorphisms mapping $\\mathbb{Z}_{12}$ onto the subgroups of order 2 generated by transpositions.\n3.  Two homomorphisms mapping $\\mathbb{Z}_{12}$ onto the subgroup of order 3 generated by 3-cycles.\n\nThere are **6** homomorphisms from $\\mathbb{Z}_{12}$ to $S_3$.",
        "raw_answer": "**Problem Statement**\nLet $\\mathbb{Z}_{12}$ be the cyclic group of order 12 and $S_3$ be the symmetric group on 3 elements. Determine the number of homomorphisms from $\\mathbb{Z}_{12}$ to $S_3$.\n\n**Solution**\n\nTo find the number of group homomorphisms from $\\mathbb{Z}_{12}$ to $S_3$, we analyze the structural properties of homomorphisms originating from a cyclic group.\n\n**1. General Property of Homomorphisms from a Cyclic Group**\nLet $G = \\mathbb{Z}_n$ be a cyclic group of order $n$ generated by an element $g$ (usually denoted as $1$ in additive notation). Let $H$ be any target group. A homomorphism $\\phi: \\mathbb{Z}_n \\to H$ is completely determined by the image of the generator, $\\phi(1)$.\n\nLet $y = \\phi(1) \\in H$. For $\\phi$ to be a well-defined homomorphism, it must respect the group relation of the domain. In $\\mathbb{Z}_n$, the defining relation is $n \\cdot 1 = 0$ (the identity element of $\\mathbb{Z}_n$). Applying $\\phi$ to this relation requires:\n$$ \\phi(n \\cdot 1) = \\phi(0) $$\n$$ (\\phi(1))^n = e_H $$\n$$ y^n = e_H $$\nwhere $e_H$ is the identity element of $H$.\n\nThis condition implies that the order of the element $y$ in $H$, denoted $|y|$, must divide $n$. Conversely, if we choose any element $y \\in H$ such that $|y|$ divides $n$, the map $\\phi(k) = y^k$ defines a unique homomorphism.\n\nThus, the number of homomorphisms from $\\mathbb{Z}_{12}$ to $S_3$ is equal to the number of elements $y \\in S_3$ such that $|y|$ divides 12.\n\n**2. Analyzing the Elements of $S_3$**\nThe symmetric group $S_3$ consists of permutations of the set $\\{1, 2, 3\\}$. The order of the group is $|S_3| = 3! = 6$. We list the elements and determine their orders:\n\n*   **Identity:**\n    *   $e = (1)(2)(3)$\n    *   Order: 1\n\n*   **Transpositions (2-cycles):**\n    *   $(1 \\ 2)$, $(1 \\ 3)$, $(2 \\ 3)$\n    *   Order: 2 (since $(a \\ b)^2 = e$)\n\n*   **3-cycles:**\n    *   $(1 \\ 2 \\ 3)$, $(1 \\ 3 \\ 2)$\n    *   Order: 3 (since $(a \\ b \\ c)^3 = e$)\n\n**3. Checking the Divisibility Condition**\nWe require $|y|$ to be a divisor of 12. We check each order found in $S_3$:\n\n*   **Elements of order 1:**\n    *   $1$ divides $12$.\n    *   Count: 1 element ($e$).\n\n*   **Elements of order 2:**\n    *   $2$ divides $12$.\n    *   Count: 3 elements ($(1 \\ 2), (1 \\ 3), (2 \\ 3)$).\n\n*   **Elements of order 3:**\n    *   $3$ divides $12$.\n    *   Count: 2 elements ($(1 \\ 2 \\ 3), (1 \\ 3 \\ 2)$).\n\n**4. Calculation and Conclusion**\nEvery element in $S_3$ has an order (1, 2, or 3) that divides 12. Therefore, every element in $S_3$ can serve as the image of the generator $1 \\in \\mathbb{Z}_{12}$ to define a valid homomorphism.\n\nTotal number of homomorphisms = (Number of elements of order 1) + (Number of elements of order 2) + (Number of elements of order 3)\n$$ \\text{Total} = 1 + 3 + 2 = 6 $$\n\nThe set of homomorphisms consists of:\n1.  The trivial homomorphism mapping everything to $e$.\n2.  Three homomorphisms mapping $\\mathbb{Z}_{12}$ onto the subgroups of order 2 generated by transpositions.\n3.  Two homomorphisms mapping $\\mathbb{Z}_{12}$ onto the subgroup of order 3 generated by 3-cycles.\n\nThere are **6** homomorphisms from $\\mathbb{Z}_{12}$ to $S_3$.",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ],
    "outer_attempt": 1
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "Let $G$ be a connected Lie group with Lie algebra $\\mathfrak{g}$. Suppose that $\\mathfrak{g}$ is simple and that $G$ has a discrete subgroup $\\Gamma$ such that $G/\\Gamma$ is compact. Prove that $\\Gamma$ is contained in the center of $G$ if and only if the adjoint representation of $G$ on $\\mathfrak{g}$ descends to a representation of $G/\\Gamma$.",
    "run_id": "15",
    "topic_slug": "topological_groups_and_lie_groups",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "Here is the complete proof and analysis of the statement.\n\n### **Theorem**\nLet $G$ be a connected Lie group with simple Lie algebra $\\mathfrak{g}$. Let $\\Gamma$ be a discrete subgroup of $G$ such that the quotient space $G/\\Gamma$ is compact.\n\nThen: $\\Gamma \\subseteq Z(G) \\iff$ the adjoint representation $\\operatorname{Ad}: G \\to \\operatorname{GL}(\\mathfrak{g})$ descends to a representation of $G/\\Gamma$.\n\n---\n\n### **Proof**\n\n**1. Definitions and Preliminary Results**\n\n*   **Adjoint Representation**: The adjoint representation $\\operatorname{Ad}: G \\to \\operatorname{Aut}(\\mathfrak{g}) \\subset \\operatorname{GL}(\\mathfrak{g})$ is defined by $\\operatorname{Ad}(g) = (dL_g \\circ dR_{g^{-1}})_e$. Equivalently, it satisfies $g \\exp(X) g^{-1} = \\exp(\\operatorname{Ad}(g)X)$ for all $X \\in \\mathfrak{g}$.\n*   **Center of $G$**: The center is defined as $Z(G) = \\{g \\in G \\mid gh = hg, \\forall h \\in G\\}$.\n*   **Descending to a Representation**: We say a representation $\\rho: G \\to V$ descends to a representation of $G/\\Gamma$ if there exists a homomorphism $\\bar{\\rho}: G/\\Gamma \\to V$ such that $\\bar{\\rho} \\circ \\pi = \\rho$, where $\\pi: G \\to G/\\Gamma$ is the canonical projection.\n    *   **Implicit Condition**: For $G/\\Gamma$ to be the domain of a group homomorphism, $G/\\Gamma$ must be a group. This requires $\\Gamma$ to be a normal subgroup of $G$ ($\\Gamma \\unlhd G$).\n\n**Lemma**: For any connected Lie group $G$, the kernel of the adjoint representation is exactly the center of $G$. i.e., $\\ker(\\operatorname{Ad}) = Z(G)$.\n\n*Proof of Lemma*:\n$(\\supseteq)$ Let $z \\in Z(G)$. Then conjugation $c_z(g) = zgz^{-1} = g$ is the identity map on $G$. The differential of the identity map is the identity transformation. Thus, $\\operatorname{Ad}(z) = (dc_z)_e = I$. So $Z(G) \\subseteq \\ker(\\operatorname{Ad})$.\n$(\\subseteq)$ Let $g \\in \\ker(\\operatorname{Ad})$. Then $\\operatorname{Ad}(g) = I$. Using the identity $g \\exp(X) g^{-1} = \\exp(\\operatorname{Ad}(g)X)$, we have $g \\exp(X) g^{-1} = \\exp(X)$ for all $X \\in \\mathfrak{g}$. Thus, $g$ commutes with all elements in the image of the exponential map. Since $G$ is connected, the image of the exponential map generates $G$. Therefore, $g$ commutes with all of $G$, so $g \\in Z(G)$.\n$\\square$\n\n---\n\n**2. Proof of the Equivalence**\n\n**Direction $(\\Leftarrow)$:** Assume the adjoint representation descends to a representation of $G/\\Gamma$.\n\n1.  **Well-definedness**: Let $\\overline{\\operatorname{Ad}}: G/\\Gamma \\to \\operatorname{GL}(\\mathfrak{g})$ be the induced map. For this map to be well-defined, it must be constant on the fibers of the projection $\\pi$. That is, for any $g \\in G$ and $\\gamma \\in \\Gamma$, we must have $\\operatorname{Ad}(g\\gamma) = \\operatorname{Ad}(g)$.\n2.  **Kernel Condition**: Since $\\operatorname{Ad}$ is a homomorphism, $\\operatorname{Ad}(g\\gamma) = \\operatorname{Ad}(g)\\operatorname{Ad}(\\gamma)$. The condition $\\operatorname{Ad}(g)\\operatorname{Ad}(\\gamma) = \\operatorname{Ad}(g)$ implies $\\operatorname{Ad}(\\gamma) = I$ (since $\\operatorname{Ad}(g)$ is invertible).\n3.  **Conclusion**: This holds for all $\\gamma \\in \\Gamma$, so $\\Gamma \\subseteq \\ker(\\operatorname{Ad})$. By the Lemma, $\\ker(\\operatorname{Ad}) = Z(G)$. Therefore, $\\Gamma \\subseteq Z(G)$.\n\n**Direction $(\\Rightarrow)$:** Assume $\\Gamma \\subseteq Z(G)$.\n\n1.  **Subgroup Normality**: Since $\\Gamma$ is contained in the center, $\\Gamma$ is a normal subgroup of $G$. Thus, the quotient space $G/\\Gamma$ inherits a Lie group structure.\n2.  **Kernel Containment**: Since $\\Gamma \\subseteq Z(G)$ and $Z(G) = \\ker(\\operatorname{Ad})$ (by the Lemma), we have $\\Gamma \\subseteq \\ker(\\operatorname{Ad})$.\n3.  **Factorization**: By the fundamental theorem of homomorphisms (Isomorphism Theorem), if the kernel of a homomorphism $\\phi: G \\to H$ contains a normal subgroup $N$, then $\\phi$ factors uniquely through a homomorphism $\\bar{\\phi}: G/N \\to H$.\n4.  **Conclusion**: Therefore, $\\operatorname{Ad}$ descends to a well-defined homomorphism (representation) $\\overline{\\operatorname{Ad}}: G/\\Gamma \\to \\operatorname{GL}(\\mathfrak{g})$.\n\n---\n\n**3. Analysis of Constraints (Completeness)**\n\nThe problem specifies that $\\mathfrak{g}$ is simple and $G/\\Gamma$ is compact. While logical equivalence holds generally for connected $G$, these constraints dictate which direction the proof takes in terms of existence.\n\n*   **Case 1: $G$ is Compact.**\n    If $G$ is compact, $\\mathfrak{g}$ is a compact simple Lie algebra. $G/\\Gamma$ is compact for any discrete $\\Gamma$ (as discrete subgroups of compact groups are finite). The center $Z(G)$ is non-trivial but finite. It is perfectly possible for $\\Gamma \\subseteq Z(G)$, in which case both sides of the \"iff\" statement are **True**.\n\n*   **Case 2: $G$ is Non-Compact.**\n    If $G$ is non-compact and simple, then $\\mathfrak{g}$ is a non-compact simple Lie algebra.\n    *   Suppose $\\Gamma \\subseteq Z(G)$. Then $G/\\Gamma$ is a group locally isomorphic to $G$. Since $\\mathfrak{g}$ is non-compact simple, any group with this Lie algebra is non-compact. This contradicts the hypothesis that \"$G/\\Gamma$ is compact.\" Thus, $\\Gamma$ cannot be central.\n    *   Consequently, for non-compact $G$, the condition $\\Gamma \\subseteq Z(G)$ is always **False**.\n    *   By the equivalence proven above, \"Ad descends to $G/\\Gamma$\" is also **False**.\n    *   The statement holds as **False $\\iff$ False**.\n\n### **Final Conclusion**\n\nWe have proven that $\\Gamma$ is contained in the center of $G$ if and only if the adjoint representation descends to $G/\\Gamma$. This relies on the fact that for a connected Lie group, the kernel of the adjoint representation is exactly the center.\n\n**Result**: $\\Gamma \\subseteq Z(G) \\iff \\operatorname{Ad} \\text{ descends to } G/\\Gamma$.",
        "raw_answer": "Here is the complete proof and analysis of the statement.\n\n### **Theorem**\nLet $G$ be a connected Lie group with simple Lie algebra $\\mathfrak{g}$. Let $\\Gamma$ be a discrete subgroup of $G$ such that the quotient space $G/\\Gamma$ is compact.\n\nThen: $\\Gamma \\subseteq Z(G) \\iff$ the adjoint representation $\\operatorname{Ad}: G \\to \\operatorname{GL}(\\mathfrak{g})$ descends to a representation of $G/\\Gamma$.\n\n---\n\n### **Proof**\n\n**1. Definitions and Preliminary Results**\n\n*   **Adjoint Representation**: The adjoint representation $\\operatorname{Ad}: G \\to \\operatorname{Aut}(\\mathfrak{g}) \\subset \\operatorname{GL}(\\mathfrak{g})$ is defined by $\\operatorname{Ad}(g) = (dL_g \\circ dR_{g^{-1}})_e$. Equivalently, it satisfies $g \\exp(X) g^{-1} = \\exp(\\operatorname{Ad}(g)X)$ for all $X \\in \\mathfrak{g}$.\n*   **Center of $G$**: The center is defined as $Z(G) = \\{g \\in G \\mid gh = hg, \\forall h \\in G\\}$.\n*   **Descending to a Representation**: We say a representation $\\rho: G \\to V$ descends to a representation of $G/\\Gamma$ if there exists a homomorphism $\\bar{\\rho}: G/\\Gamma \\to V$ such that $\\bar{\\rho} \\circ \\pi = \\rho$, where $\\pi: G \\to G/\\Gamma$ is the canonical projection.\n    *   **Implicit Condition**: For $G/\\Gamma$ to be the domain of a group homomorphism, $G/\\Gamma$ must be a group. This requires $\\Gamma$ to be a normal subgroup of $G$ ($\\Gamma \\unlhd G$).\n\n**Lemma**: For any connected Lie group $G$, the kernel of the adjoint representation is exactly the center of $G$. i.e., $\\ker(\\operatorname{Ad}) = Z(G)$.\n\n*Proof of Lemma*:\n$(\\supseteq)$ Let $z \\in Z(G)$. Then conjugation $c_z(g) = zgz^{-1} = g$ is the identity map on $G$. The differential of the identity map is the identity transformation. Thus, $\\operatorname{Ad}(z) = (dc_z)_e = I$. So $Z(G) \\subseteq \\ker(\\operatorname{Ad})$.\n$(\\subseteq)$ Let $g \\in \\ker(\\operatorname{Ad})$. Then $\\operatorname{Ad}(g) = I$. Using the identity $g \\exp(X) g^{-1} = \\exp(\\operatorname{Ad}(g)X)$, we have $g \\exp(X) g^{-1} = \\exp(X)$ for all $X \\in \\mathfrak{g}$. Thus, $g$ commutes with all elements in the image of the exponential map. Since $G$ is connected, the image of the exponential map generates $G$. Therefore, $g$ commutes with all of $G$, so $g \\in Z(G)$.\n$\\square$\n\n---\n\n**2. Proof of the Equivalence**\n\n**Direction $(\\Leftarrow)$:** Assume the adjoint representation descends to a representation of $G/\\Gamma$.\n\n1.  **Well-definedness**: Let $\\overline{\\operatorname{Ad}}: G/\\Gamma \\to \\operatorname{GL}(\\mathfrak{g})$ be the induced map. For this map to be well-defined, it must be constant on the fibers of the projection $\\pi$. That is, for any $g \\in G$ and $\\gamma \\in \\Gamma$, we must have $\\operatorname{Ad}(g\\gamma) = \\operatorname{Ad}(g)$.\n2.  **Kernel Condition**: Since $\\operatorname{Ad}$ is a homomorphism, $\\operatorname{Ad}(g\\gamma) = \\operatorname{Ad}(g)\\operatorname{Ad}(\\gamma)$. The condition $\\operatorname{Ad}(g)\\operatorname{Ad}(\\gamma) = \\operatorname{Ad}(g)$ implies $\\operatorname{Ad}(\\gamma) = I$ (since $\\operatorname{Ad}(g)$ is invertible).\n3.  **Conclusion**: This holds for all $\\gamma \\in \\Gamma$, so $\\Gamma \\subseteq \\ker(\\operatorname{Ad})$. By the Lemma, $\\ker(\\operatorname{Ad}) = Z(G)$. Therefore, $\\Gamma \\subseteq Z(G)$.\n\n**Direction $(\\Rightarrow)$:** Assume $\\Gamma \\subseteq Z(G)$.\n\n1.  **Subgroup Normality**: Since $\\Gamma$ is contained in the center, $\\Gamma$ is a normal subgroup of $G$. Thus, the quotient space $G/\\Gamma$ inherits a Lie group structure.\n2.  **Kernel Containment**: Since $\\Gamma \\subseteq Z(G)$ and $Z(G) = \\ker(\\operatorname{Ad})$ (by the Lemma), we have $\\Gamma \\subseteq \\ker(\\operatorname{Ad})$.\n3.  **Factorization**: By the fundamental theorem of homomorphisms (Isomorphism Theorem), if the kernel of a homomorphism $\\phi: G \\to H$ contains a normal subgroup $N$, then $\\phi$ factors uniquely through a homomorphism $\\bar{\\phi}: G/N \\to H$.\n4.  **Conclusion**: Therefore, $\\operatorname{Ad}$ descends to a well-defined homomorphism (representation) $\\overline{\\operatorname{Ad}}: G/\\Gamma \\to \\operatorname{GL}(\\mathfrak{g})$.\n\n---\n\n**3. Analysis of Constraints (Completeness)**\n\nThe problem specifies that $\\mathfrak{g}$ is simple and $G/\\Gamma$ is compact. While logical equivalence holds generally for connected $G$, these constraints dictate which direction the proof takes in terms of existence.\n\n*   **Case 1: $G$ is Compact.**\n    If $G$ is compact, $\\mathfrak{g}$ is a compact simple Lie algebra. $G/\\Gamma$ is compact for any discrete $\\Gamma$ (as discrete subgroups of compact groups are finite). The center $Z(G)$ is non-trivial but finite. It is perfectly possible for $\\Gamma \\subseteq Z(G)$, in which case both sides of the \"iff\" statement are **True**.\n\n*   **Case 2: $G$ is Non-Compact.**\n    If $G$ is non-compact and simple, then $\\mathfrak{g}$ is a non-compact simple Lie algebra.\n    *   Suppose $\\Gamma \\subseteq Z(G)$. Then $G/\\Gamma$ is a group locally isomorphic to $G$. Since $\\mathfrak{g}$ is non-compact simple, any group with this Lie algebra is non-compact. This contradicts the hypothesis that \"$G/\\Gamma$ is compact.\" Thus, $\\Gamma$ cannot be central.\n    *   Consequently, for non-compact $G$, the condition $\\Gamma \\subseteq Z(G)$ is always **False**.\n    *   By the equivalence proven above, \"Ad descends to $G/\\Gamma$\" is also **False**.\n    *   The statement holds as **False $\\iff$ False**.\n\n### **Final Conclusion**\n\nWe have proven that $\\Gamma$ is contained in the center of $G$ if and only if the adjoint representation descends to $G/\\Gamma$. This relies on the fact that for a connected Lie group, the kernel of the adjoint representation is exactly the center.\n\n**Result**: $\\Gamma \\subseteq Z(G) \\iff \\operatorname{Ad} \\text{ descends to } G/\\Gamma$.",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ],
    "outer_attempt": 1
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "## Step 1: Understand the task and the requirements for the problem.\nThe task is to generate a challenging but solvable mathematics problem in the domain of real functions, along with a complete and verifiable solution. The problem must be well-posed, clear, non-trivial, and solvable with established mathematical methods.\n\n## Step 2: Determine the specific area within real functions to focus on.\nGiven the broad domain of real functions, we can consider various aspects such as differentiation, integration, functional equations, or properties of specific types of functions (e.g., continuous, differentiable, integrable).\n\n## Step 3: Choose a specific topic within real functions that can be used to create a challenging problem.\nLet's consider the topic of functional equations, which involves finding a function that satisfies a given equation involving the function and its inputs.\n\n## Step 4: Generate a challenging problem in the chosen topic.\nConsider the functional equation $f(x + y) = \\frac{f(x) + f(y)}{1 - f(x)f(y)}$ for $x, y \\in \\mathbb{R}$, where $f: \\mathbb{R} \\to \\mathbb{R}$. The task is to find all $f$ that satisfy this equation.\n\n## Step 5: Ensure the problem is well-posed and meets the quality rubric.\nThe given functional equation is well-defined, and the task is clear. We need to verify that it's non-trivial and solvable.\n\n## Step 6: Provide a solution to the generated problem.\nTo solve $f(x + y) = \\frac{f(x) + f(y)}{1 - f(x)f(y)}$, let's first check if there are any obvious solutions. For $y = 0$, we get $f(x) = \\frac{f(x) + f(0)}{1 - f(x)f(0)}$. This simplifies to $f(x)(1 - f(x)f(0)) = f(x) + f(0)$, or $f(x) - f(x)^2f(0) = f(x) + f(0)$, leading to $-f(x)^2f(0) = f(0)$. If $f(0) \\neq 0$, this gives $f(x)^2 = -1$ for all $x$, which is impossible for real-valued $f$. Thus, $f(0) = 0$.\n\n## Step 7: Continue solving the functional equation.\nSetting $y = x$, we have $f(2x) = \\frac{2f(x)}{1 - f(x)^2}$. This is a useful relation but let's first explore the derivative of $f$ if it exists. Assuming $f$ is differentiable, differentiating both sides with respect to $y$ and then setting $y = 0$ gives $f'(x) = \\frac{f'(0)(1 + f(x)^2)}{(1 - f(x)f(0))^2} = f'(0)(1 + f(x)^2)$ because $f(0) = 0$. So, $f'(x) = f'(0)(1 + f(x)^2)$.\n\n## Step 8: Solve the differential equation obtained from the functional equation.\nThe differential equation $f'(x) = f'(0)(1 + f(x)^2)$ can be solved by separation of variables: $\\frac{df}{1 + f^2} = f'(0)dx$. Integrating both sides gives $\\arctan(f(x)) = f'(0)x + C$, where $C$ is a constant. Since $f(0) = 0$, we have $C = 0$. Thus, $f(x) = \\tan(f'(0)x)$.\n\n## Step 9: Verify that the solution satisfies the original functional equation.\nSubstituting $f(x) = \\tan(f'(0)x)$ into the original equation: $\\tan(f'(0)(x+y)) = \\frac{\\tan(f'(0)x) + \\tan(f'(0)y)}{1 - \\tan(f'(0)x)\\tan(f'(0)y)}$, which is a known trigonometric identity, thus verifying that $f(x) = \\tan(f'(0)x)$ is a solution.\n\n## Step 10: Check if there are any constraints on $f'(0)$.\nFor $f(x) = \\tan(f'(0)x)$ to be defined for all $x \\in \\mathbb{R}$, $f'(0)$ must be $0$, otherwise, $f(x)$ will have discontinuities. If $f'(0) = 0$, then $f(x) = 0$ for all $x$, which is indeed a solution.\n\n## Step 11: Formulate the final problem and solution.\nLet's formulate the problem: \"Find all $f: \\mathbb{R} \\to \\mathbb{R}$ that satisfy $f(x + y) = \\frac{f(x) + f(y)}{1 - f(x)f(y)}$ for all $x, y \\in \\mathbb{R}$.\"\n\n\nFind all $f: \\mathbb{R} \\to \\mathbb{R}$ that satisfy $f(x + y) = \\frac{f(x) + f(y)}{1 - f(x)f(y)}$ for all $x, y \\in \\mathbb{R}$.",
    "run_id": "16",
    "topic_slug": "real_functions",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "To find all functions $f: \\mathbb{R} \\to \\mathbb{R}$ satisfying the equation\n$$f(x + y) = \\frac{f(x) + f(y)}{1 - f(x)f(y)}$$\nfor all $x, y \\in \\mathbb{R}$, we proceed with the following steps.\n\n### Step 1: Preliminary Analysis and Values\n\nFirst, we determine the value of $f(0)$. Letting $y = 0$ in the functional equation, we get:\n$$f(x) = \\frac{f(x) + f(0)}{1 - f(x)f(0)}$$\nAssuming the denominator is non-zero, we multiply through:\n$$f(x) [1 - f(x)f(0)] = f(x) + f(0)$$\n$$f(x) - f(x)^2 f(0) = f(x) + f(0)$$\n$$- f(x)^2 f(0) = f(0)$$\n$$f(0) [1 + f(x)^2] = 0$$\nSince $1 + f(x)^2 \\geq 1$ for any real $f(x)$, it is never zero. Therefore, we must have:\n$$f(0) = 0$$\n\nNext, we establish that $f(x)$ is an odd function. Letting $y = -x$, and using $f(0)=0$:\n$$f(0) = \\frac{f(x) + f(-x)}{1 - f(x)f(-x)} \\implies 0 = \\frac{f(x) + f(-x)}{1 - f(x)f(-x)}$$\nThis implies the numerator must be zero:\n$$f(x) + f(-x) = 0 \\implies f(-x) = -f(x)$$\n\n### Step 2: Domain Constraints and Range\n\nThe functional equation is valid only if the denominator $1 - f(x)f(y)$ is never zero. Thus, for all $x, y \\in \\mathbb{R}$:\n$$f(x)f(y) \\neq 1$$\nSetting $y = x$, we have $f(x)^2 \\neq 1$. This implies:\n$$f(x) \\neq 1 \\quad \\text{and} \\quad f(x) \\neq -1 \\quad \\text{for all } x \\in \\mathbb{R}.$$\n\n### Step 3: Transforming the Equation\n\nSince $f(x)$ never takes the values $\\pm 1$, the function $\\arctan(f(x))$ is well-defined. However, the standard arctangent maps to $(-\\pi/2, \\pi/2)$, while the general solution to $\\tan \\theta = z$ is $\\theta = \\arctan z + k\\pi$.\n\nLet us express $f(x)$ in terms of an auxiliary function $L(x)$:\n$$f(x) = \\tan(L(x))$$\nwhere $L(x)$ is a real-valued function. Substituting this into the original equation:\n$$\\tan(L(x+y)) = \\frac{\\tan(L(x)) + \\tan(L(y))}{1 - \\tan(L(x))\\tan(L(y))}$$\nBy the tangent addition formula, the right-hand side is $\\tan(L(x) + L(y))$. Thus:\n$$\\tan(L(x+y)) = \\tan(L(x) + L(y))$$\nThe general solution to $\\tan \\alpha = \\tan \\beta$ is $\\alpha = \\beta + k\\pi$ for some integer $k$. Therefore:\n$$L(x+y) = L(x) + L(y) + \\pi k(x, y)$$\nwhere $k(x, y)$ is an integer-valued function. This indicates that $L$ behaves like an additive function modulo $\\pi$. We can choose $L$ specifically to be an additive function $A: \\mathbb{R} \\to \\mathbb{R}$ satisfying the Cauchy functional equation:\n$$A(x+y) = A(x) + A(y)$$\nThen $f(x) = \\tan(A(x))$ satisfies the functional equation, provided the denominator condition is met.\n\n### Step 4: Validating the Constraint\n\nWe must ensure that the denominator in the original equation is never zero.\n$$1 - f(x)f(y) \\neq 0 \\iff 1 - \\tan(A(x))\\tan(A(y)) \\neq 0$$\nUsing the identity $\\cos(A+B) = \\cos A \\cos B - \\sin A \\sin B = \\cos A \\cos B (1 - \\tan A \\tan B)$, the condition is equivalent to:\n$$\\cos(A(x) + A(y)) \\neq 0$$\nThis holds if and only if:\n$$A(x) + A(y) \\neq \\frac{\\pi}{2} + n\\pi \\quad \\text{for all } n \\in \\mathbb{Z}$$\nSince $A$ is additive, $A(x+y) = A(x) + A(y)$. Let $z = x+y$. As $x$ and $y$ vary over $\\mathbb{R}$, $z$ varies over $\\mathbb{R}$. The condition simplifies to:\n$$\\text{Range}(A) \\cap \\left\\{ \\frac{\\pi}{2} + n\\pi \\mid n \\in \\mathbb{Z} \\right\\} = \\emptyset$$\n\n### Step 5: Classifying Solutions\n\n**Case 1: Continuous or Measurable Solutions**\nIf $f$ is continuous (or even Lebesgue measurable), then $A(x) = \\arctan(f(x))$ (lifted continuously) must be continuous. The only continuous solutions to the Cauchy functional equation $A(x+y) = A(x) + A(y)$ are of the form $A(x) = cx$ for some constant $c \\in \\mathbb{R}$.\nThe condition that the range of $A(x) = cx$ avoids the set $\\{\\pi/2 + n\\pi\\}$ implies that the range cannot be all of $\\mathbb{R}$. Since the range of $A(x) = cx$ is $\\mathbb{R}$ unless $c=0$ (in which case the range is $\\{0\\}$), we must have $c = 0$.\nIf $c = 0$, then $A(x) = 0$ for all $x$, which implies $f(x) = \\tan(0) = 0$.\n\n**Case 2: General Solutions**\nUsing the Axiom of Choice, there exist non-continuous additive functions (constructed using a Hamel basis of $\\mathbb{R}$ over $\\mathbb{Q}$).\nLet $A: \\mathbb{R} \\to \\mathbb{R}$ be an additive function. For $f(x) = \\tan(A(x))$ to be a solution, the image of $A$ must avoid the values $\\frac{\\pi}{2} + n\\pi$.\nOne class of such functions can be constructed by defining $A: \\mathbb{R} \\to \\mathbb{Q}$ (viewing $\\mathbb{R}$ as a vector space over $\\mathbb{Q}$ and mapping basis elements to rationals). Since $\\pi$ is irrational, the set $\\mathbb{Q}$ contains no elements of the form $\\frac{\\pi}{2} + n\\pi$ (which are irrational). Thus, any additive function with a rational range yields a valid non-trivial solution.\n\n### Conclusion\n\nThe solutions fall into two categories depending on regularity assumptions:\n\n1.  **Continuous Solution:** The only continuous function satisfying the equation is the zero function:\n    $$f(x) = 0$$\n\n2.  **General Solution:** The set of all solutions is given by:\n    $$f(x) = \\tan(A(x))$$\n    where $A: \\mathbb{R} \\to \\mathbb{R}$ is an additive function (i.e., $A(x+y) = A(x) + A(y)$) such that the range of $A$ contains no values of the form $\\frac{\\pi}{2} + n\\pi$ for any integer $n$.",
        "raw_answer": "To find all functions $f: \\mathbb{R} \\to \\mathbb{R}$ satisfying the equation\n$$f(x + y) = \\frac{f(x) + f(y)}{1 - f(x)f(y)}$$\nfor all $x, y \\in \\mathbb{R}$, we proceed with the following steps.\n\n### Step 1: Preliminary Analysis and Values\n\nFirst, we determine the value of $f(0)$. Letting $y = 0$ in the functional equation, we get:\n$$f(x) = \\frac{f(x) + f(0)}{1 - f(x)f(0)}$$\nAssuming the denominator is non-zero, we multiply through:\n$$f(x) [1 - f(x)f(0)] = f(x) + f(0)$$\n$$f(x) - f(x)^2 f(0) = f(x) + f(0)$$\n$$- f(x)^2 f(0) = f(0)$$\n$$f(0) [1 + f(x)^2] = 0$$\nSince $1 + f(x)^2 \\geq 1$ for any real $f(x)$, it is never zero. Therefore, we must have:\n$$f(0) = 0$$\n\nNext, we establish that $f(x)$ is an odd function. Letting $y = -x$, and using $f(0)=0$:\n$$f(0) = \\frac{f(x) + f(-x)}{1 - f(x)f(-x)} \\implies 0 = \\frac{f(x) + f(-x)}{1 - f(x)f(-x)}$$\nThis implies the numerator must be zero:\n$$f(x) + f(-x) = 0 \\implies f(-x) = -f(x)$$\n\n### Step 2: Domain Constraints and Range\n\nThe functional equation is valid only if the denominator $1 - f(x)f(y)$ is never zero. Thus, for all $x, y \\in \\mathbb{R}$:\n$$f(x)f(y) \\neq 1$$\nSetting $y = x$, we have $f(x)^2 \\neq 1$. This implies:\n$$f(x) \\neq 1 \\quad \\text{and} \\quad f(x) \\neq -1 \\quad \\text{for all } x \\in \\mathbb{R}.$$\n\n### Step 3: Transforming the Equation\n\nSince $f(x)$ never takes the values $\\pm 1$, the function $\\arctan(f(x))$ is well-defined. However, the standard arctangent maps to $(-\\pi/2, \\pi/2)$, while the general solution to $\\tan \\theta = z$ is $\\theta = \\arctan z + k\\pi$.\n\nLet us express $f(x)$ in terms of an auxiliary function $L(x)$:\n$$f(x) = \\tan(L(x))$$\nwhere $L(x)$ is a real-valued function. Substituting this into the original equation:\n$$\\tan(L(x+y)) = \\frac{\\tan(L(x)) + \\tan(L(y))}{1 - \\tan(L(x))\\tan(L(y))}$$\nBy the tangent addition formula, the right-hand side is $\\tan(L(x) + L(y))$. Thus:\n$$\\tan(L(x+y)) = \\tan(L(x) + L(y))$$\nThe general solution to $\\tan \\alpha = \\tan \\beta$ is $\\alpha = \\beta + k\\pi$ for some integer $k$. Therefore:\n$$L(x+y) = L(x) + L(y) + \\pi k(x, y)$$\nwhere $k(x, y)$ is an integer-valued function. This indicates that $L$ behaves like an additive function modulo $\\pi$. We can choose $L$ specifically to be an additive function $A: \\mathbb{R} \\to \\mathbb{R}$ satisfying the Cauchy functional equation:\n$$A(x+y) = A(x) + A(y)$$\nThen $f(x) = \\tan(A(x))$ satisfies the functional equation, provided the denominator condition is met.\n\n### Step 4: Validating the Constraint\n\nWe must ensure that the denominator in the original equation is never zero.\n$$1 - f(x)f(y) \\neq 0 \\iff 1 - \\tan(A(x))\\tan(A(y)) \\neq 0$$\nUsing the identity $\\cos(A+B) = \\cos A \\cos B - \\sin A \\sin B = \\cos A \\cos B (1 - \\tan A \\tan B)$, the condition is equivalent to:\n$$\\cos(A(x) + A(y)) \\neq 0$$\nThis holds if and only if:\n$$A(x) + A(y) \\neq \\frac{\\pi}{2} + n\\pi \\quad \\text{for all } n \\in \\mathbb{Z}$$\nSince $A$ is additive, $A(x+y) = A(x) + A(y)$. Let $z = x+y$. As $x$ and $y$ vary over $\\mathbb{R}$, $z$ varies over $\\mathbb{R}$. The condition simplifies to:\n$$\\text{Range}(A) \\cap \\left\\{ \\frac{\\pi}{2} + n\\pi \\mid n \\in \\mathbb{Z} \\right\\} = \\emptyset$$\n\n### Step 5: Classifying Solutions\n\n**Case 1: Continuous or Measurable Solutions**\nIf $f$ is continuous (or even Lebesgue measurable), then $A(x) = \\arctan(f(x))$ (lifted continuously) must be continuous. The only continuous solutions to the Cauchy functional equation $A(x+y) = A(x) + A(y)$ are of the form $A(x) = cx$ for some constant $c \\in \\mathbb{R}$.\nThe condition that the range of $A(x) = cx$ avoids the set $\\{\\pi/2 + n\\pi\\}$ implies that the range cannot be all of $\\mathbb{R}$. Since the range of $A(x) = cx$ is $\\mathbb{R}$ unless $c=0$ (in which case the range is $\\{0\\}$), we must have $c = 0$.\nIf $c = 0$, then $A(x) = 0$ for all $x$, which implies $f(x) = \\tan(0) = 0$.\n\n**Case 2: General Solutions**\nUsing the Axiom of Choice, there exist non-continuous additive functions (constructed using a Hamel basis of $\\mathbb{R}$ over $\\mathbb{Q}$).\nLet $A: \\mathbb{R} \\to \\mathbb{R}$ be an additive function. For $f(x) = \\tan(A(x))$ to be a solution, the image of $A$ must avoid the values $\\frac{\\pi}{2} + n\\pi$.\nOne class of such functions can be constructed by defining $A: \\mathbb{R} \\to \\mathbb{Q}$ (viewing $\\mathbb{R}$ as a vector space over $\\mathbb{Q}$ and mapping basis elements to rationals). Since $\\pi$ is irrational, the set $\\mathbb{Q}$ contains no elements of the form $\\frac{\\pi}{2} + n\\pi$ (which are irrational). Thus, any additive function with a rational range yields a valid non-trivial solution.\n\n### Conclusion\n\nThe solutions fall into two categories depending on regularity assumptions:\n\n1.  **Continuous Solution:** The only continuous function satisfying the equation is the zero function:\n    $$f(x) = 0$$\n\n2.  **General Solution:** The set of all solutions is given by:\n    $$f(x) = \\tan(A(x))$$\n    where $A: \\mathbb{R} \\to \\mathbb{R}$ is an additive function (i.e., $A(x+y) = A(x) + A(y)$) such that the range of $A$ contains no values of the form $\\frac{\\pi}{2} + n\\pi$ for any integer $n$.",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ],
    "outer_attempt": 1
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "Let $f: [0,1] \\to \\mathbb{R}$ be a bounded measurable function. Prove that $\\lim_{n \\to \\infty} \\int_{0}^{1} f(x) \\sin(nx) dx = 0$.",
    "run_id": "17",
    "topic_slug": "measure_and_integration",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "This problem asks for a proof of the **Riemann-Lebesgue Lemma** in the specific case where the function $f$ is bounded and measurable on $[0,1]$. Below is a rigorous proof based on the density of step functions in $L^1([0,1])$.\n\n### 1. Preliminaries and Integrability\n\nFirst, we establish that the integral exists. Since $f: [0,1] \\to \\mathbb{R}$ is measurable and bounded, there exists a constant $M > 0$ such that $|f(x)| \\le M$ for all $x \\in [0,1]$. The domain $[0,1]$ has finite measure (specifically, Lebesgue measure $\\mu([0,1]) = 1$).\n\nTherefore, $f$ is Lebesgue integrable:\n$$ \\int_0^1 |f(x)| \\, dx \\le \\int_0^1 M \\, dx = M < \\infty. $$\nThus, $f \\in L^1([0,1])$.\n\n### 2. Proof Strategy\n\nWe will use a density argument. The strategy is as follows:\n1.  Prove the limit is zero for the characteristic function of an interval (the simplest type of function).\n2.  Extend this result to **step functions** (finite linear combinations of characteristic functions of intervals) by linearity.\n3.  Use the fact that step functions are dense in $L^1([0,1])$ to approximate $f$ and bound the error.\n\n### 3. Step 1: Characteristic Functions of Intervals\n\nLet $I = (a, b) \\subseteq [0,1]$ be an open interval. Let $\\chi_I(x)$ be the characteristic function of $I$, defined as $\\chi_I(x) = 1$ if $x \\in (a, b)$ and $0$ otherwise.\n\nConsider the integral for this function:\n$$ \\int_0^1 \\chi_I(x) \\sin(nx) \\, dx = \\int_a^b \\sin(nx) \\, dx $$\nWe evaluate this integral directly:\n$$ \\int_a^b \\sin(nx) \\, dx = \\left[ -\\frac{\\cos(nx)}{n} \\right]_a^b = \\frac{\\cos(na) - \\cos(nb)}{n} $$\nSince $|\\cos(\\theta)| \\le 1$ for any real $\\theta$, we can bound the absolute value of the integral:\n$$ \\left| \\int_0^1 \\chi_I(x) \\sin(nx) \\, dx \\right| = \\left| \\frac{\\cos(na) - \\cos(nb)}{n} \\right| \\le \\frac{|\\cos(na)| + |\\cos(nb)|}{n} \\le \\frac{2}{n} $$\nAs $n \\to \\infty$, $\\frac{2}{n} \\to 0$. Thus:\n$$ \\lim_{n \\to \\infty} \\int_0^1 \\chi_I(x) \\sin(nx) \\, dx = 0 $$\n\n### 4. Step 2: Step Functions\n\nLet $\\psi(x)$ be a step function on $[0,1]$. A step function can be written as a finite linear combination of characteristic functions of disjoint intervals:\n$$ \\psi(x) = \\sum_{k=1}^m c_k \\chi_{I_k}(x) $$\nwhere $c_k \\in \\mathbb{R}$ are constants and $I_k \\subseteq [0,1]$ are intervals.\n\nBy the linearity of the integral:\n$$ \\int_0^1 \\psi(x) \\sin(nx) \\, dx = \\sum_{k=1}^m c_k \\int_0^1 \\chi_{I_k}(x) \\sin(nx) \\, dx $$\nTaking the limit as $n \\to \\infty$:\n$$ \\lim_{n \\to \\infty} \\int_0^1 \\psi(x) \\sin(nx) \\, dx = \\sum_{k=1}^m c_k \\left( \\lim_{n \\to \\infty} \\int_0^1 \\chi_{I_k}(x) \\sin(nx) \\, dx \\right) $$\nFrom Step 1, each term in the sum is zero. Therefore:\n$$ \\lim_{n \\to \\infty} \\int_0^1 \\psi(x) \\sin(nx) \\, dx = 0 $$\n\n### 5. Step 3: Approximation of $f$\n\nSince $f \\in L^1([0,1])$, for any $\\epsilon > 0$, there exists a step function $\\psi$ such that:\n$$ \\int_0^1 |f(x) - \\psi(x)| \\, dx < \\frac{\\epsilon}{2} $$\n(This is a standard result in measure theory: step functions are dense in $L^1$).\n\nWe decompose the target integral:\n$$ \\int_0^1 f(x) \\sin(nx) \\, dx = \\int_0^1 (f(x) - \\psi(x)) \\sin(nx) \\, dx + \\int_0^1 \\psi(x) \\sin(nx) \\, dx $$\nNow, take the absolute value and apply the triangle inequality:\n$$ \\left| \\int_0^1 f(x) \\sin(nx) \\, dx \\right| \\le \\left| \\int_0^1 (f(x) - \\psi(x)) \\sin(nx) \\, dx \\right| + \\left| \\int_0^1 \\psi(x) \\sin(nx) \\, dx \\right| $$\n\n**Bounding the first term:**\nUsing the property $|\\sin(nx)| \\le 1$:\n$$ \\left| \\int_0^1 (f(x) - \\psi(x)) \\sin(nx) \\, dx \\right| \\le \\int_0^1 |f(x) - \\psi(x)| \\cdot |\\sin(nx)| \\, dx \\le \\int_0^1 |f(x) - \\psi(x)| \\, dx < \\frac{\\epsilon}{2} $$\n\n**Bounding the second term:**\nFrom Step 2, we know that $\\lim_{n \\to \\infty} \\int_0^1 \\psi(x) \\sin(nx) \\, dx = 0$.\nTherefore, there exists an integer $N$ such that for all $n > N$:\n$$ \\left| \\int_0^1 \\psi(x) \\sin(nx) \\, dx \\right| < \\frac{\\epsilon}{2} $$\n\n### 6. Conclusion\n\nCombining the bounds, for any given $\\epsilon > 0$, there exists an $N$ such that for all $n > N$:\n$$ \\left| \\int_0^1 f(x) \\sin(nx) \\, dx \\right| < \\frac{\\epsilon}{2} + \\frac{\\epsilon}{2} = \\epsilon $$\nBy the definition of the limit, this proves that:\n$$ \\lim_{n \\to \\infty} \\int_{0}^{1} f(x) \\sin(nx) \\, dx = 0 $$",
        "raw_answer": "This problem asks for a proof of the **Riemann-Lebesgue Lemma** in the specific case where the function $f$ is bounded and measurable on $[0,1]$. Below is a rigorous proof based on the density of step functions in $L^1([0,1])$.\n\n### 1. Preliminaries and Integrability\n\nFirst, we establish that the integral exists. Since $f: [0,1] \\to \\mathbb{R}$ is measurable and bounded, there exists a constant $M > 0$ such that $|f(x)| \\le M$ for all $x \\in [0,1]$. The domain $[0,1]$ has finite measure (specifically, Lebesgue measure $\\mu([0,1]) = 1$).\n\nTherefore, $f$ is Lebesgue integrable:\n$$ \\int_0^1 |f(x)| \\, dx \\le \\int_0^1 M \\, dx = M < \\infty. $$\nThus, $f \\in L^1([0,1])$.\n\n### 2. Proof Strategy\n\nWe will use a density argument. The strategy is as follows:\n1.  Prove the limit is zero for the characteristic function of an interval (the simplest type of function).\n2.  Extend this result to **step functions** (finite linear combinations of characteristic functions of intervals) by linearity.\n3.  Use the fact that step functions are dense in $L^1([0,1])$ to approximate $f$ and bound the error.\n\n### 3. Step 1: Characteristic Functions of Intervals\n\nLet $I = (a, b) \\subseteq [0,1]$ be an open interval. Let $\\chi_I(x)$ be the characteristic function of $I$, defined as $\\chi_I(x) = 1$ if $x \\in (a, b)$ and $0$ otherwise.\n\nConsider the integral for this function:\n$$ \\int_0^1 \\chi_I(x) \\sin(nx) \\, dx = \\int_a^b \\sin(nx) \\, dx $$\nWe evaluate this integral directly:\n$$ \\int_a^b \\sin(nx) \\, dx = \\left[ -\\frac{\\cos(nx)}{n} \\right]_a^b = \\frac{\\cos(na) - \\cos(nb)}{n} $$\nSince $|\\cos(\\theta)| \\le 1$ for any real $\\theta$, we can bound the absolute value of the integral:\n$$ \\left| \\int_0^1 \\chi_I(x) \\sin(nx) \\, dx \\right| = \\left| \\frac{\\cos(na) - \\cos(nb)}{n} \\right| \\le \\frac{|\\cos(na)| + |\\cos(nb)|}{n} \\le \\frac{2}{n} $$\nAs $n \\to \\infty$, $\\frac{2}{n} \\to 0$. Thus:\n$$ \\lim_{n \\to \\infty} \\int_0^1 \\chi_I(x) \\sin(nx) \\, dx = 0 $$\n\n### 4. Step 2: Step Functions\n\nLet $\\psi(x)$ be a step function on $[0,1]$. A step function can be written as a finite linear combination of characteristic functions of disjoint intervals:\n$$ \\psi(x) = \\sum_{k=1}^m c_k \\chi_{I_k}(x) $$\nwhere $c_k \\in \\mathbb{R}$ are constants and $I_k \\subseteq [0,1]$ are intervals.\n\nBy the linearity of the integral:\n$$ \\int_0^1 \\psi(x) \\sin(nx) \\, dx = \\sum_{k=1}^m c_k \\int_0^1 \\chi_{I_k}(x) \\sin(nx) \\, dx $$\nTaking the limit as $n \\to \\infty$:\n$$ \\lim_{n \\to \\infty} \\int_0^1 \\psi(x) \\sin(nx) \\, dx = \\sum_{k=1}^m c_k \\left( \\lim_{n \\to \\infty} \\int_0^1 \\chi_{I_k}(x) \\sin(nx) \\, dx \\right) $$\nFrom Step 1, each term in the sum is zero. Therefore:\n$$ \\lim_{n \\to \\infty} \\int_0^1 \\psi(x) \\sin(nx) \\, dx = 0 $$\n\n### 5. Step 3: Approximation of $f$\n\nSince $f \\in L^1([0,1])$, for any $\\epsilon > 0$, there exists a step function $\\psi$ such that:\n$$ \\int_0^1 |f(x) - \\psi(x)| \\, dx < \\frac{\\epsilon}{2} $$\n(This is a standard result in measure theory: step functions are dense in $L^1$).\n\nWe decompose the target integral:\n$$ \\int_0^1 f(x) \\sin(nx) \\, dx = \\int_0^1 (f(x) - \\psi(x)) \\sin(nx) \\, dx + \\int_0^1 \\psi(x) \\sin(nx) \\, dx $$\nNow, take the absolute value and apply the triangle inequality:\n$$ \\left| \\int_0^1 f(x) \\sin(nx) \\, dx \\right| \\le \\left| \\int_0^1 (f(x) - \\psi(x)) \\sin(nx) \\, dx \\right| + \\left| \\int_0^1 \\psi(x) \\sin(nx) \\, dx \\right| $$\n\n**Bounding the first term:**\nUsing the property $|\\sin(nx)| \\le 1$:\n$$ \\left| \\int_0^1 (f(x) - \\psi(x)) \\sin(nx) \\, dx \\right| \\le \\int_0^1 |f(x) - \\psi(x)| \\cdot |\\sin(nx)| \\, dx \\le \\int_0^1 |f(x) - \\psi(x)| \\, dx < \\frac{\\epsilon}{2} $$\n\n**Bounding the second term:**\nFrom Step 2, we know that $\\lim_{n \\to \\infty} \\int_0^1 \\psi(x) \\sin(nx) \\, dx = 0$.\nTherefore, there exists an integer $N$ such that for all $n > N$:\n$$ \\left| \\int_0^1 \\psi(x) \\sin(nx) \\, dx \\right| < \\frac{\\epsilon}{2} $$\n\n### 6. Conclusion\n\nCombining the bounds, for any given $\\epsilon > 0$, there exists an $N$ such that for all $n > N$:\n$$ \\left| \\int_0^1 f(x) \\sin(nx) \\, dx \\right| < \\frac{\\epsilon}{2} + \\frac{\\epsilon}{2} = \\epsilon $$\nBy the definition of the limit, this proves that:\n$$ \\lim_{n \\to \\infty} \\int_{0}^{1} f(x) \\sin(nx) \\, dx = 0 $$",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ],
    "outer_attempt": 1
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "## Step 1: Understand the task\nThe task is to generate a challenging mathematics problem in the domain of \"Functions of a complex variable\" along with a complete and verifiable solution. The problem must be well-posed, clear, non-trivial, and solvable with established mathematical methods.\n\n## Step 2: Identify the domain and requirements\nThe domain is \"Functions of a complex variable.\" This involves complex analysis, including topics like analytic functions, contour integration, and possibly properties of specific functions like the exponential or logarithmic function in the complex plane.\n\n## Step 3: Generate a problem\nTo create a challenging problem, we should consider a topic that requires a good understanding of complex analysis. One such topic is contour integration and the application of Cauchy's Integral Formula or the Residue Theorem.\n\n## Step 4: Formulate the problem\nLet's consider a problem that involves evaluating a contour integral using the Residue Theorem, which is a fundamental technique in complex analysis.\n\n## Step 5: Specify the problem\nThe problem could be: \"Evaluate the integral $\\oint_{|z|=2} \\frac{e^z}{z^2 + 1} dz$, where the contour $|z| = 2$ is traversed in the positive direction.\"\n\n## Step 6: Provide the solution\nTo solve $\\oint_{|z|=2} \\frac{e^z}{z^2 + 1} dz$, first identify the singularities of the integrand within the contour $|z| = 2$. The singularities occur at $z = \\pm i$, both of which are inside $|z| = 2$ since $|i| = |-i| = 1 < 2$.\n\n## Step 7: Apply the Residue Theorem\nThe Residue Theorem states that $\\oint_C f(z) dz = 2\\pi i \\sum \\text{Res}(f, z_j)$, where $z_j$ are the singularities enclosed by $C$. We need to calculate the residues at $z = i$ and $z = -i$.\n\n## Step 8: Calculate the residue at $z = i$\nThe residue of $\\frac{e^z}{z^2 + 1}$ at $z = i$ can be found using the formula for simple poles: $\\text{Res}(f, z_0) = \\lim_{z \\to z_0} (z - z_0)f(z)$. So, $\\text{Res}(f, i) = \\lim_{z \\to i} (z - i)\\frac{e^z}{(z - i)(z + i)} = \\lim_{z \\to i} \\frac{e^z}{z + i} = \\frac{e^i}{2i}$.\n\n## Step 9: Calculate the residue at $z = -i$\nSimilarly, $\\text{Res}(f, -i) = \\lim_{z \\to -i} (z + i)\\frac{e^z}{(z + i)(z - i)} = \\lim_{z \\to -i} \\frac{e^z}{z - i} = \\frac{e^{-i}}{-2i}$.\n\n## Step 10: Sum the residues and apply the Residue Theorem\n$\\sum \\text{Res} = \\frac{e^i}{2i} - \\frac{e^{-i}}{2i} = \\frac{e^i - e^{-i}}{2i} = \\sin(1)$, because $e^i - e^{-i} = 2i\\sin(1)$.\n\n## Step 11: Calculate the integral\n$\\oint_{|z|=2} \\frac{e^z}{z^2 + 1} dz = 2\\pi i \\sin(1)$.\n\n## Step 12: Write the final answer in the required format\n\nEvaluate the integral $\\oint_{|z|=2} \\frac{e^z}{z^2 + 1} dz$, where the contour $|z| = 2$ is traversed in the positive direction.",
    "run_id": "18",
    "topic_slug": "functions_of_a_complex_variable",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "**Problem Statement**\nEvaluate the contour integral\n$$ I = \\oint_{|z|=2} \\frac{e^z}{z^2 + 1} \\, dz $$\nwhere the contour $C = \\{z \\in \\mathbb{C} : |z| = 2\\}$ is traversed in the positive (counter-clockwise) direction.\n\n**Solution**\n\n**1. Identification of Singularities**\nLet $f(z)$ be the integrand:\n$$ f(z) = \\frac{e^z}{z^2 + 1} $$\nThe function $e^z$ is entire (analytic everywhere in $\\mathbb{C}$). The singularities of $f(z)$ occur where the denominator is zero:\n$$ z^2 + 1 = 0 \\implies z^2 = -1 \\implies z = i \\quad \\text{or} \\quad z = -i $$\nThus, the integrand has two simple poles at $z_1 = i$ and $z_2 = -i$.\n\n**2. Verification of Singularities Within the Contour**\nThe contour is the circle centered at the origin with radius $R = 2$. We check if the poles lie inside this contour:\n*   For $z_1 = i$: $|i| = 1 < 2$.\n*   For $z_2 = -i$: $|-i| = 1 < 2$.\n\nBoth poles lie strictly inside the contour $|z|=2$. Therefore, by the Cauchy Residue Theorem, the value of the integral is:\n$$ I = 2\\pi i \\left( \\text{Res}(f, i) + \\text{Res}(f, -i) \\right) $$\n\n**3. Calculation of Residues**\nSince the poles are simple (roots of multiplicity 1), we can calculate the residue at a pole $z_0$ using the formula:\n$$ \\text{Res}(f, z_0) = \\lim_{z \\to z_0} (z - z_0)f(z) = \\frac{P(z_0)}{Q'(z_0)} $$\nwhere $f(z) = \\frac{P(z)}{Q(z)} = \\frac{e^z}{z^2+1}$.\nHere, $P(z) = e^z$ and $Q'(z) = \\frac{d}{dz}(z^2+1) = 2z$.\n\n*   **Residue at $z = i$:**\n    $$ \\text{Res}(f, i) = \\frac{e^i}{2(i)} = \\frac{e^i}{2i} $$\n\n*   **Residue at $z = -i$:**\n    $$ \\text{Res}(f, -i) = \\frac{e^{-i}}{2(-i)} = -\\frac{e^{-i}}{2i} $$\n\n**4. Summation and Final Evaluation**\nWe sum the residues:\n$$ \\sum \\text{Res} = \\frac{e^i}{2i} - \\frac{e^{-i}}{2i} = \\frac{e^i - e^{-i}}{2i} $$\nUsing the Euler definition of the sine function, $\\sin(z) = \\frac{e^{iz} - e^{-iz}}{2i}$, we observe that for $z=1$:\n$$ \\frac{e^i - e^{-i}}{2i} = \\sin(1) $$\nThus, the sum of the residues is $\\sin(1)$.\n\nSubstituting this back into the Residue Theorem formula:\n$$ I = 2\\pi i (\\sin(1)) $$\n\n**Final Answer**\nThe value of the integral is:\n$$ \\oint_{|z|=2} \\frac{e^z}{z^2 + 1} \\, dz = 2\\pi i \\sin(1) $$",
        "raw_answer": "**Problem Statement**\nEvaluate the contour integral\n$$ I = \\oint_{|z|=2} \\frac{e^z}{z^2 + 1} \\, dz $$\nwhere the contour $C = \\{z \\in \\mathbb{C} : |z| = 2\\}$ is traversed in the positive (counter-clockwise) direction.\n\n**Solution**\n\n**1. Identification of Singularities**\nLet $f(z)$ be the integrand:\n$$ f(z) = \\frac{e^z}{z^2 + 1} $$\nThe function $e^z$ is entire (analytic everywhere in $\\mathbb{C}$). The singularities of $f(z)$ occur where the denominator is zero:\n$$ z^2 + 1 = 0 \\implies z^2 = -1 \\implies z = i \\quad \\text{or} \\quad z = -i $$\nThus, the integrand has two simple poles at $z_1 = i$ and $z_2 = -i$.\n\n**2. Verification of Singularities Within the Contour**\nThe contour is the circle centered at the origin with radius $R = 2$. We check if the poles lie inside this contour:\n*   For $z_1 = i$: $|i| = 1 < 2$.\n*   For $z_2 = -i$: $|-i| = 1 < 2$.\n\nBoth poles lie strictly inside the contour $|z|=2$. Therefore, by the Cauchy Residue Theorem, the value of the integral is:\n$$ I = 2\\pi i \\left( \\text{Res}(f, i) + \\text{Res}(f, -i) \\right) $$\n\n**3. Calculation of Residues**\nSince the poles are simple (roots of multiplicity 1), we can calculate the residue at a pole $z_0$ using the formula:\n$$ \\text{Res}(f, z_0) = \\lim_{z \\to z_0} (z - z_0)f(z) = \\frac{P(z_0)}{Q'(z_0)} $$\nwhere $f(z) = \\frac{P(z)}{Q(z)} = \\frac{e^z}{z^2+1}$.\nHere, $P(z) = e^z$ and $Q'(z) = \\frac{d}{dz}(z^2+1) = 2z$.\n\n*   **Residue at $z = i$:**\n    $$ \\text{Res}(f, i) = \\frac{e^i}{2(i)} = \\frac{e^i}{2i} $$\n\n*   **Residue at $z = -i$:**\n    $$ \\text{Res}(f, -i) = \\frac{e^{-i}}{2(-i)} = -\\frac{e^{-i}}{2i} $$\n\n**4. Summation and Final Evaluation**\nWe sum the residues:\n$$ \\sum \\text{Res} = \\frac{e^i}{2i} - \\frac{e^{-i}}{2i} = \\frac{e^i - e^{-i}}{2i} $$\nUsing the Euler definition of the sine function, $\\sin(z) = \\frac{e^{iz} - e^{-iz}}{2i}$, we observe that for $z=1$:\n$$ \\frac{e^i - e^{-i}}{2i} = \\sin(1) $$\nThus, the sum of the residues is $\\sin(1)$.\n\nSubstituting this back into the Residue Theorem formula:\n$$ I = 2\\pi i (\\sin(1)) $$\n\n**Final Answer**\nThe value of the integral is:\n$$ \\oint_{|z|=2} \\frac{e^z}{z^2 + 1} \\, dz = 2\\pi i \\sin(1) $$",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ],
    "outer_attempt": 1
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "## Step 1: Understand the task and the domain\nThe task is to generate a challenging but solvable mathematics problem in the domain of Potential theory, along with a complete and verifiable solution. Potential theory is a branch of mathematics that deals with the study of potentials and their properties, particularly in relation to partial differential equations, especially Laplace's equation.\n\n## Step 2: Identify key concepts in Potential theory\nPotential theory involves the study of harmonic functions, potentials (such as Newtonian or logarithmic potentials), and their applications. Key concepts include the Laplace equation ($\\Delta u = 0$), Poisson's equation ($\\Delta u = f$), and boundary value problems (Dirichlet and Neumann problems).\n\n## Step 3: Formulate a problem\nTo create a challenging problem, we need to involve some of the advanced concepts or techniques from Potential theory. Let's consider a problem involving the Dirichlet problem for the Laplace equation on a specific domain.\n\n## Step 4: Specify the problem\nLet's consider the following problem: \"Let $D = \\{x \\in \\mathbb{R}^2: |x| < 1\\}$ be the unit disk in $\\mathbb{R}^2$. Find the solution $u(x)$ to the Dirichlet problem $\\Delta u = 0$ in $D$, $u(x) = \\log|x - (1,0)|$ on $\\partial D$.\"\n\n## Step 5: Provide the solution\nTo solve this problem, we can use the Poisson integral formula, which gives the solution to the Dirichlet problem on the unit disk. The Poisson integral formula is given by $u(x) = \\frac{1}{2\\pi} \\int_{\\partial D} \\frac{1 - |x|^2}{|x - y|^2} f(y) ds(y)$, where $f(y)$ is the boundary data.\n\n## Step 6: Apply the Poisson integral formula\nFor $x = (r\\cos\\theta, r\\sin\\theta)$ and $y = (\\cos\\phi, \\sin\\phi)$, we have $|x - y|^2 = r^2 - 2r\\cos(\\theta - \\phi) + 1$ and $ds(y) = d\\phi$. The boundary data is $f(y) = \\log|y - (1,0)| = \\log\\sqrt{2 - 2\\cos\\phi}$. So, $u(x) = \\frac{1}{2\\pi} \\int_{0}^{2\\pi} \\frac{1 - r^2}{r^2 - 2r\\cos(\\theta - \\phi) + 1} \\log\\sqrt{2 - 2\\cos\\phi} d\\phi$.\n\n## Step 7: Simplify the integral\nThe integral can be simplified by recognizing that $\\log\\sqrt{2 - 2\\cos\\phi} = \\log|1 - e^{i\\phi}| = \\log|1 - e^{i\\phi}| = \\Re(\\log(1 - e^{i\\phi}))$. However, directly evaluating this integral is complex and typically involves recognizing it as a special case that can be related to the properties of harmonic functions and the Poisson kernel.\n\n## Step 8: Evaluate the solution using properties of harmonic functions\nFor $x \\in D$, the solution can be found by directly applying properties of harmonic functions and recognizing that the given boundary condition corresponds to a specific harmonic function. The function $u(x) = \\log|x - (1,0)|$ is harmonic in $D$ except at $x = (1,0)$, which is outside $D$ since $|1,0| = 1$ is on $\\partial D$. Thus, we directly check if $u(x)$ satisfies the given boundary condition and is harmonic.\n\n## Step 9: Check if the given boundary data directly provides the solution\nSince $\\log|x - (1,0)|$ is harmonic in $D$ and equals $\\log|y - (1,0)|$ on $\\partial D$, it is the solution to the Dirichlet problem.\n\nThe final answer is: \n\nLet $D = \\{x \\in \\mathbb{R}^2: |x| < 1\\}$ be the unit disk in $\\mathbb{R}^2$. Find the solution $u(x)$ to the Dirichlet problem $\\Delta u = 0$ in $D$, $u(x) = \\log|x - (1,0)|$ on $\\partial D$.",
    "run_id": "19",
    "topic_slug": "potential_theory",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "To find the solution to the Dirichlet problem for the Laplace equation on the unit disk with the specified boundary condition, we will proceed through a rigorous mathematical derivation.\n\n### 1. Problem Statement\n\nLet $D = \\{x \\in \\mathbb{R}^2 : |x| < 1\\}$ be the open unit disk. We seek a function $u: D \\to \\mathbb{R}$ satisfying:\n1.  **Laplace's Equation:** $\\Delta u(x) = 0$ for all $x \\in D$.\n2.  **Boundary Condition:** $\\lim_{x \\to y, x \\in D} u(x) = \\log|y - (1,0)|$ for $y \\in \\partial D \\setminus \\{(1,0)\\}$.\n\nThe boundary data is given by $f(y) = \\log|y - (1,0)|$. We note that the point $(1,0)$ lies on the boundary $\\partial D$. The function $f(y)$ is singular at $(1,0)$ (approaching $-\\infty$), so we interpret the boundary condition in the sense of the Poisson integral for $L^1$ boundary data.\n\n### 2. Analysis of the Candidate Solution\n\nA powerful technique in potential theory is to inspect whether the function defining the boundary data is itself harmonic in the interior domain.\n\nConsider the function defined on $\\mathbb{R}^2 \\setminus \\{(1,0)\\}$:\n$$ v(x) = \\log|x - (1,0)| $$\n\n**Step 2.1: Verify Harmonicity**\nWe identify $\\mathbb{R}^2$ with the complex plane $\\mathbb{C}$. Let $x \\cong z = x_1 + i x_2$. Then $(1,0) \\cong 1$.\nThe function becomes $v(z) = \\log|z - 1|$.\nRecall that for any holomorphic function $g(z)$, its real part $\\Re(g(z))$ is harmonic.\nConsider the principal branch of the complex logarithm, $\\log(z - 1)$. This function is holomorphic on $\\mathbb{C} \\setminus (-\\infty, 1]$.\nHowever, we only need the function to be harmonic in the domain $D$ (the unit disk).\nThe singularity of $\\log(z - 1)$ is at $z = 1$. The branch cut can be taken along the ray $[1, \\infty)$ on the real axis, or any ray starting at 1 that does not intersect $D$.\nRegardless of the branch cut, the real part is well-defined:\n$$ \\Re(\\log(z - 1)) = \\log|z - 1| $$\nSince $\\log(z - 1)$ is holomorphic locally around any point in $D$ (as $1 \\notin D$), its real part $v(z) = \\log|z - 1|$ is harmonic in $D$.\n$$ \\Delta v = 0 \\quad \\text{for } x \\in D $$\n\n**Step 2.2: Verify Boundary Conditions**\nFor any $y \\in \\partial D \\setminus \\{(1,0)\\}$, the proposed solution $v(y)$ is exactly $\\log|y - (1,0)|$.\nThus, $v(x)$ satisfies the boundary condition pointwise everywhere except at the singularity $(1,0)$.\n\n### 3. Rigorous Derivation via Fourier Series\n\nTo ensure $v(x)$ is the unique solution in the appropriate class (Poisson integral of $L^1$ data), we derive it constructively using Fourier series.\n\nParametrize the boundary $\\partial D$ by $y(\\theta) = e^{i\\theta}$ for $\\theta \\in (0, 2\\pi)$.\nThe boundary data is:\n$$ f(\\theta) = \\log|e^{i\\theta} - 1| $$\nUsing the identity $|e^{i\\theta} - 1| = 2|\\sin(\\frac{\\theta}{2})|$, we have $f(\\theta) = \\log(2|\\sin(\\frac{\\theta}{2})|)$.\n\nWe utilize the known Fourier series expansion for this function. Recall the Taylor series for the principal logarithm for $|z| < 1$:\n$$ \\log(1 - z) = -\\sum_{n=1}^{\\infty} \\frac{z^n}{n} $$\nLet $z = r e^{i\\theta}$ with $r < 1$. Taking the real part:\n$$ \\Re(\\log(1 - r e^{i\\theta})) = -\\sum_{n=1}^{\\infty} \\frac{r^n \\cos(n\\theta)}{n} $$\nAccording to Abel's theorem, as $r \\to 1^-$, this series converges to the boundary function almost everywhere (specifically for $\\theta \\neq 0$):\n$$ \\Re(\\log(1 - e^{i\\theta})) = \\log|1 - e^{i\\theta}| = \\log|e^{i\\theta} - 1| $$\nThus, the Fourier series of our boundary data is:\n$$ f(\\theta) \\sim -\\sum_{n=1}^{\\infty} \\frac{\\cos(n\\theta)}{n} $$\nThe solution to the Dirichlet problem is the harmonic extension of this series into the interior $D$ (where $x \\cong r e^{i\\theta}$):\n$$ u(r, \\theta) = -\\sum_{n=1}^{\\infty} r^n \\frac{\\cos(n\\theta)}{n} $$\nRecognizing this sum as the real part of the Taylor series for $\\log(1-z)$ evaluated at $z = r e^{i\\theta}$:\n$$ u(r, \\theta) = \\Re\\left( \\log(1 - r e^{i\\theta}) \\right) = \\log|1 - r e^{i\\theta}| $$\nIn Cartesian coordinates, $|1 - r e^{i\\theta}| = |1 - x| = |x - (1,0)|$.\n\n### 4. Conclusion\n\nThe function $u(x) = \\log|x - (1,0)|$ is harmonic in $D$ and satisfies the prescribed boundary conditions. It is the unique solution given by the Poisson integral of the boundary data.\n\n**Final Answer:**\nThe solution to the Dirichlet problem is:\n$$ u(x) = \\log|x - (1,0)| $$",
        "raw_answer": "To find the solution to the Dirichlet problem for the Laplace equation on the unit disk with the specified boundary condition, we will proceed through a rigorous mathematical derivation.\n\n### 1. Problem Statement\n\nLet $D = \\{x \\in \\mathbb{R}^2 : |x| < 1\\}$ be the open unit disk. We seek a function $u: D \\to \\mathbb{R}$ satisfying:\n1.  **Laplace's Equation:** $\\Delta u(x) = 0$ for all $x \\in D$.\n2.  **Boundary Condition:** $\\lim_{x \\to y, x \\in D} u(x) = \\log|y - (1,0)|$ for $y \\in \\partial D \\setminus \\{(1,0)\\}$.\n\nThe boundary data is given by $f(y) = \\log|y - (1,0)|$. We note that the point $(1,0)$ lies on the boundary $\\partial D$. The function $f(y)$ is singular at $(1,0)$ (approaching $-\\infty$), so we interpret the boundary condition in the sense of the Poisson integral for $L^1$ boundary data.\n\n### 2. Analysis of the Candidate Solution\n\nA powerful technique in potential theory is to inspect whether the function defining the boundary data is itself harmonic in the interior domain.\n\nConsider the function defined on $\\mathbb{R}^2 \\setminus \\{(1,0)\\}$:\n$$ v(x) = \\log|x - (1,0)| $$\n\n**Step 2.1: Verify Harmonicity**\nWe identify $\\mathbb{R}^2$ with the complex plane $\\mathbb{C}$. Let $x \\cong z = x_1 + i x_2$. Then $(1,0) \\cong 1$.\nThe function becomes $v(z) = \\log|z - 1|$.\nRecall that for any holomorphic function $g(z)$, its real part $\\Re(g(z))$ is harmonic.\nConsider the principal branch of the complex logarithm, $\\log(z - 1)$. This function is holomorphic on $\\mathbb{C} \\setminus (-\\infty, 1]$.\nHowever, we only need the function to be harmonic in the domain $D$ (the unit disk).\nThe singularity of $\\log(z - 1)$ is at $z = 1$. The branch cut can be taken along the ray $[1, \\infty)$ on the real axis, or any ray starting at 1 that does not intersect $D$.\nRegardless of the branch cut, the real part is well-defined:\n$$ \\Re(\\log(z - 1)) = \\log|z - 1| $$\nSince $\\log(z - 1)$ is holomorphic locally around any point in $D$ (as $1 \\notin D$), its real part $v(z) = \\log|z - 1|$ is harmonic in $D$.\n$$ \\Delta v = 0 \\quad \\text{for } x \\in D $$\n\n**Step 2.2: Verify Boundary Conditions**\nFor any $y \\in \\partial D \\setminus \\{(1,0)\\}$, the proposed solution $v(y)$ is exactly $\\log|y - (1,0)|$.\nThus, $v(x)$ satisfies the boundary condition pointwise everywhere except at the singularity $(1,0)$.\n\n### 3. Rigorous Derivation via Fourier Series\n\nTo ensure $v(x)$ is the unique solution in the appropriate class (Poisson integral of $L^1$ data), we derive it constructively using Fourier series.\n\nParametrize the boundary $\\partial D$ by $y(\\theta) = e^{i\\theta}$ for $\\theta \\in (0, 2\\pi)$.\nThe boundary data is:\n$$ f(\\theta) = \\log|e^{i\\theta} - 1| $$\nUsing the identity $|e^{i\\theta} - 1| = 2|\\sin(\\frac{\\theta}{2})|$, we have $f(\\theta) = \\log(2|\\sin(\\frac{\\theta}{2})|)$.\n\nWe utilize the known Fourier series expansion for this function. Recall the Taylor series for the principal logarithm for $|z| < 1$:\n$$ \\log(1 - z) = -\\sum_{n=1}^{\\infty} \\frac{z^n}{n} $$\nLet $z = r e^{i\\theta}$ with $r < 1$. Taking the real part:\n$$ \\Re(\\log(1 - r e^{i\\theta})) = -\\sum_{n=1}^{\\infty} \\frac{r^n \\cos(n\\theta)}{n} $$\nAccording to Abel's theorem, as $r \\to 1^-$, this series converges to the boundary function almost everywhere (specifically for $\\theta \\neq 0$):\n$$ \\Re(\\log(1 - e^{i\\theta})) = \\log|1 - e^{i\\theta}| = \\log|e^{i\\theta} - 1| $$\nThus, the Fourier series of our boundary data is:\n$$ f(\\theta) \\sim -\\sum_{n=1}^{\\infty} \\frac{\\cos(n\\theta)}{n} $$\nThe solution to the Dirichlet problem is the harmonic extension of this series into the interior $D$ (where $x \\cong r e^{i\\theta}$):\n$$ u(r, \\theta) = -\\sum_{n=1}^{\\infty} r^n \\frac{\\cos(n\\theta)}{n} $$\nRecognizing this sum as the real part of the Taylor series for $\\log(1-z)$ evaluated at $z = r e^{i\\theta}$:\n$$ u(r, \\theta) = \\Re\\left( \\log(1 - r e^{i\\theta}) \\right) = \\log|1 - r e^{i\\theta}| $$\nIn Cartesian coordinates, $|1 - r e^{i\\theta}| = |1 - x| = |x - (1,0)|$.\n\n### 4. Conclusion\n\nThe function $u(x) = \\log|x - (1,0)|$ is harmonic in $D$ and satisfies the prescribed boundary conditions. It is the unique solution given by the Poisson integral of the boundary data.\n\n**Final Answer:**\nThe solution to the Dirichlet problem is:\n$$ u(x) = \\log|x - (1,0)| $$",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ],
    "outer_attempt": 1
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "## Step 1: Understand the task and the domain\nThe task is to generate a challenging but solvable mathematics problem in the domain of \"Several complex variables and analytic spaces\" along with a complete and verifiable solution. The problem should be well-posed, clear, non-trivial, and solvable with established mathematical methods.\n\n## Step 2: Identify key concepts in the domain\nSeveral complex variables and analytic spaces involve the study of functions of multiple complex variables and the spaces on which these functions are defined. Key concepts include holomorphic functions, complex manifolds, analytic continuation, and various types of domains (e.g., domains of holomorphy, pseudoconvex domains).\n\n## Step 3: Formulate a problem\nTo create a problem, we need to focus on a specific aspect of several complex variables. Let's consider the concept of holomorphic functions on a domain in $\\mathbb{C}^n$. A potential problem could involve characterizing or analyzing the properties of such functions.\n\n## Step 4: Specify the problem\nLet's formulate a problem related to the extension of holomorphic functions. A well-known result in several complex variables is the Hartogs' extension theorem, which states that any holomorphic function defined on a domain in $\\mathbb{C}^n$ ($n > 1$) with a compact hole can be extended holomorphically to the whole domain. We can create a problem related to this concept.\n\n## Step 5: Create the problem statement\nConsider the domain $\\Omega = \\{z \\in \\mathbb{C}^2 : |z_1| < 1, |z_2| < 1\\}$ and the subset $S = \\{z \\in \\Omega : |z_1| < \\frac{1}{2}\\}$. Let $f$ be a holomorphic function on $\\Omega \\setminus S$. Show that $f$ can be extended to a holomorphic function on $\\Omega$.\n\n## Step 6: Provide the solution\nTo solve this, we recognize that $\\Omega \\setminus S$ is not a compact hole in $\\Omega$, so we cannot directly apply Hartogs' theorem. However, we can analyze the problem by considering the properties of holomorphic functions in $\\mathbb{C}^2$ and the specific geometry of $\\Omega$ and $S$.\n\n## Step 7: Detail the solution\nFor $z = (z_1, z_2) \\in \\Omega \\setminus S$, $f(z)$ is holomorphic. We need to show that $f$ can be extended holomorphically to $\\Omega$. For any fixed $z_2$ with $|z_2| < 1$, $f(z_1, z_2)$ is holomorphic in $z_1$ for $\\frac{1}{2} \\leq |z_1| < 1$. By the principle of analytic continuation in one variable, if $f(z_1, z_2)$ is holomorphic for $|z_1| < 1$ except on a smaller disk, it can be extended. However, the direct application of Hartogs' theorem is not possible here due to the shape of $S$. Instead, we can use the fact that for each $z_2$, $f(\\cdot, z_2)$ can be extended to $|z_1| < 1$ because the singularity set is contained within $|z_1| \\leq \\frac{1}{2}$, and then apply Hartogs' theorem or a similar principle to conclude the extension is holomorphic in both variables.\n\n## Step 8: Write the problem and solution in the required format\n\nLet $\\Omega = \\{z \\in \\mathbb{C}^2 : |z_1| < 1, |z_2| < 1\\}$ and $S = \\{z \\in \\Omega : |z_1| < \\frac{1}{2}\\}$. Suppose $f$ is a holomorphic function on $\\Omega \\setminus S$. Prove that $f$ can be extended to a holomorphic function on $\\Omega$.",
    "run_id": "20",
    "topic_slug": "several_complex_variables_and_analytic_spaces",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "The question presented in Step 8 is **ill-posed** because it asks for a proof of a mathematical assertion that is **false**. The problem violates the **Contradictory Premises** validity condition, as it assumes that a specific class of functions always admits a holomorphic extension when, in fact, it does not.\n\nBelow is the rigorous demonstration that the assertion is false, provided via a counterexample, followed by an analysis of the geometric obstruction.\n\n### 1. Analysis of the Domain and Statement\n\nLet us explicitly define the sets given in the problem statement:\n- The domain $\\Omega = \\{ z \\in \\mathbb{C}^2 : |z_1| < 1, |z_2| < 1 \\}$ is the unit polydisc $\\Delta^2$.\n- The subset $S = \\{ z \\in \\Omega : |z_1| < \\frac{1}{2} \\}$.\n\nThe domain of definition for the function $f$ is $D = \\Omega \\setminus S$. We can determine the explicit structure of $D$:\n$$\nD = \\{ (z_1, z_2) \\in \\mathbb{C}^2 : |z_1| < 1, |z_2| < 1 \\} \\setminus \\{ (z_1, z_2) \\in \\mathbb{C}^2 : |z_1| < \\frac{1}{2}, |z_2| < 1 \\}\n$$\n$$\nD = \\{ (z_1, z_2) \\in \\mathbb{C}^2 : \\frac{1}{2} \\leq |z_1| < 1, \\, |z_2| < 1 \\}\n$$\nGeometrically, $D$ is the Cartesian product of an annulus $A = \\{ z_1 \\in \\mathbb{C} : \\frac{1}{2} \\leq |z_1| < 1 \\}$ and the unit disc $\\Delta = \\{ z_2 \\in \\mathbb{C} : |z_2| < 1 \\}$.\n\nThe question asks to prove that **any** holomorphic function $f$ on $D$ can be extended to a holomorphic function on $\\Omega$.\n\n### 2. Counterexample\n\nWe construct a function $f$ that is holomorphic on $D$ but cannot be extended holomorphically to $\\Omega$.\n\nConsider the function:\n$$\nf(z_1, z_2) = \\frac{1}{z_1}\n$$\n\n**Verification of Holomorphy on $D$:**\nFor any point $(z_1, z_2) \\in D$, we have the constraint $\\frac{1}{2} \\leq |z_1| < 1$.\nSince $|z_1| \\geq \\frac{1}{2}$, it follows that $z_1 \\neq 0$. The function $z \\mapsto \\frac{1}{z_1}$ is a rational function singular only at $z_1 = 0$. Since the singular locus $\\{z_1 = 0\\}$ is disjoint from $D$, $f$ is holomorphic on $D$.\n\n**Impossibility of Extension to $\\Omega$:**\nThe domain $\\Omega$ contains the set $\\{ (0, z_2) : |z_2| < 1 \\}$, which corresponds to $z_1 = 0$.\nSuppose there exists a holomorphic function $F: \\Omega \\to \\mathbb{C}$ such that $F|_D = f$.\nConsider the restriction of $F$ to the slice $z_2 = 0$. Let $g(z_1) = F(z_1, 0)$.\nSince $F$ is holomorphic on $\\Omega$, $g$ must be holomorphic on the unit disc $\\{ z_1 \\in \\mathbb{C} : |z_1| < 1 \\}$.\nHowever, for $\\frac{1}{2} \\leq |z_1| < 1$, we have $g(z_1) = f(z_1, 0) = \\frac{1}{z_1}$.\nBy the Identity Theorem for holomorphic functions, $g(z_1)$ must equal $\\frac{1}{z_1}$ everywhere on its domain of definition.\nBut $\\lim_{z_1 \\to 0} |g(z_1)| = \\lim_{z_1 \\to 0} \\frac{1}{|z_1|} = \\infty$.\nThis contradicts the requirement that $g$ is holomorphic (and thus continuous and finite) at $z_1 = 0$.\n\nTherefore, $f$ cannot be extended to a holomorphic function on $\\Omega$.\n\n### 3. Mathematical Context and Correction\n\nThe assertion fails because the domain $D = A \\times \\Delta$ is a **domain of holomorphy**. A fundamental result in Several Complex Variables states that a product domain $D_1 \\times D_2$ is a domain of holomorphy if and only if $D_1$ and $D_2$ are domains of holomorphy in their respective spaces.\n- The annulus $A = \\{ 1/2 \\leq |z_1| < 1 \\}$ is a domain of holomorphy in $\\mathbb{C}$.\n- The disc $\\Delta = \\{ |z_2| < 1 \\}$ is a domain of holomorphy in $\\mathbb{C}$.\nThus, $D$ is a domain of holomorphy, meaning there exists a holomorphic function on $D$ that cannot be extended to any larger domain.\n\n**To make the question answerable (Well-Posedness):**\nThe problem likely intended to describe a **Hartogs figure**, where extension is forced. A modification that would make the assertion true is:\nDefine $S = \\{ z \\in \\Omega : |z_1| < \\frac{1}{2} \\text{ and } |z_2| < \\frac{1}{2} \\}$.\nIn this case, $\\Omega \\setminus S$ is connected and \"surrounds\" the compact hole (or thick hole) in such a way that Hartogs' Extension Theorem applies, allowing every holomorphic function on $\\Omega \\setminus S$ to extend to $\\Omega$.\n\nHowever, as stated with $S$ defined solely by $|z_1| < 1/2$, the statement is false.",
        "raw_answer": "The question presented in Step 8 is **ill-posed** because it asks for a proof of a mathematical assertion that is **false**. The problem violates the **Contradictory Premises** validity condition, as it assumes that a specific class of functions always admits a holomorphic extension when, in fact, it does not.\n\nBelow is the rigorous demonstration that the assertion is false, provided via a counterexample, followed by an analysis of the geometric obstruction.\n\n### 1. Analysis of the Domain and Statement\n\nLet us explicitly define the sets given in the problem statement:\n- The domain $\\Omega = \\{ z \\in \\mathbb{C}^2 : |z_1| < 1, |z_2| < 1 \\}$ is the unit polydisc $\\Delta^2$.\n- The subset $S = \\{ z \\in \\Omega : |z_1| < \\frac{1}{2} \\}$.\n\nThe domain of definition for the function $f$ is $D = \\Omega \\setminus S$. We can determine the explicit structure of $D$:\n$$\nD = \\{ (z_1, z_2) \\in \\mathbb{C}^2 : |z_1| < 1, |z_2| < 1 \\} \\setminus \\{ (z_1, z_2) \\in \\mathbb{C}^2 : |z_1| < \\frac{1}{2}, |z_2| < 1 \\}\n$$\n$$\nD = \\{ (z_1, z_2) \\in \\mathbb{C}^2 : \\frac{1}{2} \\leq |z_1| < 1, \\, |z_2| < 1 \\}\n$$\nGeometrically, $D$ is the Cartesian product of an annulus $A = \\{ z_1 \\in \\mathbb{C} : \\frac{1}{2} \\leq |z_1| < 1 \\}$ and the unit disc $\\Delta = \\{ z_2 \\in \\mathbb{C} : |z_2| < 1 \\}$.\n\nThe question asks to prove that **any** holomorphic function $f$ on $D$ can be extended to a holomorphic function on $\\Omega$.\n\n### 2. Counterexample\n\nWe construct a function $f$ that is holomorphic on $D$ but cannot be extended holomorphically to $\\Omega$.\n\nConsider the function:\n$$\nf(z_1, z_2) = \\frac{1}{z_1}\n$$\n\n**Verification of Holomorphy on $D$:**\nFor any point $(z_1, z_2) \\in D$, we have the constraint $\\frac{1}{2} \\leq |z_1| < 1$.\nSince $|z_1| \\geq \\frac{1}{2}$, it follows that $z_1 \\neq 0$. The function $z \\mapsto \\frac{1}{z_1}$ is a rational function singular only at $z_1 = 0$. Since the singular locus $\\{z_1 = 0\\}$ is disjoint from $D$, $f$ is holomorphic on $D$.\n\n**Impossibility of Extension to $\\Omega$:**\nThe domain $\\Omega$ contains the set $\\{ (0, z_2) : |z_2| < 1 \\}$, which corresponds to $z_1 = 0$.\nSuppose there exists a holomorphic function $F: \\Omega \\to \\mathbb{C}$ such that $F|_D = f$.\nConsider the restriction of $F$ to the slice $z_2 = 0$. Let $g(z_1) = F(z_1, 0)$.\nSince $F$ is holomorphic on $\\Omega$, $g$ must be holomorphic on the unit disc $\\{ z_1 \\in \\mathbb{C} : |z_1| < 1 \\}$.\nHowever, for $\\frac{1}{2} \\leq |z_1| < 1$, we have $g(z_1) = f(z_1, 0) = \\frac{1}{z_1}$.\nBy the Identity Theorem for holomorphic functions, $g(z_1)$ must equal $\\frac{1}{z_1}$ everywhere on its domain of definition.\nBut $\\lim_{z_1 \\to 0} |g(z_1)| = \\lim_{z_1 \\to 0} \\frac{1}{|z_1|} = \\infty$.\nThis contradicts the requirement that $g$ is holomorphic (and thus continuous and finite) at $z_1 = 0$.\n\nTherefore, $f$ cannot be extended to a holomorphic function on $\\Omega$.\n\n### 3. Mathematical Context and Correction\n\nThe assertion fails because the domain $D = A \\times \\Delta$ is a **domain of holomorphy**. A fundamental result in Several Complex Variables states that a product domain $D_1 \\times D_2$ is a domain of holomorphy if and only if $D_1$ and $D_2$ are domains of holomorphy in their respective spaces.\n- The annulus $A = \\{ 1/2 \\leq |z_1| < 1 \\}$ is a domain of holomorphy in $\\mathbb{C}$.\n- The disc $\\Delta = \\{ |z_2| < 1 \\}$ is a domain of holomorphy in $\\mathbb{C}$.\nThus, $D$ is a domain of holomorphy, meaning there exists a holomorphic function on $D$ that cannot be extended to any larger domain.\n\n**To make the question answerable (Well-Posedness):**\nThe problem likely intended to describe a **Hartogs figure**, where extension is forced. A modification that would make the assertion true is:\nDefine $S = \\{ z \\in \\Omega : |z_1| < \\frac{1}{2} \\text{ and } |z_2| < \\frac{1}{2} \\}$.\nIn this case, $\\Omega \\setminus S$ is connected and \"surrounds\" the compact hole (or thick hole) in such a way that Hartogs' Extension Theorem applies, allowing every holomorphic function on $\\Omega \\setminus S$ to extend to $\\Omega$.\n\nHowever, as stated with $S$ defined solely by $|z_1| < 1/2$, the statement is false.",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": true,
          "issues": [],
          "improvements": ""
        }
      }
    ],
    "outer_attempt": 1
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "## Step 1: Understand the task\nThe task is to generate a challenging mathematical problem in the domain of special functions, along with a clear and detailed solution, following specific guidelines.\n[/QUESTION]",
    "run_id": "21",
    "topic_slug": "special_functions",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "Based on the task requirements, I have generated a challenging problem involving special functions (specifically the Gamma function and properties of improper integrals) and provided a complete, rigorous solution.\n\n---\n\n### **Problem Statement**\n\n**Problem:** For a real parameter $n > 1$, evaluate the definite integral:\n\n$$ I(n) = \\int_0^{\\infty} \\frac{\\ln x}{1+x^n} \\, dx $$\n\nProvide a closed-form expression for $I(n)$ in terms of standard mathematical constants and trigonometric functions. Justify all steps, including the convergence of the integral and the validity of any operations involving differentiation under the integral sign.\n\n---\n\n### **Solution**\n\nWe aim to evaluate the integral $I(n) = \\int_0^{\\infty} \\frac{\\ln x}{1+x^n} \\, dx$ for $n > 1$.\n\n#### **1. Convergence Analysis**\n\nFirst, we establish that the integral is well-defined.\nLet $f(x) = \\frac{\\ln x}{1+x^n}$.\n*   **Near $x \\to 0^+$:** The denominator $1+x^n \\approx 1$. Thus, $f(x) \\sim \\ln x$. Since $\\int_0^1 \\ln x \\, dx$ is absolutely convergent (calculating to $-1$), the integral converges at the lower bound.\n*   **Near $x \\to \\infty$:** For large $x$, $1+x^n \\sim x^n$. Thus, $f(x) \\sim \\frac{\\ln x}{x^n}$. Since $n > 1$, let $\\epsilon = \\frac{n-1}{2} > 0$. We have $\\frac{\\ln x}{x^n} = \\frac{\\ln x}{x^\\epsilon} \\cdot \\frac{1}{x^{n-\\epsilon}}$. Since $\\frac{\\ln x}{x^\\epsilon} \\to 0$, the tail is bounded by $C \\cdot \\frac{1}{x^{1+\\epsilon}}$, which converges by the $p$-test ($p = 1 + \\epsilon > 1$).\n\nThus, $I(n)$ converges for all $n > 1$.\n\n#### **2. Introduction of a Parameterized Integral**\n\nWe introduce a parameterized auxiliary integral (the Mellin transform of the function $\\frac{1}{1+x^n}$):\n$$ J(s) = \\int_0^{\\infty} \\frac{x^{s-1}}{1+x^n} \\, dx $$\nNotice that\n$$ \\frac{\\partial}{\\partial s} \\left( \\frac{x^{s-1}}{1+x^n} \\right) = \\frac{x^{s-1} \\ln x}{1+x^n} $$\nEvaluating this derivative at $s=1$ yields our target integrand:\n$$ J'(1) = \\int_0^{\\infty} \\frac{\\ln x}{1+x^n} \\, dx = I(n) $$\n\nTo rigorously justify differentiation under the integral sign, we verify the uniform convergence of the integral of the derivative. Let $s \\in [1-\\delta, 1+\\delta]$. We require the domain of integration for $s$ to satisfy $0 < s < n$ for $J(s)$ to converge. Since $n > 1$, we can choose $\\delta > 0$ such that $[1-\\delta, 1+\\delta] \\subset (0, n)$.\n*   On $(0, 1]$, $|x^{s-1}| \\le x^{-\\delta}$.\n*   On $[1, \\infty)$, $|x^{s-1}| \\le x^{\\delta}$.\nThe dominating function $g(x) = \\frac{|\\ln x| (x^{-\\delta} + x^{\\delta})}{1+x^n}$ is integrable on $(0, \\infty)$ given the choice of $\\delta$. By the Leibniz Integral Rule, $I(n) = J'(1)$.\n\n#### **3. Evaluation of $J(s)$ via Beta and Gamma Functions**\n\nWe evaluate $J(s)$ using a substitution to map it to the Beta function definition, $B(a, b) = \\int_0^\\infty \\frac{t^{a-1}}{(1+t)^{a+b}} \\, dt$.\n\nLet $u = x^n$. Then $x = u^{1/n}$ and $dx = \\frac{1}{n} u^{1/n - 1} \\, du$.\nSubstituting into $J(s)$:\n$$ J(s) = \\int_0^{\\infty} \\frac{(u^{1/n})^{s-1}}{1+u} \\cdot \\frac{1}{n} u^{1/n - 1} \\, du $$\n$$ J(s) = \\frac{1}{n} \\int_0^{\\infty} \\frac{u^{s/n - 1/n + 1/n - 1}}{1+u} \\, du $$\n$$ J(s) = \\frac{1}{n} \\int_0^{\\infty} \\frac{u^{s/n - 1}}{1+u} \\, du $$\n\nComparing this to the Beta function form $B(a, b)$ with $a = s/n$:\nSince the denominator is $(1+u)^1$, we must have $a+b=1$, implying $b = 1 - s/n$.\nThus:\n$$ J(s) = \\frac{1}{n} B\\left(\\frac{s}{n}, 1 - \\frac{s}{n}\\right) $$\n\nWe relate the Beta function to the Gamma function using the identity $B(x, y) = \\frac{\\Gamma(x)\\Gamma(y)}{\\Gamma(x+y)}$:\n$$ J(s) = \\frac{1}{n} \\frac{\\Gamma(s/n)\\Gamma(1 - s/n)}{\\Gamma(1)} $$\nSince $\\Gamma(1) = 1$, we invoke Euler's Reflection Formula, $\\Gamma(z)\\Gamma(1-z) = \\frac{\\pi}{\\sin(\\pi z)}$, with $z = s/n$:\n$$ J(s) = \\frac{1}{n} \\cdot \\frac{\\pi}{\\sin(\\pi s/n)} $$\nThis closed form is valid for $0 < s < n$.\n\n#### **4. Computing the Derivative**\n\nWe now compute $I(n) = J'(1)$.\n$$ J(s) = \\frac{\\pi}{n} \\csc\\left(\\frac{\\pi s}{n}\\right) $$\nDifferentiating with respect to $s$:\n$$ J'(s) = \\frac{\\pi}{n} \\cdot \\frac{d}{ds} \\left( \\csc\\left(\\frac{\\pi s}{n}\\right) \\right) $$\nUsing the chain rule $\\frac{d}{dx} \\csc(u) = -\\csc(u)\\cot(u) \\frac{du}{dx}$:\n$$ J'(s) = \\frac{\\pi}{n} \\left( -\\csc\\left(\\frac{\\pi s}{n}\\right) \\cot\\left(\\frac{\\pi s}{n}\\right) \\cdot \\frac{\\pi}{n} \\right) $$\n$$ J'(s) = - \\frac{\\pi^2}{n^2} \\frac{\\cos(\\pi s/n)}{\\sin^2(\\pi s/n)} $$\n\nNow, evaluate at $s=1$:\n$$ I(n) = J'(1) = - \\frac{\\pi^2}{n^2} \\frac{\\cos(\\pi/n)}{\\sin^2(\\pi/n)} $$\n\n#### **Conclusion**\n\nThe value of the integral for $n > 1$ is:\n\n$$ \\int_0^{\\infty} \\frac{\\ln x}{1+x^n} \\, dx = - \\frac{\\pi^2}{n^2} \\frac{\\cos(\\frac{\\pi}{n})}{\\sin^2(\\frac{\\pi}{n})} $$\n\n**Verification of Special Cases:**\n1.  **$n=2$**: $\\cos(\\pi/2) = 0$. The formula gives $0$.\n    *   Check: $\\int_0^\\infty \\frac{\\ln x}{1+x^2} dx$. Let $x=1/t$, $dx = -1/t^2 dt$.\n    *   $I = \\int_\\infty^0 \\frac{\\ln(1/t)}{1+1/t^2} (-1/t^2) dt = \\int_0^\\infty \\frac{-\\ln t}{t^2+1} dt = -I \\implies 2I=0 \\implies I=0$. Consistent.\n2.  **Sign check**: For large $n$, $\\frac{\\pi}{n}$ is small and positive. $\\cos(\\pi/n) \\approx 1$, $\\sin^2(\\pi/n) > 0$. The result is negative.\n    *   In the integral, for $x \\in (0,1)$, $\\ln x < 0$. For $x > 1$, $\\ln x > 0$.\n    *   For large $n$, $\\frac{1}{1+x^n} \\approx 1$ on $(0,1)$ and $\\approx 0$ on $(1, \\infty)$. The negative contribution from $(0,1)$ dominates. The negative sign is consistent.",
        "raw_answer": "Based on the task requirements, I have generated a challenging problem involving special functions (specifically the Gamma function and properties of improper integrals) and provided a complete, rigorous solution.\n\n---\n\n### **Problem Statement**\n\n**Problem:** For a real parameter $n > 1$, evaluate the definite integral:\n\n$$ I(n) = \\int_0^{\\infty} \\frac{\\ln x}{1+x^n} \\, dx $$\n\nProvide a closed-form expression for $I(n)$ in terms of standard mathematical constants and trigonometric functions. Justify all steps, including the convergence of the integral and the validity of any operations involving differentiation under the integral sign.\n\n---\n\n### **Solution**\n\nWe aim to evaluate the integral $I(n) = \\int_0^{\\infty} \\frac{\\ln x}{1+x^n} \\, dx$ for $n > 1$.\n\n#### **1. Convergence Analysis**\n\nFirst, we establish that the integral is well-defined.\nLet $f(x) = \\frac{\\ln x}{1+x^n}$.\n*   **Near $x \\to 0^+$:** The denominator $1+x^n \\approx 1$. Thus, $f(x) \\sim \\ln x$. Since $\\int_0^1 \\ln x \\, dx$ is absolutely convergent (calculating to $-1$), the integral converges at the lower bound.\n*   **Near $x \\to \\infty$:** For large $x$, $1+x^n \\sim x^n$. Thus, $f(x) \\sim \\frac{\\ln x}{x^n}$. Since $n > 1$, let $\\epsilon = \\frac{n-1}{2} > 0$. We have $\\frac{\\ln x}{x^n} = \\frac{\\ln x}{x^\\epsilon} \\cdot \\frac{1}{x^{n-\\epsilon}}$. Since $\\frac{\\ln x}{x^\\epsilon} \\to 0$, the tail is bounded by $C \\cdot \\frac{1}{x^{1+\\epsilon}}$, which converges by the $p$-test ($p = 1 + \\epsilon > 1$).\n\nThus, $I(n)$ converges for all $n > 1$.\n\n#### **2. Introduction of a Parameterized Integral**\n\nWe introduce a parameterized auxiliary integral (the Mellin transform of the function $\\frac{1}{1+x^n}$):\n$$ J(s) = \\int_0^{\\infty} \\frac{x^{s-1}}{1+x^n} \\, dx $$\nNotice that\n$$ \\frac{\\partial}{\\partial s} \\left( \\frac{x^{s-1}}{1+x^n} \\right) = \\frac{x^{s-1} \\ln x}{1+x^n} $$\nEvaluating this derivative at $s=1$ yields our target integrand:\n$$ J'(1) = \\int_0^{\\infty} \\frac{\\ln x}{1+x^n} \\, dx = I(n) $$\n\nTo rigorously justify differentiation under the integral sign, we verify the uniform convergence of the integral of the derivative. Let $s \\in [1-\\delta, 1+\\delta]$. We require the domain of integration for $s$ to satisfy $0 < s < n$ for $J(s)$ to converge. Since $n > 1$, we can choose $\\delta > 0$ such that $[1-\\delta, 1+\\delta] \\subset (0, n)$.\n*   On $(0, 1]$, $|x^{s-1}| \\le x^{-\\delta}$.\n*   On $[1, \\infty)$, $|x^{s-1}| \\le x^{\\delta}$.\nThe dominating function $g(x) = \\frac{|\\ln x| (x^{-\\delta} + x^{\\delta})}{1+x^n}$ is integrable on $(0, \\infty)$ given the choice of $\\delta$. By the Leibniz Integral Rule, $I(n) = J'(1)$.\n\n#### **3. Evaluation of $J(s)$ via Beta and Gamma Functions**\n\nWe evaluate $J(s)$ using a substitution to map it to the Beta function definition, $B(a, b) = \\int_0^\\infty \\frac{t^{a-1}}{(1+t)^{a+b}} \\, dt$.\n\nLet $u = x^n$. Then $x = u^{1/n}$ and $dx = \\frac{1}{n} u^{1/n - 1} \\, du$.\nSubstituting into $J(s)$:\n$$ J(s) = \\int_0^{\\infty} \\frac{(u^{1/n})^{s-1}}{1+u} \\cdot \\frac{1}{n} u^{1/n - 1} \\, du $$\n$$ J(s) = \\frac{1}{n} \\int_0^{\\infty} \\frac{u^{s/n - 1/n + 1/n - 1}}{1+u} \\, du $$\n$$ J(s) = \\frac{1}{n} \\int_0^{\\infty} \\frac{u^{s/n - 1}}{1+u} \\, du $$\n\nComparing this to the Beta function form $B(a, b)$ with $a = s/n$:\nSince the denominator is $(1+u)^1$, we must have $a+b=1$, implying $b = 1 - s/n$.\nThus:\n$$ J(s) = \\frac{1}{n} B\\left(\\frac{s}{n}, 1 - \\frac{s}{n}\\right) $$\n\nWe relate the Beta function to the Gamma function using the identity $B(x, y) = \\frac{\\Gamma(x)\\Gamma(y)}{\\Gamma(x+y)}$:\n$$ J(s) = \\frac{1}{n} \\frac{\\Gamma(s/n)\\Gamma(1 - s/n)}{\\Gamma(1)} $$\nSince $\\Gamma(1) = 1$, we invoke Euler's Reflection Formula, $\\Gamma(z)\\Gamma(1-z) = \\frac{\\pi}{\\sin(\\pi z)}$, with $z = s/n$:\n$$ J(s) = \\frac{1}{n} \\cdot \\frac{\\pi}{\\sin(\\pi s/n)} $$\nThis closed form is valid for $0 < s < n$.\n\n#### **4. Computing the Derivative**\n\nWe now compute $I(n) = J'(1)$.\n$$ J(s) = \\frac{\\pi}{n} \\csc\\left(\\frac{\\pi s}{n}\\right) $$\nDifferentiating with respect to $s$:\n$$ J'(s) = \\frac{\\pi}{n} \\cdot \\frac{d}{ds} \\left( \\csc\\left(\\frac{\\pi s}{n}\\right) \\right) $$\nUsing the chain rule $\\frac{d}{dx} \\csc(u) = -\\csc(u)\\cot(u) \\frac{du}{dx}$:\n$$ J'(s) = \\frac{\\pi}{n} \\left( -\\csc\\left(\\frac{\\pi s}{n}\\right) \\cot\\left(\\frac{\\pi s}{n}\\right) \\cdot \\frac{\\pi}{n} \\right) $$\n$$ J'(s) = - \\frac{\\pi^2}{n^2} \\frac{\\cos(\\pi s/n)}{\\sin^2(\\pi s/n)} $$\n\nNow, evaluate at $s=1$:\n$$ I(n) = J'(1) = - \\frac{\\pi^2}{n^2} \\frac{\\cos(\\pi/n)}{\\sin^2(\\pi/n)} $$\n\n#### **Conclusion**\n\nThe value of the integral for $n > 1$ is:\n\n$$ \\int_0^{\\infty} \\frac{\\ln x}{1+x^n} \\, dx = - \\frac{\\pi^2}{n^2} \\frac{\\cos(\\frac{\\pi}{n})}{\\sin^2(\\frac{\\pi}{n})} $$\n\n**Verification of Special Cases:**\n1.  **$n=2$**: $\\cos(\\pi/2) = 0$. The formula gives $0$.\n    *   Check: $\\int_0^\\infty \\frac{\\ln x}{1+x^2} dx$. Let $x=1/t$, $dx = -1/t^2 dt$.\n    *   $I = \\int_\\infty^0 \\frac{\\ln(1/t)}{1+1/t^2} (-1/t^2) dt = \\int_0^\\infty \\frac{-\\ln t}{t^2+1} dt = -I \\implies 2I=0 \\implies I=0$. Consistent.\n2.  **Sign check**: For large $n$, $\\frac{\\pi}{n}$ is small and positive. $\\cos(\\pi/n) \\approx 1$, $\\sin^2(\\pi/n) > 0$. The result is negative.\n    *   In the integral, for $x \\in (0,1)$, $\\ln x < 0$. For $x > 1$, $\\ln x > 0$.\n    *   For large $n$, $\\frac{1}{1+x^n} \\approx 1$ on $(0,1)$ and $\\approx 0$ on $(1, \\infty)$. The negative contribution from $(0,1)$ dominates. The negative sign is consistent.",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ],
    "outer_attempt": 1
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "## Step 1: Understand the task\nThe task is to generate a challenging mathematical problem in the domain of ordinary differential equations (ODEs), along with a complete and clear solution. The problem should be solvable, verifiable, and not require external resources or conjectures.\n\n## Step 2: Formulate the problem\nTo create a meaningful problem, we need to consider an ODE that is not trivial and involves some level of mathematical sophistication. Let's consider a second-order linear homogeneous differential equation with constant coefficients, which is a standard topic in ODEs.\n\n## Step 3: Propose a specific problem\nConsider the differential equation $y'' + 4y' + 4y = 0$. The task is to find the general solution to this equation.\n\n## Step 4: Verify the problem meets the criteria\n- **Non-trivial**: The equation is not straightforward to solve by inspection.\n- **Solvable**: The equation can be solved using standard methods for linear differential equations with constant coefficients.\n- **Verifiable**: The solution can be verified by substitution into the differential equation.",
    "run_id": "22",
    "topic_slug": "ordinary_differential_equations",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "Based on the input provided, the mathematical task is identified in \"Step 3\" of the description.\n\n**Problem:** Find the general solution to the differential equation:\n$$ y'' + 4y' + 4y = 0 $$\n\n---\n\n### Solution\n\n**1. Classification of the Differential Equation**\nThe given equation, $y'' + 4y' + 4y = 0$, is a second-order, linear, homogeneous ordinary differential equation with constant coefficients. We seek a solution $y(x)$ defined for $x \\in \\mathbb{R}$.\n\n**2. The Characteristic Equation**\nWe assume a solution of the form $y = e^{rx}$, where $r$ is a constant. Substituting $y = e^{rx}$, $y' = re^{rx}$, and $y'' = r^2e^{rx}$ into the differential equation gives:\n$$ r^2 e^{rx} + 4r e^{rx} + 4 e^{rx} = 0 $$\nSince $e^{rx} \\neq 0$ for any real $x$, we can divide by $e^{rx}$ to obtain the characteristic equation:\n$$ r^2 + 4r + 4 = 0 $$\n\n**3. Solving for the Roots**\nWe solve the quadratic equation for $r$ by factoring:\n$$ (r + 2)^2 = 0 $$\nThis equation yields a single repeated real root:\n$$ r = -2 $$\nwith multiplicity 2.\n\n**4. Construction of the General Solution**\nFor a second-order linear homogeneous differential equation with constant coefficients, if the characteristic equation has a repeated real root $r$, the two linearly independent solutions are:\n$$ y_1(x) = e^{rx} \\quad \\text{and} \\quad y_2(x) = x e^{rx} $$\nSubstituting $r = -2$, we have:\n$$ y_1(x) = e^{-2x} \\quad \\text{and} \\quad y_2(x) = x e^{-2x} $$\nThe general solution is the linear combination of these basis functions:\n$$ y(x) = C_1 e^{-2x} + C_2 x e^{-2x} $$\nwhere $C_1$ and $C_2$ are arbitrary constants.\n\n**5. Verification**\nTo ensure correctness, we differentiate the proposed solution and substitute it back into the original equation.\nLet $y = (C_1 + C_2 x)e^{-2x}$.\n\nFirst derivative using the product rule:\n$$ \\begin{aligned} y' &= C_2 e^{-2x} + (C_1 + C_2 x)(-2e^{-2x}) \\\\ &= (C_2 - 2C_1 - 2C_2 x)e^{-2x} \\end{aligned} $$\n\nSecond derivative:\n$$ \\begin{aligned} y'' &= (-2C_2)e^{-2x} + (C_2 - 2C_1 - 2C_2 x)(-2e^{-2x}) \\\\ &= (-2C_2 - 2C_2 + 4C_1 + 4C_2 x)e^{-2x} \\\\ &= (4C_1 - 4C_2 + 4C_2 x)e^{-2x} \\end{aligned} $$\n\nSubstitute $y$, $y'$, and $y''$ into $y'' + 4y' + 4y$:\n$$ \\begin{aligned} \\text{LHS} &= e^{-2x} [ (4C_1 - 4C_2 + 4C_2 x) + 4(C_2 - 2C_1 - 2C_2 x) + 4(C_1 + C_2 x) ] \\\\ &= e^{-2x} [ 4C_1 - 4C_2 + 4C_2 x + 4C_2 - 8C_1 - 8C_2 x + 4C_1 + 4C_2 x ] \\end{aligned} $$\nGrouping terms by powers of $x$:\n- Constant terms inside bracket: $4C_1 - 4C_2 + 4C_2 - 8C_1 + 4C_1 = (4-8+4)C_1 + (-4+4)C_2 = 0$.\n- $x$ terms inside bracket: $4C_2 x - 8C_2 x + 4C_2 x = (4-8+4)C_2 x = 0$.\n\n$$ \\text{LHS} = e^{-2x} [ 0 ] = 0 = \\text{RHS} $$\nThe solution satisfies the differential equation.\n\n**Conclusion**\nThe general solution to the differential equation $y'' + 4y' + 4y = 0$ is:\n$$ y(x) = C_1 e^{-2x} + C_2 x e^{-2x} $$\nwhere $C_1$ and $C_2$ are arbitrary constants.",
        "raw_answer": "Based on the input provided, the mathematical task is identified in \"Step 3\" of the description.\n\n**Problem:** Find the general solution to the differential equation:\n$$ y'' + 4y' + 4y = 0 $$\n\n---\n\n### Solution\n\n**1. Classification of the Differential Equation**\nThe given equation, $y'' + 4y' + 4y = 0$, is a second-order, linear, homogeneous ordinary differential equation with constant coefficients. We seek a solution $y(x)$ defined for $x \\in \\mathbb{R}$.\n\n**2. The Characteristic Equation**\nWe assume a solution of the form $y = e^{rx}$, where $r$ is a constant. Substituting $y = e^{rx}$, $y' = re^{rx}$, and $y'' = r^2e^{rx}$ into the differential equation gives:\n$$ r^2 e^{rx} + 4r e^{rx} + 4 e^{rx} = 0 $$\nSince $e^{rx} \\neq 0$ for any real $x$, we can divide by $e^{rx}$ to obtain the characteristic equation:\n$$ r^2 + 4r + 4 = 0 $$\n\n**3. Solving for the Roots**\nWe solve the quadratic equation for $r$ by factoring:\n$$ (r + 2)^2 = 0 $$\nThis equation yields a single repeated real root:\n$$ r = -2 $$\nwith multiplicity 2.\n\n**4. Construction of the General Solution**\nFor a second-order linear homogeneous differential equation with constant coefficients, if the characteristic equation has a repeated real root $r$, the two linearly independent solutions are:\n$$ y_1(x) = e^{rx} \\quad \\text{and} \\quad y_2(x) = x e^{rx} $$\nSubstituting $r = -2$, we have:\n$$ y_1(x) = e^{-2x} \\quad \\text{and} \\quad y_2(x) = x e^{-2x} $$\nThe general solution is the linear combination of these basis functions:\n$$ y(x) = C_1 e^{-2x} + C_2 x e^{-2x} $$\nwhere $C_1$ and $C_2$ are arbitrary constants.\n\n**5. Verification**\nTo ensure correctness, we differentiate the proposed solution and substitute it back into the original equation.\nLet $y = (C_1 + C_2 x)e^{-2x}$.\n\nFirst derivative using the product rule:\n$$ \\begin{aligned} y' &= C_2 e^{-2x} + (C_1 + C_2 x)(-2e^{-2x}) \\\\ &= (C_2 - 2C_1 - 2C_2 x)e^{-2x} \\end{aligned} $$\n\nSecond derivative:\n$$ \\begin{aligned} y'' &= (-2C_2)e^{-2x} + (C_2 - 2C_1 - 2C_2 x)(-2e^{-2x}) \\\\ &= (-2C_2 - 2C_2 + 4C_1 + 4C_2 x)e^{-2x} \\\\ &= (4C_1 - 4C_2 + 4C_2 x)e^{-2x} \\end{aligned} $$\n\nSubstitute $y$, $y'$, and $y''$ into $y'' + 4y' + 4y$:\n$$ \\begin{aligned} \\text{LHS} &= e^{-2x} [ (4C_1 - 4C_2 + 4C_2 x) + 4(C_2 - 2C_1 - 2C_2 x) + 4(C_1 + C_2 x) ] \\\\ &= e^{-2x} [ 4C_1 - 4C_2 + 4C_2 x + 4C_2 - 8C_1 - 8C_2 x + 4C_1 + 4C_2 x ] \\end{aligned} $$\nGrouping terms by powers of $x$:\n- Constant terms inside bracket: $4C_1 - 4C_2 + 4C_2 - 8C_1 + 4C_1 = (4-8+4)C_1 + (-4+4)C_2 = 0$.\n- $x$ terms inside bracket: $4C_2 x - 8C_2 x + 4C_2 x = (4-8+4)C_2 x = 0$.\n\n$$ \\text{LHS} = e^{-2x} [ 0 ] = 0 = \\text{RHS} $$\nThe solution satisfies the differential equation.\n\n**Conclusion**\nThe general solution to the differential equation $y'' + 4y' + 4y = 0$ is:\n$$ y(x) = C_1 e^{-2x} + C_2 x e^{-2x} $$\nwhere $C_1$ and $C_2$ are arbitrary constants.",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ],
    "outer_attempt": 1
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "Consider the one-dimensional heat equation given by $\\frac{\\partial u}{\\partial t} = \\alpha \\frac{\\partial^2 u}{\\partial x^2}$, where $\\alpha > 0$ is a constant, $x \\in [0, L]$, and $t \\geq 0$. The boundary conditions are $u(0, t) = u(L, t) = 0$ for all $t \\geq 0$, and the initial condition is $u(x, 0) = \\sin(\\frac{\\pi x}{L}) + 2\\sin(\\frac{3\\pi x}{L})$. Find the solution $u(x, t)$ for $x \\in [0, L]$ and $t \\geq 0$.",
    "run_id": "23",
    "topic_slug": "partial_differential_equations",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "To solve the given Initial-Boundary Value Problem (IBVP) for the one-dimensional heat equation, we will use the method of **separation of variables**.\n\n### 1. Problem Statement\n\nWe seek the function $u(x, t)$ satisfying the partial differential equation (PDE):\n$$ \\frac{\\partial u}{\\partial t} = \\alpha \\frac{\\partial^2 u}{\\partial x^2}, \\quad 0 < x < L, \\quad t > 0 $$\nsubject to the Dirichlet boundary conditions:\n$$ u(0, t) = 0, \\quad u(L, t) = 0, \\quad \\text{for } t \\ge 0 $$\nand the initial condition:\n$$ u(x, 0) = \\sin\\left(\\frac{\\pi x}{L}\\right) + 2\\sin\\left(\\frac{3\\pi x}{L}\\right), \\quad 0 \\le x \\le L $$\n\n### 2. Separation of Variables\n\nWe assume the solution admits a separable form:\n$$ u(x, t) = X(x)T(t) $$\nSubstituting this into the PDE gives:\n$$ X(x)T'(t) = \\alpha X''(x)T(t) $$\nDividing by $\\alpha X(x)T(t)$ (assuming non-zero solution) separates the variables:\n$$ \\frac{T'(t)}{\\alpha T(t)} = \\frac{X''(x)}{X(x)} $$\nSince the left side depends only on $t$ and the right side only on $x$, both must equal a constant, which we denote by $-\\lambda$:\n$$ \\frac{T'}{\\alpha T} = \\frac{X''}{X} = -\\lambda $$\n\nThis yields two ordinary differential equations:\n1.  **Spatial Equation:** $X''(x) + \\lambda X(x) = 0$\n2.  **Temporal Equation:** $T'(t) + \\alpha \\lambda T(t) = 0$\n\n### 3. Solving the Spatial Problem\n\nWe apply the boundary conditions to the spatial equation:\n$$ u(0, t) = X(0)T(t) = 0 \\implies X(0) = 0 $$\n$$ u(L, t) = X(L)T(t) = 0 \\implies X(L) = 0 $$\n(We assume $T(t)$ is not identically zero to avoid the trivial solution $u \\equiv 0$).\n\nWe solve the Sturm-Liouville problem:\n$$ X'' + \\lambda X = 0, \\quad X(0) = 0, \\quad X(L) = 0 $$\n\nWe analyze the possible values for $\\lambda$:\n\n*   **Case 1: $\\lambda < 0$** (Let $\\lambda = -k^2$ where $k > 0$)\n    The general solution is $X(x) = c_1 e^{kx} + c_2 e^{-kx}$.\n    $X(0) = c_1 + c_2 = 0 \\implies c_2 = -c_1$.\n    $X(L) = c_1(e^{kL} - e^{-kL}) = 2c_1 \\sinh(kL) = 0$.\n    Since $k > 0$ and $L > 0$, $\\sinh(kL) \\neq 0$, so $c_1 = 0$, implying $X(x) \\equiv 0$. No non-trivial solutions.\n\n*   **Case 2: $\\lambda = 0$**\n    The general solution is $X(x) = c_1 x + c_2$.\n    $X(0) = c_2 = 0$.\n    $X(L) = c_1 L = 0 \\implies c_1 = 0$ (since $L > 0$).\n    Thus $X(x) \\equiv 0$. No non-trivial solutions.\n\n*   **Case 3: $\\lambda > 0$** (Let $\\lambda = k^2$ where $k > 0$)\n    The general solution is $X(x) = A \\cos(kx) + B \\sin(kx)$.\n    $X(0) = A = 0$.\n    $X(L) = B \\sin(kL) = 0$.\n    For a non-trivial solution, we require $B \\neq 0$, so $\\sin(kL) = 0$.\n    This implies $kL = n\\pi$ for $n = 1, 2, 3, \\dots$\n    Thus, $k_n = \\frac{n\\pi}{L}$.\n\nThe eigenvalues are $\\lambda_n = \\left(\\frac{n\\pi}{L}\\right)^2$ and the corresponding eigenfunctions are:\n$$ X_n(x) = \\sin\\left(\\frac{n\\pi x}{L}\\right), \\quad n = 1, 2, 3, \\dots $$\n\n### 4. Solving the Temporal Problem\n\nFor each eigenvalue $\\lambda_n$, the temporal equation is:\n$$ T_n'(t) + \\alpha \\left(\\frac{n\\pi}{L}\\right)^2 T_n(t) = 0 $$\nThe solution is:\n$$ T_n(t) = C_n \\exp\\left[-\\alpha \\left(\\frac{n\\pi}{L}\\right)^2 t\\right] $$\n\n### 5. General Solution Construction\n\nBy the principle of superposition for linear homogeneous PDEs, the general solution is the series sum of the product solutions $u_n(x,t) = X_n(x)T_n(t)$:\n$$ u(x, t) = \\sum_{n=1}^{\\infty} B_n \\sin\\left(\\frac{n\\pi x}{L}\\right) \\exp\\left[-\\alpha \\left(\\frac{n\\pi}{L}\\right)^2 t\\right] $$\nwhere $B_n$ are constants determined by the initial condition.\n\n### 6. Applying the Initial Condition\n\nWe evaluate the general solution at $t = 0$:\n$$ u(x, 0) = \\sum_{n=1}^{\\infty} B_n \\sin\\left(\\frac{n\\pi x}{L}\\right) $$\nThe problem states the initial condition is:\n$$ u(x, 0) = \\sin\\left(\\frac{\\pi x}{L}\\right) + 2\\sin\\left(\\frac{3\\pi x}{L}\\right) $$\nBecause the eigenfunctions $\\left\\{\\sin\\left(\\frac{n\\pi x}{L}\\right)\\right\\}_{n=1}^{\\infty}$ form an orthogonal basis on $[0, L]$, the Fourier sine series coefficients are unique. We can determine $B_n$ by comparing the terms directly:\n\n1.  For $n=1$, the term is $\\sin\\left(\\frac{\\pi x}{L}\\right)$. The coefficient is $B_1 = 1$.\n2.  For $n=3$, the term is $\\sin\\left(\\frac{3\\pi x}{L}\\right)$. The coefficient is $B_3 = 2$.\n3.  For all other $n \\neq 1, 3$, the coefficients are $B_n = 0$.\n\nSubstituting these non-zero coefficients back into the general series solution:\n\n$$ u(x, t) = (1) \\sin\\left(\\frac{\\pi x}{L}\\right) e^{-\\alpha \\left(\\frac{\\pi}{L}\\right)^2 t} + (2) \\sin\\left(\\frac{3\\pi x}{L}\\right) e^{-\\alpha \\left(\\frac{3\\pi}{L}\\right)^2 t} $$\n\n### 7. Final Result\n\nThe solution to the heat equation is:\n\n$$ u(x, t) = \\sin\\left(\\frac{\\pi x}{L}\\right) e^{-\\frac{\\alpha \\pi^2 t}{L^2}} + 2\\sin\\left(\\frac{3\\pi x}{L}\\right) e^{-\\frac{9\\alpha \\pi^2 t}{L^2}} $$",
        "raw_answer": "To solve the given Initial-Boundary Value Problem (IBVP) for the one-dimensional heat equation, we will use the method of **separation of variables**.\n\n### 1. Problem Statement\n\nWe seek the function $u(x, t)$ satisfying the partial differential equation (PDE):\n$$ \\frac{\\partial u}{\\partial t} = \\alpha \\frac{\\partial^2 u}{\\partial x^2}, \\quad 0 < x < L, \\quad t > 0 $$\nsubject to the Dirichlet boundary conditions:\n$$ u(0, t) = 0, \\quad u(L, t) = 0, \\quad \\text{for } t \\ge 0 $$\nand the initial condition:\n$$ u(x, 0) = \\sin\\left(\\frac{\\pi x}{L}\\right) + 2\\sin\\left(\\frac{3\\pi x}{L}\\right), \\quad 0 \\le x \\le L $$\n\n### 2. Separation of Variables\n\nWe assume the solution admits a separable form:\n$$ u(x, t) = X(x)T(t) $$\nSubstituting this into the PDE gives:\n$$ X(x)T'(t) = \\alpha X''(x)T(t) $$\nDividing by $\\alpha X(x)T(t)$ (assuming non-zero solution) separates the variables:\n$$ \\frac{T'(t)}{\\alpha T(t)} = \\frac{X''(x)}{X(x)} $$\nSince the left side depends only on $t$ and the right side only on $x$, both must equal a constant, which we denote by $-\\lambda$:\n$$ \\frac{T'}{\\alpha T} = \\frac{X''}{X} = -\\lambda $$\n\nThis yields two ordinary differential equations:\n1.  **Spatial Equation:** $X''(x) + \\lambda X(x) = 0$\n2.  **Temporal Equation:** $T'(t) + \\alpha \\lambda T(t) = 0$\n\n### 3. Solving the Spatial Problem\n\nWe apply the boundary conditions to the spatial equation:\n$$ u(0, t) = X(0)T(t) = 0 \\implies X(0) = 0 $$\n$$ u(L, t) = X(L)T(t) = 0 \\implies X(L) = 0 $$\n(We assume $T(t)$ is not identically zero to avoid the trivial solution $u \\equiv 0$).\n\nWe solve the Sturm-Liouville problem:\n$$ X'' + \\lambda X = 0, \\quad X(0) = 0, \\quad X(L) = 0 $$\n\nWe analyze the possible values for $\\lambda$:\n\n*   **Case 1: $\\lambda < 0$** (Let $\\lambda = -k^2$ where $k > 0$)\n    The general solution is $X(x) = c_1 e^{kx} + c_2 e^{-kx}$.\n    $X(0) = c_1 + c_2 = 0 \\implies c_2 = -c_1$.\n    $X(L) = c_1(e^{kL} - e^{-kL}) = 2c_1 \\sinh(kL) = 0$.\n    Since $k > 0$ and $L > 0$, $\\sinh(kL) \\neq 0$, so $c_1 = 0$, implying $X(x) \\equiv 0$. No non-trivial solutions.\n\n*   **Case 2: $\\lambda = 0$**\n    The general solution is $X(x) = c_1 x + c_2$.\n    $X(0) = c_2 = 0$.\n    $X(L) = c_1 L = 0 \\implies c_1 = 0$ (since $L > 0$).\n    Thus $X(x) \\equiv 0$. No non-trivial solutions.\n\n*   **Case 3: $\\lambda > 0$** (Let $\\lambda = k^2$ where $k > 0$)\n    The general solution is $X(x) = A \\cos(kx) + B \\sin(kx)$.\n    $X(0) = A = 0$.\n    $X(L) = B \\sin(kL) = 0$.\n    For a non-trivial solution, we require $B \\neq 0$, so $\\sin(kL) = 0$.\n    This implies $kL = n\\pi$ for $n = 1, 2, 3, \\dots$\n    Thus, $k_n = \\frac{n\\pi}{L}$.\n\nThe eigenvalues are $\\lambda_n = \\left(\\frac{n\\pi}{L}\\right)^2$ and the corresponding eigenfunctions are:\n$$ X_n(x) = \\sin\\left(\\frac{n\\pi x}{L}\\right), \\quad n = 1, 2, 3, \\dots $$\n\n### 4. Solving the Temporal Problem\n\nFor each eigenvalue $\\lambda_n$, the temporal equation is:\n$$ T_n'(t) + \\alpha \\left(\\frac{n\\pi}{L}\\right)^2 T_n(t) = 0 $$\nThe solution is:\n$$ T_n(t) = C_n \\exp\\left[-\\alpha \\left(\\frac{n\\pi}{L}\\right)^2 t\\right] $$\n\n### 5. General Solution Construction\n\nBy the principle of superposition for linear homogeneous PDEs, the general solution is the series sum of the product solutions $u_n(x,t) = X_n(x)T_n(t)$:\n$$ u(x, t) = \\sum_{n=1}^{\\infty} B_n \\sin\\left(\\frac{n\\pi x}{L}\\right) \\exp\\left[-\\alpha \\left(\\frac{n\\pi}{L}\\right)^2 t\\right] $$\nwhere $B_n$ are constants determined by the initial condition.\n\n### 6. Applying the Initial Condition\n\nWe evaluate the general solution at $t = 0$:\n$$ u(x, 0) = \\sum_{n=1}^{\\infty} B_n \\sin\\left(\\frac{n\\pi x}{L}\\right) $$\nThe problem states the initial condition is:\n$$ u(x, 0) = \\sin\\left(\\frac{\\pi x}{L}\\right) + 2\\sin\\left(\\frac{3\\pi x}{L}\\right) $$\nBecause the eigenfunctions $\\left\\{\\sin\\left(\\frac{n\\pi x}{L}\\right)\\right\\}_{n=1}^{\\infty}$ form an orthogonal basis on $[0, L]$, the Fourier sine series coefficients are unique. We can determine $B_n$ by comparing the terms directly:\n\n1.  For $n=1$, the term is $\\sin\\left(\\frac{\\pi x}{L}\\right)$. The coefficient is $B_1 = 1$.\n2.  For $n=3$, the term is $\\sin\\left(\\frac{3\\pi x}{L}\\right)$. The coefficient is $B_3 = 2$.\n3.  For all other $n \\neq 1, 3$, the coefficients are $B_n = 0$.\n\nSubstituting these non-zero coefficients back into the general series solution:\n\n$$ u(x, t) = (1) \\sin\\left(\\frac{\\pi x}{L}\\right) e^{-\\alpha \\left(\\frac{\\pi}{L}\\right)^2 t} + (2) \\sin\\left(\\frac{3\\pi x}{L}\\right) e^{-\\alpha \\left(\\frac{3\\pi}{L}\\right)^2 t} $$\n\n### 7. Final Result\n\nThe solution to the heat equation is:\n\n$$ u(x, t) = \\sin\\left(\\frac{\\pi x}{L}\\right) e^{-\\frac{\\alpha \\pi^2 t}{L^2}} + 2\\sin\\left(\\frac{3\\pi x}{L}\\right) e^{-\\frac{9\\alpha \\pi^2 t}{L^2}} $$",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ],
    "outer_attempt": 1
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "## Step 1: Understand the task\nThe task is to generate a challenging but solvable mathematics problem in the domain of dynamical systems and ergodic theory, along with a complete and verifiable solution.\n\n## Step 2: Identify key concepts in dynamical systems and ergodic theory\nDynamical systems involve the study of the behavior of systems that change over time according to certain rules. Ergodic theory is a branch of mathematics that studies the behavior of dynamical systems over time, focusing on their statistical properties. Key concepts include ergodicity, measure-preserving transformations, and the ergodic theorem.\n\n## Step 3: Formulate a problem\nTo create a problem, we need to incorporate concepts from dynamical systems and ergodic theory. A potential problem could involve analyzing the behavior of a specific dynamical system, such as determining if it is ergodic or calculating its invariant measures.\n\n## Step 4: Specify the problem\nLet's consider a simple yet challenging example: the rotation map on the unit circle. The unit circle can be represented as $\\mathbb{T} = \\mathbb{R}/\\mathbb{Z}$, and a rotation map $T: \\mathbb{T} \\to \\mathbb{T}$ is defined by $T(x) = x + \\alpha \\mod 1$, where $\\alpha \\in \\mathbb{R}$.\n\n## Step 5: Formulate the question\nWe can ask whether the rotation map $T$ is ergodic with respect to the Lebesgue measure on $\\mathbb{T}$ when $\\alpha$ is irrational.\n\n## Step 6: Write the problem statement\n\"Let $T: \\mathbb{T} \\to \\mathbb{T}$ be defined by $T(x) = x + \\alpha \\mod 1$, where $\\alpha \\in \\mathbb{R}$ is irrational. Prove that $T$ is ergodic with respect to the Lebesgue measure on $\\mathbb{T}$.\"\n\n## Step 7: Provide the solution\nTo prove that $T$ is ergodic, we need to show that if $A \\subset \\mathbb{T}$ is $T$-invariant (i.e., $T(A) = A$) and measurable, then either $m(A) = 0$ or $m(A) = 1$, where $m$ denotes the Lebesgue measure.\n\n## Step 8: Detail the solution\nFirst, recall that $T$ is measure-preserving because $m(T^{-1}(A)) = m(A)$ for any measurable $A \\subset \\mathbb{T}$. For $T$ to be ergodic, we must show that $T$-invariant sets have measure 0 or 1.\n\n## Step 9: Use Fourier analysis to prove ergodicity\nConsider a $T$-invariant set $A$. The characteristic function $\\chi_A$ of $A$ is $T$-invariant, meaning $\\chi_A(T(x)) = \\chi_A(x)$. We can expand $\\chi_A$ in a Fourier series: $\\chi_A(x) = \\sum_{n=-\\infty}^{\\infty} c_n e^{2\\pi inx}$, where $c_n = \\int_{\\mathbb{T}} \\chi_A(x)e^{-2\\pi inx} dx$.\n\n## Step 10: Apply $T$-invariance to the Fourier series\nSince $\\chi_A(T(x)) = \\chi_A(x + \\alpha) = \\sum_{n=-\\infty}^{\\infty} c_n e^{2\\pi in(x + \\alpha)} = \\sum_{n=-\\infty}^{\\infty} c_n e^{2\\pi in\\alpha}e^{2\\pi inx}$, and this equals $\\chi_A(x)$, we have $c_n = c_n e^{2\\pi in\\alpha}$ for all $n$. For $\\alpha$ irrational, $e^{2\\pi in\\alpha} \\neq 1$ unless $n = 0$. Thus, $c_n = 0$ for $n \\neq 0$.\n\n## Step 11: Conclude ergodicity from the Fourier analysis\nThis implies $\\chi_A(x) = c_0 = \\int_{\\mathbb{T}} \\chi_A(x) dx = m(A)$, meaning $\\chi_A$ is constant. Hence, $A$ is either $\\mathbb{T}$ (if $m(A) = 1$) or $\\emptyset$ (if $m(A) = 0$), proving $T$ is ergodic.\n\n## Step 12: Format the question and answer according to the required structure\n\nLet $T: \\mathbb{T} \\to \\mathbb{T}$ be defined by $T(x) = x + \\alpha \\mod 1$, where $\\alpha \\in \\mathbb{R}$ is irrational. Prove that $T$ is ergodic with respect to the Lebesgue measure on $\\mathbb{T}$.",
    "run_id": "24",
    "topic_slug": "dynamical_systems_and_ergodic_theory",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "Here is the complete and rigorous proof for the ergodic property of the irrational rotation map.\n\n### **Theorem**\nLet $\\mathbb{T} = \\mathbb{R}/\\mathbb{Z}$ be the unit circle equipped with the Borel $\\sigma$-algebra and the Lebesgue measure $\\mu$. Let $\\alpha \\in \\mathbb{R}$ be irrational. Define the rotation map $T: \\mathbb{T} \\to \\mathbb{T}$ by:\n$$T(x) = x + \\alpha \\pmod 1$$\nThen $T$ is ergodic with respect to $\\mu$.\n\n### **Proof**\n\n**1. Definitions and Characterization of Ergodicity**\nFirst, we note that $T$ is measure-preserving. Since the Lebesgue measure is translation-invariant on $\\mathbb{R}$, the map $x \\mapsto x+\\alpha$ preserves measure locally, and wrapping modulo 1 preserves the measure on $\\mathbb{T}$.\n\nTo prove ergodicity, we must show that any $T$-invariant measurable set $A \\subseteq \\mathbb{T}$ (meaning $T^{-1}(A) = A$ up to a set of measure zero) satisfies $\\mu(A) = 0$ or $\\mu(A) = 1$.\n\nAn equivalent and often more convenient characterization of ergodicity involves $L^2$ functions. A measure-preserving transformation $T$ is ergodic if and only if every function $f \\in L^2(\\mathbb{T}, \\mu)$ that satisfies $f \\circ T = f$ almost everywhere is constant almost everywhere. We will proceed using this functional analytic approach.\n\n**2. Fourier Analysis of Invariant Functions**\nLet $f \\in L^2(\\mathbb{T})$ be a function such that $f(Tx) = f(x)$ for $\\mu$-almost every $x$. Since $\\{e_n(x) = e^{2\\pi i n x} : n \\in \\mathbb{Z}\\}$ forms an orthonormal basis for $L^2(\\mathbb{T})$, we can expand $f$ as a Fourier series:\n$$f(x) = \\sum_{n \\in \\mathbb{Z}} \\hat{f}(n) e^{2\\pi i n x}$$\nwhere the convergence is in the $L^2$ norm, and the Fourier coefficients are given by:\n$$\\hat{f}(n) = \\int_{\\mathbb{T}} f(x) e^{-2\\pi i n x} \\, d\\mu(x)$$\n\nConsider the composition $f \\circ T$. Using the definition of $T(x) = x + \\alpha$, we have:\n$$(f \\circ T)(x) = f(x + \\alpha) = \\sum_{n \\in \\mathbb{Z}} \\hat{f}(n) e^{2\\pi i n (x + \\alpha)}$$\n$$ (f \\circ T)(x) = \\sum_{n \\in \\mathbb{Z}} \\left( \\hat{f}(n) e^{2\\pi i n \\alpha} \\right) e^{2\\pi i n x}$$\n\n**3. Comparing Coefficients**\nBy hypothesis, $f \\circ T = f$ almost everywhere. Since the Fourier coefficients of a function are unique, the coefficients of the expansion of $f(x)$ must match the coefficients of the expansion of $f(x+\\alpha)$ for all $n \\in \\mathbb{Z}$.\n\nEquating the coefficients for the $n$-th mode:\n$$\\hat{f}(n) = \\hat{f}(n) e^{2\\pi i n \\alpha}$$\nRearranging this equation, we obtain:\n$$\\hat{f}(n) (1 - e^{2\\pi i n \\alpha}) = 0$$\n\n**4. Utilizing the Irrationality of $\\alpha$**\nWe analyze the term $(1 - e^{2\\pi i n \\alpha})$ for different values of $n$:\n\n*   **Case $n = 0$:**\n    The equation becomes $\\hat{f}(0)(1 - e^0) = \\hat{f}(0)(0) = 0$. This places no constraint on $\\hat{f}(0)$.\n\n*   **Case $n \\neq 0$:**\n    We examine the factor $e^{2\\pi i n \\alpha}$. Since $\\alpha$ is irrational, the product $n\\alpha$ is never an integer for any non-zero integer $n$.\n    Consequently:\n    $$n\\alpha \\notin \\mathbb{Z} \\implies e^{2\\pi i n \\alpha} \\neq 1 \\implies 1 - e^{2\\pi i n \\alpha} \\neq 0$$\n    Since the product $\\hat{f}(n) (1 - e^{2\\pi i n \\alpha})$ is zero and the second factor is non-zero, it must be that:\n    $$\\hat{f}(n) = 0 \\quad \\text{for all } n \\neq 0$$\n\n**5. Conclusion**\nThe Fourier series for $f$ reduces to a single term:\n$$f(x) = \\hat{f}(0) e^{2\\pi i (0) x} = \\hat{f}(0)$$\nSince the Fourier series converges to $f$ in $L^2$, we conclude that $f(x) = \\text{constant}$ almost everywhere (specifically, the constant is the mean of the function).\n\nTo relate this back to the set-theoretic definition: Let $A$ be a measurable set such that $T^{-1}(A) = A$ a.e. Let $f = \\chi_A$ be the characteristic function of $A$. Since $A$ is invariant, $\\chi_A$ is an invariant function in $L^2(\\mathbb{T})$. By the proof above, $\\chi_A$ must be constant almost everywhere.\nSince $\\chi_A$ only takes values in $\\{0, 1\\}$, for it to be constant almost everywhere, it must be either $0$ a.e. or $1$ a.e.\n*   If $\\chi_A = 0$ a.e., then $\\mu(A) = 0$.\n*   If $\\chi_A = 1$ a.e., then $\\mu(A) = 1$.\n\nThus, $T$ is ergodic.\n\n**Q.E.D.**",
        "raw_answer": "Here is the complete and rigorous proof for the ergodic property of the irrational rotation map.\n\n### **Theorem**\nLet $\\mathbb{T} = \\mathbb{R}/\\mathbb{Z}$ be the unit circle equipped with the Borel $\\sigma$-algebra and the Lebesgue measure $\\mu$. Let $\\alpha \\in \\mathbb{R}$ be irrational. Define the rotation map $T: \\mathbb{T} \\to \\mathbb{T}$ by:\n$$T(x) = x + \\alpha \\pmod 1$$\nThen $T$ is ergodic with respect to $\\mu$.\n\n### **Proof**\n\n**1. Definitions and Characterization of Ergodicity**\nFirst, we note that $T$ is measure-preserving. Since the Lebesgue measure is translation-invariant on $\\mathbb{R}$, the map $x \\mapsto x+\\alpha$ preserves measure locally, and wrapping modulo 1 preserves the measure on $\\mathbb{T}$.\n\nTo prove ergodicity, we must show that any $T$-invariant measurable set $A \\subseteq \\mathbb{T}$ (meaning $T^{-1}(A) = A$ up to a set of measure zero) satisfies $\\mu(A) = 0$ or $\\mu(A) = 1$.\n\nAn equivalent and often more convenient characterization of ergodicity involves $L^2$ functions. A measure-preserving transformation $T$ is ergodic if and only if every function $f \\in L^2(\\mathbb{T}, \\mu)$ that satisfies $f \\circ T = f$ almost everywhere is constant almost everywhere. We will proceed using this functional analytic approach.\n\n**2. Fourier Analysis of Invariant Functions**\nLet $f \\in L^2(\\mathbb{T})$ be a function such that $f(Tx) = f(x)$ for $\\mu$-almost every $x$. Since $\\{e_n(x) = e^{2\\pi i n x} : n \\in \\mathbb{Z}\\}$ forms an orthonormal basis for $L^2(\\mathbb{T})$, we can expand $f$ as a Fourier series:\n$$f(x) = \\sum_{n \\in \\mathbb{Z}} \\hat{f}(n) e^{2\\pi i n x}$$\nwhere the convergence is in the $L^2$ norm, and the Fourier coefficients are given by:\n$$\\hat{f}(n) = \\int_{\\mathbb{T}} f(x) e^{-2\\pi i n x} \\, d\\mu(x)$$\n\nConsider the composition $f \\circ T$. Using the definition of $T(x) = x + \\alpha$, we have:\n$$(f \\circ T)(x) = f(x + \\alpha) = \\sum_{n \\in \\mathbb{Z}} \\hat{f}(n) e^{2\\pi i n (x + \\alpha)}$$\n$$ (f \\circ T)(x) = \\sum_{n \\in \\mathbb{Z}} \\left( \\hat{f}(n) e^{2\\pi i n \\alpha} \\right) e^{2\\pi i n x}$$\n\n**3. Comparing Coefficients**\nBy hypothesis, $f \\circ T = f$ almost everywhere. Since the Fourier coefficients of a function are unique, the coefficients of the expansion of $f(x)$ must match the coefficients of the expansion of $f(x+\\alpha)$ for all $n \\in \\mathbb{Z}$.\n\nEquating the coefficients for the $n$-th mode:\n$$\\hat{f}(n) = \\hat{f}(n) e^{2\\pi i n \\alpha}$$\nRearranging this equation, we obtain:\n$$\\hat{f}(n) (1 - e^{2\\pi i n \\alpha}) = 0$$\n\n**4. Utilizing the Irrationality of $\\alpha$**\nWe analyze the term $(1 - e^{2\\pi i n \\alpha})$ for different values of $n$:\n\n*   **Case $n = 0$:**\n    The equation becomes $\\hat{f}(0)(1 - e^0) = \\hat{f}(0)(0) = 0$. This places no constraint on $\\hat{f}(0)$.\n\n*   **Case $n \\neq 0$:**\n    We examine the factor $e^{2\\pi i n \\alpha}$. Since $\\alpha$ is irrational, the product $n\\alpha$ is never an integer for any non-zero integer $n$.\n    Consequently:\n    $$n\\alpha \\notin \\mathbb{Z} \\implies e^{2\\pi i n \\alpha} \\neq 1 \\implies 1 - e^{2\\pi i n \\alpha} \\neq 0$$\n    Since the product $\\hat{f}(n) (1 - e^{2\\pi i n \\alpha})$ is zero and the second factor is non-zero, it must be that:\n    $$\\hat{f}(n) = 0 \\quad \\text{for all } n \\neq 0$$\n\n**5. Conclusion**\nThe Fourier series for $f$ reduces to a single term:\n$$f(x) = \\hat{f}(0) e^{2\\pi i (0) x} = \\hat{f}(0)$$\nSince the Fourier series converges to $f$ in $L^2$, we conclude that $f(x) = \\text{constant}$ almost everywhere (specifically, the constant is the mean of the function).\n\nTo relate this back to the set-theoretic definition: Let $A$ be a measurable set such that $T^{-1}(A) = A$ a.e. Let $f = \\chi_A$ be the characteristic function of $A$. Since $A$ is invariant, $\\chi_A$ is an invariant function in $L^2(\\mathbb{T})$. By the proof above, $\\chi_A$ must be constant almost everywhere.\nSince $\\chi_A$ only takes values in $\\{0, 1\\}$, for it to be constant almost everywhere, it must be either $0$ a.e. or $1$ a.e.\n*   If $\\chi_A = 0$ a.e., then $\\mu(A) = 0$.\n*   If $\\chi_A = 1$ a.e., then $\\mu(A) = 1$.\n\nThus, $T$ is ergodic.\n\n**Q.E.D.**",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ],
    "outer_attempt": 1
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "## Step 1: Understand the task and the domain\nThe task is to generate a challenging but solvable mathematics problem in the domain of difference and functional equations, along with a complete and verifiable solution. The problem should be well-posed, clear, non-trivial, and solvable with established mathematical methods.\n\n## Step 2: Identify the key elements of a good problem in the domain\nA good problem in the domain of difference and functional equations should involve a functional equation or a difference equation that requires substantive mathematical reasoning to solve. It should be fully specified, with clear notation and constraints.\n\n## Step 3: Generate a problem\nLet's generate a problem: \"Find all functions $f: \\mathbb{R} \\to \\mathbb{R}$ that satisfy the functional equation $f(x + y) = f(x) + f(y) + xy$ for all $x, y \\in \\mathbb{R}$, and $f(1) = 1$.\"\n\n## Step 4: Verify that the generated problem meets the quality rubric\n- **Well-Posedness and Completeness**: The problem is fully self-contained, with the domain and codomain of $f$ specified, and the functional equation given.\n- **Clarity**: The notation is standard, and the task is clear.\n- **Non-Triviality**: The problem requires solving a functional equation, which is non-trivial.\n- **Solvability and Verification**: The problem is solvable using established methods for functional equations.\n\n## Step 5: Solve the generated problem\nTo solve $f(x + y) = f(x) + f(y) + xy$, let's first try substituting specific values for $x$ and $y$ to derive additional information.\n\n## Step 6: Substitute $x = y = 0$ into the equation\n$f(0 + 0) = f(0) + f(0) + 0 \\cdot 0$ simplifies to $f(0) = 2f(0)$, which implies $f(0) = 0$.\n\n## Step 7: Substitute $y = 1$ into the original equation\n$f(x + 1) = f(x) + f(1) + x = f(x) + x + 1$, since $f(1) = 1$. This gives a recursive formula to find $f(x)$ for integer values of $x$.\n\n## Step 8: Use the recursive formula to find $f(n)$ for any integer $n$\nStarting from $f(0) = 0$ and $f(1) = 1$, we can derive $f(2)$, $f(3)$, etc., to look for a pattern. $f(2) = f(1) + f(1) + 1 = 3$, $f(3) = f(2) + f(1) + 2 = 6$. It appears that $f(n) = \\frac{n(n+1)}{2}$ for positive integers $n$.\n\n## Step 9: Check if $f(x) = \\frac{x(x+1)}{2}$ satisfies the original functional equation\nSubstitute $f(x) = \\frac{x(x+1)}{2}$ into $f(x + y) = f(x) + f(y) + xy$:\n- LHS: $f(x + y) = \\frac{(x+y)(x+y+1)}{2} = \\frac{x^2 + 2xy + y^2 + x + y}{2}$\n- RHS: $f(x) + f(y) + xy = \\frac{x(x+1)}{2} + \\frac{y(y+1)}{2} + xy = \\frac{x^2 + x + y^2 + y + 2xy}{2}$\nBoth sides are equal, so $f(x) = \\frac{x(x+1)}{2}$ is a solution.\n\n## Step 10: Verify that the solution satisfies the initial condition\n$f(1) = \\frac{1(1+1)}{2} = 1$, which matches the given condition.\n\nThe final answer is: \n\nFind all functions $f: \\mathbb{R} \\to \\mathbb{R}$ that satisfy the functional equation $f(x + y) = f(x) + f(y) + xy$ for all $x, y \\in \\mathbb{R}$, and $f(1) = 1$.",
    "run_id": "25",
    "topic_slug": "difference_and_functional_equations",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "To find all functions $f: \\mathbb{R} \\to \\mathbb{R}$ that satisfy the functional equation\n$$ f(x + y) = f(x) + f(y) + xy $$\nfor all $x, y \\in \\mathbb{R}$, subject to the condition $f(1) = 1$, we proceed with the following steps.\n\n### Step 1: Reduce the Non-Homogeneous Equation\nThe term $xy$ in the functional equation makes it non-homogeneous. We seek to eliminate this term by finding a particular solution.\n\nConsider a polynomial solution of the form $p(x) = ax^2$. Substituting this into the equation:\n$$ a(x+y)^2 = ax^2 + ay^2 + xy $$\n$$ a(x^2 + 2xy + y^2) = ax^2 + ay^2 + xy $$\n$$ ax^2 + 2axy + ay^2 = ax^2 + ay^2 + xy $$\nComparing the terms, we must have $2axy = xy$, which implies $2a = 1$, or $a = \\frac{1}{2}$.\n\nThus, $p(x) = \\frac{1}{2}x^2$ satisfies the non-homogeneous part of the recurrence. We can now express the general function $f(x)$ as the sum of this particular solution and a homogeneous part. Let:\n$$ f(x) = \\frac{1}{2}x^2 + g(x) $$\nwhere $g: \\mathbb{R} \\to \\mathbb{R}$ is a new function to be determined.\n\n### Step 2: Determine the Equation for $g(x)$\nSubstitute $f(x) = \\frac{1}{2}x^2 + g(x)$ into the original functional equation:\n$$ \\frac{1}{2}(x+y)^2 + g(x+y) = \\left(\\frac{1}{2}x^2 + g(x)\\right) + \\left(\\frac{1}{2}y^2 + g(y)\\right) + xy $$\nExpand the left side:\n$$ \\frac{1}{2}(x^2 + 2xy + y^2) + g(x+y) = \\frac{1}{2}x^2 + g(x) + \\frac{1}{2}y^2 + g(y) + xy $$\n$$ \\frac{1}{2}x^2 + xy + \\frac{1}{2}y^2 + g(x+y) = \\frac{1}{2}x^2 + \\frac{1}{2}y^2 + xy + g(x) + g(y) $$\nSubtracting $\\frac{1}{2}x^2 + \\frac{1}{2}y^2 + xy$ from both sides yields:\n$$ g(x+y) = g(x) + g(y) $$\nThis is the **Cauchy Functional Equation**.\n\n### Step 3: Apply the Initial Condition\nWe are given $f(1) = 1$. Using the definition of $g(x)$:\n$$ f(1) = \\frac{1}{2}(1)^2 + g(1) = 1 $$\n$$ \\frac{1}{2} + g(1) = 1 \\implies g(1) = \\frac{1}{2} $$\n\n### Step 4: Solve the Cauchy Functional Equation\nThe function $g: \\mathbb{R} \\to \\mathbb{R}$ must satisfy:\n1. $g(x+y) = g(x) + g(y)$ for all $x, y \\in \\mathbb{R}$\n2. $g(1) = \\frac{1}{2}$\n\nFrom the additivity property $g(x+y) = g(x) + g(y)$, we can prove by induction that $g(nx) = n g(x)$ for all integers $n$. Furthermore, for any rational number $q = \\frac{m}{n}$ (where $m, n \\in \\mathbb{Z}, n \\neq 0$), we have $g(q) = q g(1)$.\n\nProof for rationals:\nSince $g(1) = \\frac{1}{2}$, for any integer $n$:\n$$ g(n) = g(n \\cdot 1) = n g(1) = \\frac{n}{2} $$\nFor any rational $x = \\frac{m}{n}$:\n$$ g\\left(n \\cdot \\frac{m}{n}\\right) = n g\\left(\\frac{m}{n}\\right) $$\n$$ g(m) = n g\\left(\\frac{m}{n}\\right) $$\n$$ \\frac{m}{2} = n g\\left(\\frac{m}{n}\\right) \\implies g\\left(\\frac{m}{n}\\right) = \\frac{1}{n} \\cdot \\frac{m}{2} = \\frac{1}{2} \\cdot \\frac{m}{n} $$\nThus, for all $x \\in \\mathbb{Q}$, $g(x) = \\frac{1}{2}x$.\n\nHowever, for $x \\notin \\mathbb{Q}$, $g(x)$ is not uniquely determined by the value at $1$ without additional assumptions (such as continuity, monotonicity, or boundedness). The general solution to the Cauchy Functional Equation is any additive function $A: \\mathbb{R} \\to \\mathbb{R}$.\n\nLet $A(x)$ be any additive function (i.e., $A(x+y)=A(x)+A(y)$). The condition $g(1) = 1/2$ restricts $A$ such that $A(1) = 1/2$.\nWe can write this as $g(x) = A(x)$, where $A(1) = 1/2$. Alternatively, we can define $H(x) = A(x) - \\frac{1}{2}x$. Then $H$ is an additive function with $H(1) = A(1) - \\frac{1}{2} = 0$. Since $H(1)=0$ and $H$ is additive, $H(q) = 0$ for all $q \\in \\mathbb{Q}$.\nThus, the most general form for $g(x)$ is:\n$$ g(x) = \\frac{1}{2}x + H(x) $$\nwhere $H: \\mathbb{R} \\to \\mathbb{R}$ is an additive function such that $H(x) = 0$ for all $x \\in \\mathbb{Q}$.\n\n### Step 5: Construct the Final Solution\nSubstituting $g(x)$ back into the expression for $f(x)$:\n$$ f(x) = \\frac{1}{2}x^2 + g(x) $$\n$$ f(x) = \\frac{1}{2}x^2 + \\frac{1}{2}x + H(x) $$\n$$ f(x) = \\frac{x(x+1)}{2} + H(x) $$\n\n**Verification:**\nLHS: $f(x+y) = \\frac{(x+y)^2 + (x+y)}{2} + H(x+y)$\nRHS: $f(x) + f(y) + xy = \\left(\\frac{x^2+x}{2} + H(x)\\right) + \\left(\\frac{y^2+y}{2} + H(y)\\right) + xy$\n\nExpanding terms:\nLHS: $\\frac{x^2 + 2xy + y^2 + x + y}{2} + H(x) + H(y)$ (since H is additive)\nRHS: $\\frac{x^2 + x + y^2 + y + 2xy}{2} + H(x) + H(y)$\nThe sides are identical.\nCondition $f(1) = \\frac{1(2)}{2} + H(1) = 1 + 0 = 1$. The condition holds.\n\n### Conclusion\n\nThe set of all solutions depends on whether any regularity conditions (like continuity) are assumed. Since the problem statement does not impose such conditions, we provide the general solution.\n\nThe complete set of functions $f: \\mathbb{R} \\to \\mathbb{R}$ satisfying the conditions is given by:\n$$ f(x) = \\frac{x(x+1)}{2} + H(x) $$\nwhere $H: \\mathbb{R} \\to \\mathbb{R}$ is any additive function (satisfying $H(x+y) = H(x) + H(y)$) that vanishes on all rational numbers (i.e., $H(q) = 0$ for all $q \\in \\mathbb{Q}$).\n\n*Note: If $f$ is assumed to be continuous, then $H(x)$ must be identically zero, and the unique solution is $f(x) = \\frac{x(x+1)}{2}$.*",
        "raw_answer": "To find all functions $f: \\mathbb{R} \\to \\mathbb{R}$ that satisfy the functional equation\n$$ f(x + y) = f(x) + f(y) + xy $$\nfor all $x, y \\in \\mathbb{R}$, subject to the condition $f(1) = 1$, we proceed with the following steps.\n\n### Step 1: Reduce the Non-Homogeneous Equation\nThe term $xy$ in the functional equation makes it non-homogeneous. We seek to eliminate this term by finding a particular solution.\n\nConsider a polynomial solution of the form $p(x) = ax^2$. Substituting this into the equation:\n$$ a(x+y)^2 = ax^2 + ay^2 + xy $$\n$$ a(x^2 + 2xy + y^2) = ax^2 + ay^2 + xy $$\n$$ ax^2 + 2axy + ay^2 = ax^2 + ay^2 + xy $$\nComparing the terms, we must have $2axy = xy$, which implies $2a = 1$, or $a = \\frac{1}{2}$.\n\nThus, $p(x) = \\frac{1}{2}x^2$ satisfies the non-homogeneous part of the recurrence. We can now express the general function $f(x)$ as the sum of this particular solution and a homogeneous part. Let:\n$$ f(x) = \\frac{1}{2}x^2 + g(x) $$\nwhere $g: \\mathbb{R} \\to \\mathbb{R}$ is a new function to be determined.\n\n### Step 2: Determine the Equation for $g(x)$\nSubstitute $f(x) = \\frac{1}{2}x^2 + g(x)$ into the original functional equation:\n$$ \\frac{1}{2}(x+y)^2 + g(x+y) = \\left(\\frac{1}{2}x^2 + g(x)\\right) + \\left(\\frac{1}{2}y^2 + g(y)\\right) + xy $$\nExpand the left side:\n$$ \\frac{1}{2}(x^2 + 2xy + y^2) + g(x+y) = \\frac{1}{2}x^2 + g(x) + \\frac{1}{2}y^2 + g(y) + xy $$\n$$ \\frac{1}{2}x^2 + xy + \\frac{1}{2}y^2 + g(x+y) = \\frac{1}{2}x^2 + \\frac{1}{2}y^2 + xy + g(x) + g(y) $$\nSubtracting $\\frac{1}{2}x^2 + \\frac{1}{2}y^2 + xy$ from both sides yields:\n$$ g(x+y) = g(x) + g(y) $$\nThis is the **Cauchy Functional Equation**.\n\n### Step 3: Apply the Initial Condition\nWe are given $f(1) = 1$. Using the definition of $g(x)$:\n$$ f(1) = \\frac{1}{2}(1)^2 + g(1) = 1 $$\n$$ \\frac{1}{2} + g(1) = 1 \\implies g(1) = \\frac{1}{2} $$\n\n### Step 4: Solve the Cauchy Functional Equation\nThe function $g: \\mathbb{R} \\to \\mathbb{R}$ must satisfy:\n1. $g(x+y) = g(x) + g(y)$ for all $x, y \\in \\mathbb{R}$\n2. $g(1) = \\frac{1}{2}$\n\nFrom the additivity property $g(x+y) = g(x) + g(y)$, we can prove by induction that $g(nx) = n g(x)$ for all integers $n$. Furthermore, for any rational number $q = \\frac{m}{n}$ (where $m, n \\in \\mathbb{Z}, n \\neq 0$), we have $g(q) = q g(1)$.\n\nProof for rationals:\nSince $g(1) = \\frac{1}{2}$, for any integer $n$:\n$$ g(n) = g(n \\cdot 1) = n g(1) = \\frac{n}{2} $$\nFor any rational $x = \\frac{m}{n}$:\n$$ g\\left(n \\cdot \\frac{m}{n}\\right) = n g\\left(\\frac{m}{n}\\right) $$\n$$ g(m) = n g\\left(\\frac{m}{n}\\right) $$\n$$ \\frac{m}{2} = n g\\left(\\frac{m}{n}\\right) \\implies g\\left(\\frac{m}{n}\\right) = \\frac{1}{n} \\cdot \\frac{m}{2} = \\frac{1}{2} \\cdot \\frac{m}{n} $$\nThus, for all $x \\in \\mathbb{Q}$, $g(x) = \\frac{1}{2}x$.\n\nHowever, for $x \\notin \\mathbb{Q}$, $g(x)$ is not uniquely determined by the value at $1$ without additional assumptions (such as continuity, monotonicity, or boundedness). The general solution to the Cauchy Functional Equation is any additive function $A: \\mathbb{R} \\to \\mathbb{R}$.\n\nLet $A(x)$ be any additive function (i.e., $A(x+y)=A(x)+A(y)$). The condition $g(1) = 1/2$ restricts $A$ such that $A(1) = 1/2$.\nWe can write this as $g(x) = A(x)$, where $A(1) = 1/2$. Alternatively, we can define $H(x) = A(x) - \\frac{1}{2}x$. Then $H$ is an additive function with $H(1) = A(1) - \\frac{1}{2} = 0$. Since $H(1)=0$ and $H$ is additive, $H(q) = 0$ for all $q \\in \\mathbb{Q}$.\nThus, the most general form for $g(x)$ is:\n$$ g(x) = \\frac{1}{2}x + H(x) $$\nwhere $H: \\mathbb{R} \\to \\mathbb{R}$ is an additive function such that $H(x) = 0$ for all $x \\in \\mathbb{Q}$.\n\n### Step 5: Construct the Final Solution\nSubstituting $g(x)$ back into the expression for $f(x)$:\n$$ f(x) = \\frac{1}{2}x^2 + g(x) $$\n$$ f(x) = \\frac{1}{2}x^2 + \\frac{1}{2}x + H(x) $$\n$$ f(x) = \\frac{x(x+1)}{2} + H(x) $$\n\n**Verification:**\nLHS: $f(x+y) = \\frac{(x+y)^2 + (x+y)}{2} + H(x+y)$\nRHS: $f(x) + f(y) + xy = \\left(\\frac{x^2+x}{2} + H(x)\\right) + \\left(\\frac{y^2+y}{2} + H(y)\\right) + xy$\n\nExpanding terms:\nLHS: $\\frac{x^2 + 2xy + y^2 + x + y}{2} + H(x) + H(y)$ (since H is additive)\nRHS: $\\frac{x^2 + x + y^2 + y + 2xy}{2} + H(x) + H(y)$\nThe sides are identical.\nCondition $f(1) = \\frac{1(2)}{2} + H(1) = 1 + 0 = 1$. The condition holds.\n\n### Conclusion\n\nThe set of all solutions depends on whether any regularity conditions (like continuity) are assumed. Since the problem statement does not impose such conditions, we provide the general solution.\n\nThe complete set of functions $f: \\mathbb{R} \\to \\mathbb{R}$ satisfying the conditions is given by:\n$$ f(x) = \\frac{x(x+1)}{2} + H(x) $$\nwhere $H: \\mathbb{R} \\to \\mathbb{R}$ is any additive function (satisfying $H(x+y) = H(x) + H(y)$) that vanishes on all rational numbers (i.e., $H(q) = 0$ for all $q \\in \\mathbb{Q}$).\n\n*Note: If $f$ is assumed to be continuous, then $H(x)$ must be identically zero, and the unique solution is $f(x) = \\frac{x(x+1)}{2}$.*",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ],
    "outer_attempt": 1
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "Evaluate the infinite series $\\sum_{n=1}^{\\infty} \\frac{(-1)^{n+1}}{n(n+1)}$.",
    "run_id": "26",
    "topic_slug": "sequences_series_and_summability",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "To evaluate the infinite series $\\sum_{n=1}^{\\infty} \\frac{(-1)^{n+1}}{n(n+1)}$, we will proceed by decomposing the general term into partial fractions, expressing the series in terms of partial sums, and utilizing the known series expansion for $\\ln(2)$.\n\n### 1. Partial Fraction Decomposition\n\nFirst, we analyze the term $a_n = \\frac{(-1)^{n+1}}{n(n+1)}$. We can decompose the rational part $\\frac{1}{n(n+1)}$ as follows:\n$$ \\frac{1}{n(n+1)} = \\frac{1}{n} - \\frac{1}{n+1} $$\nThus, the general term of the series becomes:\n$$ a_n = (-1)^{n+1} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\frac{(-1)^{n+1}}{n} - \\frac{(-1)^{n+1}}{n+1} $$\nNoting that $-(-1)^{n+1} = (-1)^{n+2}$, we can rewrite this as:\n$$ a_n = \\frac{(-1)^{n+1}}{n} + \\frac{(-1)^{n+2}}{n+1} $$\n\n### 2. Analysis of Partial Sums\n\nLet $S_N$ be the partial sum of the first $N$ terms. By the linearity of finite summation, we can split $S_N$ into two separate sums:\n$$ S_N = \\sum_{n=1}^{N} \\left( \\frac{(-1)^{n+1}}{n} + \\frac{(-1)^{n+2}}{n+1} \\right) = \\sum_{n=1}^{N} \\frac{(-1)^{n+1}}{n} + \\sum_{n=1}^{N} \\frac{(-1)^{n+2}}{n+1} $$\n\nWe will shift the index of the second sum to align the denominators with the first sum. Let $k = n + 1$. When $n=1$, $k=2$, and when $n=N$, $k=N+1$.\n$$ \\sum_{n=1}^{N} \\frac{(-1)^{n+2}}{n+1} = \\sum_{k=2}^{N+1} \\frac{(-1)^{k+1}}{k} $$\n\nNow, substitute this back into the expression for $S_N$:\n$$ S_N = \\sum_{n=1}^{N} \\frac{(-1)^{n+1}}{n} + \\sum_{k=2}^{N+1} \\frac{(-1)^{k+1}}{k} $$\n\nWe can isolate the first term of the first sum ($n=1$) and the last term of the second sum ($k=N+1$) to align the ranges of summation from $2$ to $N$:\n1. The first sum is: $1 + \\sum_{n=2}^{N} \\frac{(-1)^{n+1}}{n}$\n2. The second sum is: $\\sum_{k=2}^{N} \\frac{(-1)^{k+1}}{k} + \\frac{(-1)^{N+2}}{N+1}$\n\nCombining these (and noting that the summation index variable is a dummy variable, so we can set $k=n$):\n$$ S_N = 1 + \\sum_{n=2}^{N} \\frac{(-1)^{n+1}}{n} + \\sum_{n=2}^{N} \\frac{(-1)^{n+1}}{n} + \\frac{(-1)^{N+2}}{N+1} $$\n$$ S_N = 1 + 2\\sum_{n=2}^{N} \\frac{(-1)^{n+1}}{n} + \\frac{(-1)^{N}}{N+1} $$\n*(Note: $(-1)^{N+2} = (-1)^N$)*\n\n### 3. Evaluating the Limit\n\nWe now take the limit as $N \\to \\infty$.\n$$ S = \\lim_{N \\to \\infty} S_N = 1 + 2 \\lim_{N \\to \\infty} \\sum_{n=2}^{N} \\frac{(-1)^{n+1}}{n} + \\lim_{N \\to \\infty} \\frac{(-1)^{N}}{N+1} $$\n\nThe last term clearly approaches 0:\n$$ \\lim_{N \\to \\infty} \\frac{(-1)^{N}}{N+1} = 0 $$\n\nTo evaluate the infinite sum, recall the Mercator series for $\\ln(1+x)$ evaluated at $x=1$:\n$$ \\ln(2) = \\sum_{n=1}^{\\infty} \\frac{(-1)^{n+1}}{n} = 1 - \\frac{1}{2} + \\frac{1}{3} - \\frac{1}{4} + \\dots $$\nWe can isolate the first term ($n=1$) from this series:\n$$ \\ln(2) = 1 + \\sum_{n=2}^{\\infty} \\frac{(-1)^{n+1}}{n} $$\nTherefore:\n$$ \\sum_{n=2}^{\\infty} \\frac{(-1)^{n+1}}{n} = \\ln(2) - 1 $$\n\nSubstituting this result back into our expression for the total sum $S$:\n$$ S = 1 + 2(\\ln(2) - 1) + 0 $$\n$$ S = 1 + 2\\ln(2) - 2 $$\n$$ S = 2\\ln(2) - 1 $$\n\n### Conclusion\n\nThe value of the infinite series is:\n$$ \\sum_{n=1}^{\\infty} \\frac{(-1)^{n+1}}{n(n+1)} = 2\\ln(2) - 1 $$",
        "raw_answer": "To evaluate the infinite series $\\sum_{n=1}^{\\infty} \\frac{(-1)^{n+1}}{n(n+1)}$, we will proceed by decomposing the general term into partial fractions, expressing the series in terms of partial sums, and utilizing the known series expansion for $\\ln(2)$.\n\n### 1. Partial Fraction Decomposition\n\nFirst, we analyze the term $a_n = \\frac{(-1)^{n+1}}{n(n+1)}$. We can decompose the rational part $\\frac{1}{n(n+1)}$ as follows:\n$$ \\frac{1}{n(n+1)} = \\frac{1}{n} - \\frac{1}{n+1} $$\nThus, the general term of the series becomes:\n$$ a_n = (-1)^{n+1} \\left( \\frac{1}{n} - \\frac{1}{n+1} \\right) = \\frac{(-1)^{n+1}}{n} - \\frac{(-1)^{n+1}}{n+1} $$\nNoting that $-(-1)^{n+1} = (-1)^{n+2}$, we can rewrite this as:\n$$ a_n = \\frac{(-1)^{n+1}}{n} + \\frac{(-1)^{n+2}}{n+1} $$\n\n### 2. Analysis of Partial Sums\n\nLet $S_N$ be the partial sum of the first $N$ terms. By the linearity of finite summation, we can split $S_N$ into two separate sums:\n$$ S_N = \\sum_{n=1}^{N} \\left( \\frac{(-1)^{n+1}}{n} + \\frac{(-1)^{n+2}}{n+1} \\right) = \\sum_{n=1}^{N} \\frac{(-1)^{n+1}}{n} + \\sum_{n=1}^{N} \\frac{(-1)^{n+2}}{n+1} $$\n\nWe will shift the index of the second sum to align the denominators with the first sum. Let $k = n + 1$. When $n=1$, $k=2$, and when $n=N$, $k=N+1$.\n$$ \\sum_{n=1}^{N} \\frac{(-1)^{n+2}}{n+1} = \\sum_{k=2}^{N+1} \\frac{(-1)^{k+1}}{k} $$\n\nNow, substitute this back into the expression for $S_N$:\n$$ S_N = \\sum_{n=1}^{N} \\frac{(-1)^{n+1}}{n} + \\sum_{k=2}^{N+1} \\frac{(-1)^{k+1}}{k} $$\n\nWe can isolate the first term of the first sum ($n=1$) and the last term of the second sum ($k=N+1$) to align the ranges of summation from $2$ to $N$:\n1. The first sum is: $1 + \\sum_{n=2}^{N} \\frac{(-1)^{n+1}}{n}$\n2. The second sum is: $\\sum_{k=2}^{N} \\frac{(-1)^{k+1}}{k} + \\frac{(-1)^{N+2}}{N+1}$\n\nCombining these (and noting that the summation index variable is a dummy variable, so we can set $k=n$):\n$$ S_N = 1 + \\sum_{n=2}^{N} \\frac{(-1)^{n+1}}{n} + \\sum_{n=2}^{N} \\frac{(-1)^{n+1}}{n} + \\frac{(-1)^{N+2}}{N+1} $$\n$$ S_N = 1 + 2\\sum_{n=2}^{N} \\frac{(-1)^{n+1}}{n} + \\frac{(-1)^{N}}{N+1} $$\n*(Note: $(-1)^{N+2} = (-1)^N$)*\n\n### 3. Evaluating the Limit\n\nWe now take the limit as $N \\to \\infty$.\n$$ S = \\lim_{N \\to \\infty} S_N = 1 + 2 \\lim_{N \\to \\infty} \\sum_{n=2}^{N} \\frac{(-1)^{n+1}}{n} + \\lim_{N \\to \\infty} \\frac{(-1)^{N}}{N+1} $$\n\nThe last term clearly approaches 0:\n$$ \\lim_{N \\to \\infty} \\frac{(-1)^{N}}{N+1} = 0 $$\n\nTo evaluate the infinite sum, recall the Mercator series for $\\ln(1+x)$ evaluated at $x=1$:\n$$ \\ln(2) = \\sum_{n=1}^{\\infty} \\frac{(-1)^{n+1}}{n} = 1 - \\frac{1}{2} + \\frac{1}{3} - \\frac{1}{4} + \\dots $$\nWe can isolate the first term ($n=1$) from this series:\n$$ \\ln(2) = 1 + \\sum_{n=2}^{\\infty} \\frac{(-1)^{n+1}}{n} $$\nTherefore:\n$$ \\sum_{n=2}^{\\infty} \\frac{(-1)^{n+1}}{n} = \\ln(2) - 1 $$\n\nSubstituting this result back into our expression for the total sum $S$:\n$$ S = 1 + 2(\\ln(2) - 1) + 0 $$\n$$ S = 1 + 2\\ln(2) - 2 $$\n$$ S = 2\\ln(2) - 1 $$\n\n### Conclusion\n\nThe value of the infinite series is:\n$$ \\sum_{n=1}^{\\infty} \\frac{(-1)^{n+1}}{n(n+1)} = 2\\ln(2) - 1 $$",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ],
    "outer_attempt": 1
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "## Step 1: Understand the task and the topic\nThe task is to generate a challenging mathematics problem along with a complete and verifiable solution in the domain of \"Approximations and expansions.\" The problem should be well-posed, clear, non-trivial, and solvable with established mathematical methods.\n\n## Step 2: Identify a suitable topic within \"Approximations and expansions\"\nA suitable topic could involve Taylor series expansions, asymptotic approximations, or other related areas where mathematical functions are approximated.\n\n## Step 3: Formulate a challenging problem\nLet's consider a problem involving the approximation of a function using its Taylor series expansion. A classic example is approximating $e^x$ or $\\sin(x)$ around $x = 0$. However, to make it more challenging, we could involve a less straightforward function or require the derivation of an approximation under certain conditions.\n\n## Step 4: Create a specific problem\nConsider the function $f(x) = \\frac{1}{\\sqrt{1 + x}}$. We can ask to find the Taylor series expansion of $f(x)$ around $x = 0$ up to the third term and then use this expansion to approximate $f(0.1)$.\n\n## Step 5: Formulate the problem statement\nLet's refine the problem: \"Find the Taylor series expansion of $f(x) = \\frac{1}{\\sqrt{1 + x}}$ around $x = 0$ up to and including the term involving $x^3$. Then, use this expansion to approximate $f(0.1)$ and compare it with the exact value.\"\n\n## Step 6: Write the problem in the required format\n\nFind the Taylor series expansion of $f(x) = \\frac{1}{\\sqrt{1 + x}}$ around $x = 0$ up to and including the term involving $x^3$. Then, use this expansion to approximate $f(0.1)$ and compare it with the exact value.",
    "run_id": "27",
    "topic_slug": "approximations_and_expansions",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "Here is the complete solution to the problem presented in Step 6.\n\n### Problem Statement\n\n**Find the Taylor series expansion of $f(x) = \\frac{1}{\\sqrt{1 + x}}$ around $x = 0$ up to and including the term involving $x^3$. Then, use this expansion to approximate $f(0.1)$ and compare it with the exact value.**\n\n---\n\n### Solution\n\n#### Part 1: Taylor Series Expansion\n\nWe are asked to find the Taylor series expansion of the function $f(x) = (1+x)^{-\\frac{1}{2}}$ centered at $x=0$ (also known as the Maclaurin series). The general formula for the Taylor series of $f(x)$ centered at $x=0$ is:\n$$ f(x) = \\sum_{n=0}^{\\infty} \\frac{f^{(n)}(0)}{n!} x^n = f(0) + f'(0)x + \\frac{f''(0)}{2!}x^2 + \\frac{f'''(0)}{3!}x^3 + \\dots $$\n\nWe need to determine the terms up to $n=3$. We calculate the derivatives of $f(x)$ and evaluate them at $x=0$.\n\n**1. Zeroth derivative ($n=0$):**\n$$ f(x) = (1+x)^{-\\frac{1}{2}} $$\n$$ f(0) = (1+0)^{-\\frac{1}{2}} = 1 $$\n\n**2. First derivative ($n=1$):**\nUsing the power rule $\\frac{d}{dx} u^k = k u^{k-1} u'$:\n$$ f'(x) = -\\frac{1}{2}(1+x)^{-\\frac{3}{2}} $$\n$$ f'(0) = -\\frac{1}{2}(1)^{-\\frac{3}{2}} = -\\frac{1}{2} $$\n\n**3. Second derivative ($n=2$):**\n$$ f''(x) = \\frac{d}{dx} \\left[ -\\frac{1}{2}(1+x)^{-\\frac{3}{2}} \\right] = \\left(-\\frac{1}{2}\\right)\\left(-\\frac{3}{2}\\right)(1+x)^{-\\frac{5}{2}} = \\frac{3}{4}(1+x)^{-\\frac{5}{2}} $$\n$$ f''(0) = \\frac{3}{4}(1)^{-\\frac{5}{2}} = \\frac{3}{4} $$\n\n**4. Third derivative ($n=3$):**\n$$ f'''(x) = \\frac{d}{dx} \\left[ \\frac{3}{4}(1+x)^{-\\frac{5}{2}} \\right] = \\left(\\frac{3}{4}\\right)\\left(-\\frac{5}{2}\\right)(1+x)^{-\\frac{7}{2}} = -\\frac{15}{8}(1+x)^{-\\frac{7}{2}} $$\n$$ f'''(0) = -\\frac{15}{8}(1)^{-\\frac{7}{2}} = -\\frac{15}{8} $$\n\n**Constructing the Polynomial:**\nNow substitute these values into the Taylor series formula:\n$$ P_3(x) = f(0) + f'(0)x + \\frac{f''(0)}{2!}x^2 + \\frac{f'''(0)}{3!}x^3 $$\n$$ P_3(x) = 1 + \\left(-\\frac{1}{2}\\right)x + \\frac{3/4}{2}x^2 + \\frac{-15/8}{6}x^3 $$\n\nSimplify the coefficients:\n- Coefficient of $x^2$: $\\frac{3/4}{2} = \\frac{3}{8}$\n- Coefficient of $x^3$: $\\frac{-15/8}{6} = -\\frac{15}{48} = -\\frac{5}{16}$\n\nThus, the Taylor expansion up to the term involving $x^3$ is:\n$$ P_3(x) = 1 - \\frac{1}{2}x + \\frac{3}{8}x^2 - \\frac{5}{16}x^3 $$\n\n*(Note: This result can also be verified directly using the generalized Binomial Theorem for $(1+x)^\\alpha$ with $\\alpha = -1/2$.)*\n\n#### Part 2: Approximation of $f(0.1)$\n\nWe approximate $f(0.1)$ by evaluating the polynomial $P_3(x)$ at $x = 0.1$.\n$$ P_3(0.1) = 1 - \\frac{1}{2}(0.1) + \\frac{3}{8}(0.1)^2 - \\frac{5}{16}(0.1)^3 $$\n\nLet's convert the fractions to decimals for easier calculation:\n- $\\frac{1}{2} = 0.5$\n- $\\frac{3}{8} = 0.375$\n- $\\frac{5}{16} = 0.3125$\n\nSubstitute $x = 0.1$, $x^2 = 0.01$, and $x^3 = 0.001$:\n$$ P_3(0.1) = 1 - 0.5(0.1) + 0.375(0.01) - 0.3125(0.001) $$\n$$ P_3(0.1) = 1 - 0.05 + 0.00375 - 0.0003125 $$\n\nCalculate step-by-step:\n1. $1 - 0.05 = 0.95$\n2. $0.95 + 0.00375 = 0.95375$\n3. $0.95375 - 0.0003125 = 0.9534375$\n\nSo, the approximate value is **0.9534375**.\n\n#### Part 3: Comparison with Exact Value\n\nThe exact value is:\n$$ f(0.1) = \\frac{1}{\\sqrt{1 + 0.1}} = \\frac{1}{\\sqrt{1.1}} $$\n\nTo compare, we compute the decimal value of $\\frac{1}{\\sqrt{1.1}}$.\n$$ \\sqrt{1.1} \\approx 1.04880884817 $$\n$$ f(0.1) \\approx \\frac{1}{1.04880884817} \\approx 0.95346258925 $$\n\n**Comparison:**\n- **Approximation ($P_3(0.1)$):** $0.9534375$\n- **Exact Value ($f(0.1)$):** $\\approx 0.9534626$\n\nWe calculate the absolute error:\n$$ \\text{Error} = |f(0.1) - P_3(0.1)| \\approx |0.953462589 - 0.9534375| $$\n$$ \\text{Error} \\approx 0.000025089 $$\n\nThe approximation is accurate to four decimal places.\n\n### Final Answer\n\nThe Taylor series expansion of $f(x) = \\frac{1}{\\sqrt{1 + x}}$ around $x = 0$ up to the term involving $x^3$ is:\n$$ P_3(x) = 1 - \\frac{1}{2}x + \\frac{3}{8}x^2 - \\frac{5}{16}x^3 $$\nUsing this expansion, the approximate value of $f(0.1)$ is **0.9534375**. The exact value is approximately **0.9534626**, resulting in an absolute error of approximately **0.000025**.",
        "raw_answer": "Here is the complete solution to the problem presented in Step 6.\n\n### Problem Statement\n\n**Find the Taylor series expansion of $f(x) = \\frac{1}{\\sqrt{1 + x}}$ around $x = 0$ up to and including the term involving $x^3$. Then, use this expansion to approximate $f(0.1)$ and compare it with the exact value.**\n\n---\n\n### Solution\n\n#### Part 1: Taylor Series Expansion\n\nWe are asked to find the Taylor series expansion of the function $f(x) = (1+x)^{-\\frac{1}{2}}$ centered at $x=0$ (also known as the Maclaurin series). The general formula for the Taylor series of $f(x)$ centered at $x=0$ is:\n$$ f(x) = \\sum_{n=0}^{\\infty} \\frac{f^{(n)}(0)}{n!} x^n = f(0) + f'(0)x + \\frac{f''(0)}{2!}x^2 + \\frac{f'''(0)}{3!}x^3 + \\dots $$\n\nWe need to determine the terms up to $n=3$. We calculate the derivatives of $f(x)$ and evaluate them at $x=0$.\n\n**1. Zeroth derivative ($n=0$):**\n$$ f(x) = (1+x)^{-\\frac{1}{2}} $$\n$$ f(0) = (1+0)^{-\\frac{1}{2}} = 1 $$\n\n**2. First derivative ($n=1$):**\nUsing the power rule $\\frac{d}{dx} u^k = k u^{k-1} u'$:\n$$ f'(x) = -\\frac{1}{2}(1+x)^{-\\frac{3}{2}} $$\n$$ f'(0) = -\\frac{1}{2}(1)^{-\\frac{3}{2}} = -\\frac{1}{2} $$\n\n**3. Second derivative ($n=2$):**\n$$ f''(x) = \\frac{d}{dx} \\left[ -\\frac{1}{2}(1+x)^{-\\frac{3}{2}} \\right] = \\left(-\\frac{1}{2}\\right)\\left(-\\frac{3}{2}\\right)(1+x)^{-\\frac{5}{2}} = \\frac{3}{4}(1+x)^{-\\frac{5}{2}} $$\n$$ f''(0) = \\frac{3}{4}(1)^{-\\frac{5}{2}} = \\frac{3}{4} $$\n\n**4. Third derivative ($n=3$):**\n$$ f'''(x) = \\frac{d}{dx} \\left[ \\frac{3}{4}(1+x)^{-\\frac{5}{2}} \\right] = \\left(\\frac{3}{4}\\right)\\left(-\\frac{5}{2}\\right)(1+x)^{-\\frac{7}{2}} = -\\frac{15}{8}(1+x)^{-\\frac{7}{2}} $$\n$$ f'''(0) = -\\frac{15}{8}(1)^{-\\frac{7}{2}} = -\\frac{15}{8} $$\n\n**Constructing the Polynomial:**\nNow substitute these values into the Taylor series formula:\n$$ P_3(x) = f(0) + f'(0)x + \\frac{f''(0)}{2!}x^2 + \\frac{f'''(0)}{3!}x^3 $$\n$$ P_3(x) = 1 + \\left(-\\frac{1}{2}\\right)x + \\frac{3/4}{2}x^2 + \\frac{-15/8}{6}x^3 $$\n\nSimplify the coefficients:\n- Coefficient of $x^2$: $\\frac{3/4}{2} = \\frac{3}{8}$\n- Coefficient of $x^3$: $\\frac{-15/8}{6} = -\\frac{15}{48} = -\\frac{5}{16}$\n\nThus, the Taylor expansion up to the term involving $x^3$ is:\n$$ P_3(x) = 1 - \\frac{1}{2}x + \\frac{3}{8}x^2 - \\frac{5}{16}x^3 $$\n\n*(Note: This result can also be verified directly using the generalized Binomial Theorem for $(1+x)^\\alpha$ with $\\alpha = -1/2$.)*\n\n#### Part 2: Approximation of $f(0.1)$\n\nWe approximate $f(0.1)$ by evaluating the polynomial $P_3(x)$ at $x = 0.1$.\n$$ P_3(0.1) = 1 - \\frac{1}{2}(0.1) + \\frac{3}{8}(0.1)^2 - \\frac{5}{16}(0.1)^3 $$\n\nLet's convert the fractions to decimals for easier calculation:\n- $\\frac{1}{2} = 0.5$\n- $\\frac{3}{8} = 0.375$\n- $\\frac{5}{16} = 0.3125$\n\nSubstitute $x = 0.1$, $x^2 = 0.01$, and $x^3 = 0.001$:\n$$ P_3(0.1) = 1 - 0.5(0.1) + 0.375(0.01) - 0.3125(0.001) $$\n$$ P_3(0.1) = 1 - 0.05 + 0.00375 - 0.0003125 $$\n\nCalculate step-by-step:\n1. $1 - 0.05 = 0.95$\n2. $0.95 + 0.00375 = 0.95375$\n3. $0.95375 - 0.0003125 = 0.9534375$\n\nSo, the approximate value is **0.9534375**.\n\n#### Part 3: Comparison with Exact Value\n\nThe exact value is:\n$$ f(0.1) = \\frac{1}{\\sqrt{1 + 0.1}} = \\frac{1}{\\sqrt{1.1}} $$\n\nTo compare, we compute the decimal value of $\\frac{1}{\\sqrt{1.1}}$.\n$$ \\sqrt{1.1} \\approx 1.04880884817 $$\n$$ f(0.1) \\approx \\frac{1}{1.04880884817} \\approx 0.95346258925 $$\n\n**Comparison:**\n- **Approximation ($P_3(0.1)$):** $0.9534375$\n- **Exact Value ($f(0.1)$):** $\\approx 0.9534626$\n\nWe calculate the absolute error:\n$$ \\text{Error} = |f(0.1) - P_3(0.1)| \\approx |0.953462589 - 0.9534375| $$\n$$ \\text{Error} \\approx 0.000025089 $$\n\nThe approximation is accurate to four decimal places.\n\n### Final Answer\n\nThe Taylor series expansion of $f(x) = \\frac{1}{\\sqrt{1 + x}}$ around $x = 0$ up to the term involving $x^3$ is:\n$$ P_3(x) = 1 - \\frac{1}{2}x + \\frac{3}{8}x^2 - \\frac{5}{16}x^3 $$\nUsing this expansion, the approximate value of $f(0.1)$ is **0.9534375**. The exact value is approximately **0.9534626**, resulting in an absolute error of approximately **0.000025**.",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ],
    "outer_attempt": 1
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "Let $f \\in L^1(\\mathbb{R}^n) \\cap L^2(\\mathbb{R}^n)$ and suppose that its Fourier transform $\\hat{f}$ is supported in the ball $B(0,1) = \\{ \\xi \\in \\mathbb{R}^n : |\\xi| \\leq 1 \\}$. Prove that $f$ is smooth, i.e., $f \\in C^\\infty(\\mathbb{R}^n)$, and that for any multi-index $\\alpha$, there exists a constant $C_\\alpha > 0$ such that $|\\partial^\\alpha f(x)| \\leq C_\\alpha (1 + |x|)^{-n-1}$ for all $x \\in \\mathbb{R}^n$.",
    "run_id": "28",
    "topic_slug": "harmonic_analysis_on_euclidean_spaces",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "The problem asks us to prove two properties for a function $f \\in L^1(\\mathbb{R}^n) \\cap L^2(\\mathbb{R}^n)$ whose Fourier transform $\\hat{f}$ is supported in the unit ball $B(0,1)$. The two properties are:\n1.  $f$ is smooth, i.e., $f \\in C^\\infty(\\mathbb{R}^n)$.\n2.  $f$ and its derivatives satisfy the decay estimate $|\\partial^\\alpha f(x)| \\leq C_\\alpha (1 + |x|)^{-n-1}$.\n\nWhile the first assertion (smoothness) is true and can be proven rigorously, the second assertion (decay rate) is **false** as stated. There exist functions satisfying the given hypotheses that do not decay as fast as $(1+|x|)^{-n-1}$.\n\nBelow, we provide the proof of smoothness and then demonstrate via counterexample why the decay estimate does not hold in general.\n\n### Part 1: Proof of Smoothness\n\nWe are given that $f \\in L^1(\\mathbb{R}^n)$. By the Riemann-Lebesgue lemma, the Fourier transform $\\hat{f}$ is continuous and bounded. We are also given that $\\text{supp}(\\hat{f}) \\subseteq B(0,1)$.\nSince $f \\in L^1(\\mathbb{R}^n) \\cap L^2(\\mathbb{R}^n)$, we can recover $f$ from $\\hat{f}$ via the inverse Fourier transform (which holds pointwise or almost everywhere; since $\\hat{f} \\in L^1$, it holds pointwise and $f$ is continuous):\n$$ f(x) = \\int_{\\mathbb{R}^n} \\hat{f}(\\xi) e^{2\\pi i x \\cdot \\xi} \\, d\\xi $$\nUsing the support condition, the integral is restricted to the unit ball:\n$$ f(x) = \\int_{B(0,1)} \\hat{f}(\\xi) e^{2\\pi i x \\cdot \\xi} \\, d\\xi $$\nTo prove that $f \\in C^\\infty(\\mathbb{R}^n)$, consider the partial derivative with respect to $x$ corresponding to a multi-index $\\alpha$. Formally differentiating under the integral sign gives the integrand:\n$$ g_\\alpha(x, \\xi) = (2\\pi i \\xi)^\\alpha \\hat{f}(\\xi) e^{2\\pi i x \\cdot \\xi} $$\nWe must verify that this differentiation is valid.\n1.  For any fixed $x$, the function $\\xi \\mapsto g_\\alpha(x, \\xi)$ is continuous on $B(0,1)$ (since $\\hat{f}$ is continuous) and zero outside. Thus, it is integrable.\n2.  We can bound the integrand uniformly in $x$:\n    $$ |(2\\pi i \\xi)^\\alpha \\hat{f}(\\xi) e^{2\\pi i x \\cdot \\xi}| \\leq (2\\pi)^ {|\\alpha|} |\\xi|^{|\\alpha|} |\\hat{f}(\\xi)| $$\n    Since $|\\xi| \\leq 1$ on the support of $\\hat{f}$, and $\\hat{f}$ is bounded (let $\\|\\hat{f}\\|_\\infty = M$), we have:\n    $$ |(2\\pi i \\xi)^\\alpha \\hat{f}(\\xi) e^{2\\pi i x \\cdot \\xi}| \\leq (2\\pi)^{|\\alpha|} M \\chi_{B(0,1)}(\\xi) $$\n    The dominating function $G(\\xi) = (2\\pi)^{|\\alpha|} M \\chi_{B(0,1)}(\\xi)$ is in $L^1(\\mathbb{R}^n)$ because the volume of the ball is finite.\n\nBy the standard theorem on differentiation under the integral sign (a consequence of the Dominated Convergence Theorem), $f$ is infinitely differentiable, and\n$$ \\partial^\\alpha f(x) = \\int_{B(0,1)} (2\\pi i \\xi)^\\alpha \\hat{f}(\\xi) e^{2\\pi i x \\cdot \\xi} \\, d\\xi $$\nThus, $f \\in C^\\infty(\\mathbb{R}^n)$. Furthermore, by the bound on the integrand, $\\partial^\\alpha f$ is bounded on $\\mathbb{R}^n$.\n\n---\n\n### Part 2: Analysis of the Decay Estimate\n\nThe problem asserts that $|\\partial^\\alpha f(x)| \\leq C_\\alpha (1 + |x|)^{-n-1}$. This implies that $f(x)$ must decay at least as fast as $|x|^{-n-1}$ as $|x| \\to \\infty$. We will show this is false by constructing a counterexample.\n\nConsider the family of radial functions defined by their Fourier transforms. Let $\\delta > 0$ and define $\\hat{f}_\\delta$ on $\\mathbb{R}^n$ by:\n$$ \\hat{f}_\\delta(\\xi) = \\begin{cases} (1 - |\\xi|^2)^\\delta & \\text{if } |\\xi| \\leq 1 \\\\ 0 & \\text{if } |\\xi| > 1 \\end{cases} $$\nThis function is supported in $B(0,1)$. Since $\\delta > 0$, $\\hat{f}_\\delta$ is continuous everywhere (vanishing at the boundary $|\\xi|=1$).\n\nThe inverse Fourier transform $f_\\delta(x)$ is related to the Bessel function of the first kind. Specifically, for $x \\in \\mathbb{R}^n$:\n$$ f_\\delta(x) = \\frac{\\Gamma(\\delta+1)}{\\pi^\\delta} |x|^{-(n/2 + \\delta)} J_{n/2 + \\delta}(2\\pi |x|) $$\nUsing the asymptotic expansion of the Bessel function for large argument $r$, $J_\\nu(r) \\approx \\sqrt{\\frac{2}{\\pi r}} \\cos(r - \\dots)$, the decay of $f_\\delta(x)$ as $|x| \\to \\infty$ is:\n$$ |f_\\delta(x)| \\sim C |x|^{-(n/2 + \\delta)} |x|^{-1/2} = C |x|^{-\\frac{n+1}{2} - \\delta} $$\nLet $\\gamma = \\frac{n+1}{2} + \\delta$. So $|f_\\delta(x)| \\sim |x|^{-\\gamma}$.\n\n**Condition 1: Integrability ($f \\in L^1$)**\nFor $f_\\delta$ to be in $L^1(\\mathbb{R}^n)$, we require the integral of $|x|^{-\\gamma}$ to converge at infinity (it is bounded near 0). Using polar coordinates, we need:\n$$ \\int_1^\\infty r^{-\\gamma} r^{n-1} \\, dr < \\infty \\iff -\\gamma + n - 1 < -1 \\iff \\gamma > n $$\nSubstituting $\\gamma$:\n$$ \\frac{n+1}{2} + \\delta > n \\iff \\delta > \\frac{n-1}{2} $$\n\n**Condition 2: The Claimed Decay**\nThe problem claims decay of order $|x|^{-n-1}$. This requires:\n$$ \\gamma \\ge n+1 \\iff \\frac{n+1}{2} + \\delta \\ge n+1 \\iff \\delta \\ge \\frac{n+1}{2} $$\n\n**The Contradiction**\nWe observe a gap between the condition for $L^1$ membership and the condition for the claimed decay. If we choose $\\delta$ such that:\n$$ \\frac{n-1}{2} < \\delta < \\frac{n+1}{2} $$\nthen $f_\\delta \\in L^1(\\mathbb{R}^n)$ (and $L^2$, as $\\hat{f}_\\delta \\in L^2$ trivially), and $\\text{supp}(\\hat{f}_\\delta) \\subset B(0,1)$, but $f_\\delta(x)$ decays like $|x|^{-\\frac{n+1}{2}-\\delta}$, which is **slower** than $|x|^{-n-1}$.\n\n**Specific Counterexample in 1D ($n=1$):**\nWe need $0 < \\delta < 1$. Let $\\delta = 0.5$.\nThen $\\hat{f}_{0.5}(\\xi) = (1-\\xi^2)^{0.5} \\chi_{[-1,1]}(\\xi)$.\nThe function $f_{0.5}(x)$ behaves asymptotically as $|x|^{-(1+1)/2 - 0.5} = |x|^{-1.5}$.\nSince $1.5 > 1$, $f_{0.5} \\in L^1(\\mathbb{R})$.\nHowever, the claimed decay is $(1+|x|)^{-2}$.\nSince $|x|^{-1.5}$ decays slower than $|x|^{-2}$, the inequality $|\\partial^\\alpha f(x)| \\leq C (1+|x|)^{-2}$ fails for $\\alpha=0$ and large $x$.\n\n### Conclusion\nThe function $f$ is guaranteed to be smooth ($C^\\infty$), but the decay estimate $|\\partial^\\alpha f(x)| \\leq C_\\alpha (1 + |x|)^{-n-1}$ is **not necessarily true** under the stated assumptions. The estimate holds only if stronger regularity assumptions are imposed on $\\hat{f}$ at the boundary of the unit ball (e.g., if $\\hat{f} \\in C_c^\\infty(B(0,1))$).",
        "raw_answer": "The problem asks us to prove two properties for a function $f \\in L^1(\\mathbb{R}^n) \\cap L^2(\\mathbb{R}^n)$ whose Fourier transform $\\hat{f}$ is supported in the unit ball $B(0,1)$. The two properties are:\n1.  $f$ is smooth, i.e., $f \\in C^\\infty(\\mathbb{R}^n)$.\n2.  $f$ and its derivatives satisfy the decay estimate $|\\partial^\\alpha f(x)| \\leq C_\\alpha (1 + |x|)^{-n-1}$.\n\nWhile the first assertion (smoothness) is true and can be proven rigorously, the second assertion (decay rate) is **false** as stated. There exist functions satisfying the given hypotheses that do not decay as fast as $(1+|x|)^{-n-1}$.\n\nBelow, we provide the proof of smoothness and then demonstrate via counterexample why the decay estimate does not hold in general.\n\n### Part 1: Proof of Smoothness\n\nWe are given that $f \\in L^1(\\mathbb{R}^n)$. By the Riemann-Lebesgue lemma, the Fourier transform $\\hat{f}$ is continuous and bounded. We are also given that $\\text{supp}(\\hat{f}) \\subseteq B(0,1)$.\nSince $f \\in L^1(\\mathbb{R}^n) \\cap L^2(\\mathbb{R}^n)$, we can recover $f$ from $\\hat{f}$ via the inverse Fourier transform (which holds pointwise or almost everywhere; since $\\hat{f} \\in L^1$, it holds pointwise and $f$ is continuous):\n$$ f(x) = \\int_{\\mathbb{R}^n} \\hat{f}(\\xi) e^{2\\pi i x \\cdot \\xi} \\, d\\xi $$\nUsing the support condition, the integral is restricted to the unit ball:\n$$ f(x) = \\int_{B(0,1)} \\hat{f}(\\xi) e^{2\\pi i x \\cdot \\xi} \\, d\\xi $$\nTo prove that $f \\in C^\\infty(\\mathbb{R}^n)$, consider the partial derivative with respect to $x$ corresponding to a multi-index $\\alpha$. Formally differentiating under the integral sign gives the integrand:\n$$ g_\\alpha(x, \\xi) = (2\\pi i \\xi)^\\alpha \\hat{f}(\\xi) e^{2\\pi i x \\cdot \\xi} $$\nWe must verify that this differentiation is valid.\n1.  For any fixed $x$, the function $\\xi \\mapsto g_\\alpha(x, \\xi)$ is continuous on $B(0,1)$ (since $\\hat{f}$ is continuous) and zero outside. Thus, it is integrable.\n2.  We can bound the integrand uniformly in $x$:\n    $$ |(2\\pi i \\xi)^\\alpha \\hat{f}(\\xi) e^{2\\pi i x \\cdot \\xi}| \\leq (2\\pi)^ {|\\alpha|} |\\xi|^{|\\alpha|} |\\hat{f}(\\xi)| $$\n    Since $|\\xi| \\leq 1$ on the support of $\\hat{f}$, and $\\hat{f}$ is bounded (let $\\|\\hat{f}\\|_\\infty = M$), we have:\n    $$ |(2\\pi i \\xi)^\\alpha \\hat{f}(\\xi) e^{2\\pi i x \\cdot \\xi}| \\leq (2\\pi)^{|\\alpha|} M \\chi_{B(0,1)}(\\xi) $$\n    The dominating function $G(\\xi) = (2\\pi)^{|\\alpha|} M \\chi_{B(0,1)}(\\xi)$ is in $L^1(\\mathbb{R}^n)$ because the volume of the ball is finite.\n\nBy the standard theorem on differentiation under the integral sign (a consequence of the Dominated Convergence Theorem), $f$ is infinitely differentiable, and\n$$ \\partial^\\alpha f(x) = \\int_{B(0,1)} (2\\pi i \\xi)^\\alpha \\hat{f}(\\xi) e^{2\\pi i x \\cdot \\xi} \\, d\\xi $$\nThus, $f \\in C^\\infty(\\mathbb{R}^n)$. Furthermore, by the bound on the integrand, $\\partial^\\alpha f$ is bounded on $\\mathbb{R}^n$.\n\n---\n\n### Part 2: Analysis of the Decay Estimate\n\nThe problem asserts that $|\\partial^\\alpha f(x)| \\leq C_\\alpha (1 + |x|)^{-n-1}$. This implies that $f(x)$ must decay at least as fast as $|x|^{-n-1}$ as $|x| \\to \\infty$. We will show this is false by constructing a counterexample.\n\nConsider the family of radial functions defined by their Fourier transforms. Let $\\delta > 0$ and define $\\hat{f}_\\delta$ on $\\mathbb{R}^n$ by:\n$$ \\hat{f}_\\delta(\\xi) = \\begin{cases} (1 - |\\xi|^2)^\\delta & \\text{if } |\\xi| \\leq 1 \\\\ 0 & \\text{if } |\\xi| > 1 \\end{cases} $$\nThis function is supported in $B(0,1)$. Since $\\delta > 0$, $\\hat{f}_\\delta$ is continuous everywhere (vanishing at the boundary $|\\xi|=1$).\n\nThe inverse Fourier transform $f_\\delta(x)$ is related to the Bessel function of the first kind. Specifically, for $x \\in \\mathbb{R}^n$:\n$$ f_\\delta(x) = \\frac{\\Gamma(\\delta+1)}{\\pi^\\delta} |x|^{-(n/2 + \\delta)} J_{n/2 + \\delta}(2\\pi |x|) $$\nUsing the asymptotic expansion of the Bessel function for large argument $r$, $J_\\nu(r) \\approx \\sqrt{\\frac{2}{\\pi r}} \\cos(r - \\dots)$, the decay of $f_\\delta(x)$ as $|x| \\to \\infty$ is:\n$$ |f_\\delta(x)| \\sim C |x|^{-(n/2 + \\delta)} |x|^{-1/2} = C |x|^{-\\frac{n+1}{2} - \\delta} $$\nLet $\\gamma = \\frac{n+1}{2} + \\delta$. So $|f_\\delta(x)| \\sim |x|^{-\\gamma}$.\n\n**Condition 1: Integrability ($f \\in L^1$)**\nFor $f_\\delta$ to be in $L^1(\\mathbb{R}^n)$, we require the integral of $|x|^{-\\gamma}$ to converge at infinity (it is bounded near 0). Using polar coordinates, we need:\n$$ \\int_1^\\infty r^{-\\gamma} r^{n-1} \\, dr < \\infty \\iff -\\gamma + n - 1 < -1 \\iff \\gamma > n $$\nSubstituting $\\gamma$:\n$$ \\frac{n+1}{2} + \\delta > n \\iff \\delta > \\frac{n-1}{2} $$\n\n**Condition 2: The Claimed Decay**\nThe problem claims decay of order $|x|^{-n-1}$. This requires:\n$$ \\gamma \\ge n+1 \\iff \\frac{n+1}{2} + \\delta \\ge n+1 \\iff \\delta \\ge \\frac{n+1}{2} $$\n\n**The Contradiction**\nWe observe a gap between the condition for $L^1$ membership and the condition for the claimed decay. If we choose $\\delta$ such that:\n$$ \\frac{n-1}{2} < \\delta < \\frac{n+1}{2} $$\nthen $f_\\delta \\in L^1(\\mathbb{R}^n)$ (and $L^2$, as $\\hat{f}_\\delta \\in L^2$ trivially), and $\\text{supp}(\\hat{f}_\\delta) \\subset B(0,1)$, but $f_\\delta(x)$ decays like $|x|^{-\\frac{n+1}{2}-\\delta}$, which is **slower** than $|x|^{-n-1}$.\n\n**Specific Counterexample in 1D ($n=1$):**\nWe need $0 < \\delta < 1$. Let $\\delta = 0.5$.\nThen $\\hat{f}_{0.5}(\\xi) = (1-\\xi^2)^{0.5} \\chi_{[-1,1]}(\\xi)$.\nThe function $f_{0.5}(x)$ behaves asymptotically as $|x|^{-(1+1)/2 - 0.5} = |x|^{-1.5}$.\nSince $1.5 > 1$, $f_{0.5} \\in L^1(\\mathbb{R})$.\nHowever, the claimed decay is $(1+|x|)^{-2}$.\nSince $|x|^{-1.5}$ decays slower than $|x|^{-2}$, the inequality $|\\partial^\\alpha f(x)| \\leq C (1+|x|)^{-2}$ fails for $\\alpha=0$ and large $x$.\n\n### Conclusion\nThe function $f$ is guaranteed to be smooth ($C^\\infty$), but the decay estimate $|\\partial^\\alpha f(x)| \\leq C_\\alpha (1 + |x|)^{-n-1}$ is **not necessarily true** under the stated assumptions. The estimate holds only if stronger regularity assumptions are imposed on $\\hat{f}$ at the boundary of the unit ball (e.g., if $\\hat{f} \\in C_c^\\infty(B(0,1))$).",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": true,
          "issues": [],
          "improvements": ""
        }
      }
    ],
    "outer_attempt": 1
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "## Step 1: Understand the task\nThe task requires generating a challenging mathematics problem in the domain of abstract harmonic analysis. The problem should be well-posed, solvable, and verifiable.\n[/QUESTION]",
    "run_id": "29",
    "topic_slug": "abstract_harmonic_analysis",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "Based on the instruction to act as a testee generating a challenging problem in abstract harmonic analysis, here is the generated problem followed by a complete, rigorous solution.\n\n---\n\n### **Generated Problem**\n\n**Topic:** Harmonic Analysis on Local Fields ($p$-adic Analysis)\n\n**Problem Statement:**\nLet $p$ be a prime number and let $\\mathbb{Q}_p$ denote the field of $p$-adic numbers. Let $\\mu$ be the additive Haar measure on $\\mathbb{Q}_p$, normalized such that $\\mu(\\mathbb{Z}_p) = 1$, where $\\mathbb{Z}_p$ is the ring of $p$-adic integers. Let $\\psi$ be the standard additive character of $\\mathbb{Q}_p$ defined by $\\psi(x) = e^{2\\pi i \\{x\\}_p}$, where $\\{x\\}_p$ denotes the fractional part of $x$.\n\nConsider the function $f: \\mathbb{Q}_p \\to \\mathbb{R}$ defined by:\n$$ f(x) = \\begin{cases} 1 & \\text{if } |x|_p \\le 1, \\\\ |x|_p^{-2} & \\text{if } |x|_p > 1. \\end{cases} $$\n\n1.  Prove that $f \\in L^1(\\mathbb{Q}_p) \\cap L^2(\\mathbb{Q}_p)$.\n2.  Compute the Fourier transform $\\widehat{f}(\\xi)$, defined by\n    $$ \\widehat{f}(\\xi) = \\int_{\\mathbb{Q}_p} f(x) \\psi(x\\xi) \\, d\\mu(x). $$\n    Express $\\widehat{f}(\\xi)$ explicitly as a function of $|\\xi|_p$.\n3.  Verify the Plancherel theorem for this function by explicitly computing $\\|f\\|_{L^2}^2$ and $\\|\\widehat{f}\\|_{L^2}^2$.\n\n---\n\n### **Solution**\n\n#### **Part 1: Integrability of $f$**\n\nThe group $\\mathbb{Q}_p$ can be decomposed into disjoint spherical shells $S_k = \\{ x \\in \\mathbb{Q}_p : |x|_p = p^k \\}$ for $k \\in \\mathbb{Z}$.\nThe measure of a shell $S_k$ is given by:\n$$ \\mu(S_k) = \\mu(p^{-k}\\mathbb{Z}_p) - \\mu(p^{-k+1}\\mathbb{Z}_p) = p^k - p^{k-1} = p^k(1 - p^{-1}). $$\n\n**Checking $L^1$ norm:**\n$$ \\|f\\|_1 = \\int_{\\mathbb{Q}_p} |f(x)| \\, d\\mu(x) = \\int_{|x|_p \\le 1} 1 \\, d\\mu(x) + \\sum_{k=1}^{\\infty} \\int_{S_k} |x|_p^{-2} \\, d\\mu(x). $$\nThe first term is $\\mu(\\mathbb{Z}_p) = 1$.\nFor the sum, on $S_k$, $|x|_p = p^k$, so $f(x) = p^{-2k}$.\n$$ \\sum_{k=1}^{\\infty} p^{-2k} \\mu(S_k) = \\sum_{k=1}^{\\infty} p^{-2k} \\cdot p^k (1 - p^{-1}) = (1 - p^{-1}) \\sum_{k=1}^{\\infty} p^{-k}. $$\nThis is a geometric series with ratio $1/p < 1$.\n$$ \\sum_{k=1}^{\\infty} p^{-k} = \\frac{1/p}{1 - 1/p} = \\frac{1}{p-1}. $$\nThus,\n$$ \\|f\\|_1 = 1 + (1 - p^{-1}) \\frac{1}{p-1} = 1 + \\frac{p-1}{p} \\frac{1}{p-1} = 1 + \\frac{1}{p} < \\infty. $$\nSo $f \\in L^1(\\mathbb{Q}_p)$.\n\n**Checking $L^2$ norm:**\n$$ \\|f\\|_2^2 = \\int_{\\mathbb{Q}_p} |f(x)|^2 \\, d\\mu(x) = \\int_{|x|_p \\le 1} 1^2 \\, d\\mu(x) + \\sum_{k=1}^{\\infty} \\int_{S_k} (p^{-2k})^2 \\, d\\mu(x). $$\n$$ \\|f\\|_2^2 = 1 + \\sum_{k=1}^{\\infty} p^{-4k} p^k (1 - p^{-1}) = 1 + (1 - p^{-1}) \\sum_{k=1}^{\\infty} p^{-3k}. $$\nThe geometric series sum is $\\frac{p^{-3}}{1 - p^{-3}} = \\frac{1}{p^3 - 1}$.\n$$ \\|f\\|_2^2 = 1 + \\frac{p-1}{p} \\frac{1}{(p-1)(p^2+p+1)} = 1 + \\frac{1}{p(p^2+p+1)} < \\infty. $$\nSo $f \\in L^2(\\mathbb{Q}_p)$.\n\n---\n\n#### **Part 2: Computing the Fourier Transform**\n\nSince $f$ is a radial function (depends only on $|x|_p$), $\\widehat{f}$ will also be radial. Let $|\\xi|_p = p^m$ for some integer $m$.\n$$ \\widehat{f}(\\xi) = \\sum_{k=-\\infty}^{\\infty} \\int_{S_k} f(x) \\psi(x\\xi) \\, d\\mu(x). $$\nSubstituting the values of $f(x)$:\n$$ \\widehat{f}(\\xi) = \\sum_{k=-\\infty}^{0} \\int_{S_k} \\psi(x\\xi) \\, d\\mu(x) + \\sum_{k=1}^{\\infty} p^{-2k} \\int_{S_k} \\psi(x\\xi) \\, d\\mu(x). $$\nLet $I_k(\\xi) = \\int_{S_k} \\psi(x\\xi) \\, d\\mu(x)$.\nWe define the ball $B_k = \\{ x : |x|_p \\le p^k \\}$. Then $S_k = B_k \\setminus B_{k-1}$.\nThe integral of the character over a ball is a standard result:\n$$ \\int_{B_k} \\psi(x\\xi) \\, d\\mu(x) = \\begin{cases} \\mu(B_k) = p^k & \\text{if } |\\xi|_p \\le p^{-k}, \\\\ 0 & \\text{if } |\\xi|_p > p^{-k}. \\end{cases} $$\nGiven $|\\xi|_p = p^m$, the condition $|\\xi|_p \\le p^{-k}$ is equivalent to $p^m \\le p^{-k} \\iff m \\le -k \\iff k \\le -m$.\n\nThus,\n$$ I_k(\\xi) = \\int_{B_k} \\psi(x\\xi) \\, d\\mu - \\int_{B_{k-1}} \\psi(x\\xi) \\, d\\mu. $$\nWe analyze $I_k(\\xi)$ based on the relation between $k$ and $-m$:\n1.  If $k \\le -m - 1$ (so $k-1 < k \\le -m$), both integrals are non-zero.\n    $I_k(\\xi) = p^k - p^{k-1}$. (Note: In this range, the character oscillates slowly enough to be effectively 1).\n2.  If $k = -m$:\n    The integral over $B_k$ is $p^k$ (since $k \\le -m$ holds).\n    The integral over $B_{k-1}$ is $p^{k-1}$ (since $k-1 \\le -m$ holds).\n    Wait, the condition is strict for the cutoff. Let's re-evaluate carefully.\n    \n    Let $J_k = \\mathbb{1}_{k \\le -m} p^k$. Then $I_k = J_k - J_{k-1}$.\n    \n    -   **Case $k < -m$**: $k \\le -m$ and $k-1 \\le -m$.\n        $I_k = p^k - p^{k-1}$.\n    -   **Case $k = -m$**: $k \\le -m$ is true. $k-1 \\le -m$ is true.\n        $I_k = p^{-m} - p^{-m-1}$.\n    -   **Case $k = -m+1$**: $k \\le -m$ is false ($J_k=0$). $k-1 = -m \\le -m$ is true ($J_{k-1}=p^{-m}$).\n        $I_k = 0 - p^{-m} = -p^{-m}$.\n    -   **Case $k > -m+1$**: Both are 0. $I_k = 0$.\n\nSo the only non-zero terms in the sum for $\\widehat{f}(\\xi)$ are for $k \\le -m+1$.\nWe split the calculation into cases for $m$ (the magnitude of $\\xi$).\n\n**Case A: $|\\xi|_p \\le 1$ (i.e., $m \\le 0$).**\nThe critical index is $-m+1 \\ge 1$.\nThe sum has two parts: $k \\le 0$ where $f(x)=1$, and $k \\ge 1$ where $f(x)=p^{-2k}$.\nSince $m \\le 0$, the \"cutoff\" $k = -m+1$ occurs in the $k \\ge 1$ region.\n\n1.  Sum for $k \\le 0$:\n    $$ \\sum_{k=-\\infty}^{0} 1 \\cdot (p^k - p^{k-1}) = \\lim_{N\\to-\\infty} (p^0 - p^{N-1}) = 1. $$\n    (This is simply $\\int_{|x|_p \\le 1} \\psi(x\\xi) d\\mu$, which is 1 since $|\\xi|_p \\le 1$).\n\n2.  Sum for $1 \\le k \\le -m$:\n    Here $I_k = p^k - p^{k-1}$.\n    $$ \\sum_{k=1}^{-m} p^{-2k} (p^k - p^{k-1}) = (1 - p^{-1}) \\sum_{k=1}^{-m} p^{-k} = (1 - p^{-1}) \\frac{p^{-1}(1 - p^{m})}{1 - p^{-1}} = p^{-1} - p^{m-1}. $$\n    If $-m < 1$ (i.e., $m=0$), this sum is empty (equals 0).\n\n3.  Term for $k = -m+1$:\n    $I_k = -p^{-m}$. $f(p^k) = p^{-2(-m+1)} = p^{2m-2}$.\n    $$ \\text{Term} = p^{2m-2} (-p^{-m}) = -p^{m-2}. $$\n\n**Subcase A1: $m=0$ ($|\\xi|_p=1$).**\nSum 1 (for $k \\le 0$) = 1.\nSum 2 (empty) = 0.\nTerm $k=1$: $I_1 = -1$ (since $m=0, -m+1=1$). $f(p)=p^{-2}$.\nContribution: $-p^{-2}$.\n$$ \\widehat{f}(\\xi) = 1 - p^{-2}. $$\n\n**Subcase A2: $m < 0$ ($|\\xi|_p < 1$).**\nTotal = (Sum 1) + (Sum 2) + (Term $k=-m+1$).\n$$ \\widehat{f}(\\xi) = 1 + (p^{-1} - p^{m-1}) - p^{m-2} = 1 + p^{-1} - p^{m-2}(p+1). $$\n\n**Case B: $|\\xi|_p > 1$ (i.e., $m \\ge 1$).**\nHere $-m+1 \\le 0$. The cutoff happens within the region where $f(x)=1$.\nThe terms for $k \\ge 1$ are all zero because $I_k=0$ there.\nWe only sum for $k \\le -m+1 \\le 0$.\nIn this range $f(p^k) = 1$.\nThe integral is over the ball $B_{-m+1}$.\n$$ \\widehat{f}(\\xi) = \\int_{|x|_p \\le p^{-m+1}} 1 \\cdot \\psi(x\\xi) \\, d\\mu(x). $$\nWe use the ball integral formula with radius $R = p^{-m+1}$.\nCondition for non-zero: $|\\xi|_p \\le 1/R = p^{m-1}$.\nBut we know $|\\xi|_p = p^m$.\nSince $p^m > p^{m-1}$, the condition is **false**.\nThus, $\\int_{B_{-m+1}} \\psi(x\\xi) \\, d\\mu(x) = 0$.\nWait, let's re-verify the cutoff summation logic.\n$\\widehat{f}(\\xi) = \\sum_{k=-\\infty}^{-m+1} f(p^k) I_k$.\nSince $k \\le -m+1 \\le 0$, $f(p^k) = 1$ everywhere in the sum.\nSo $\\widehat{f}(\\xi) = \\sum_{k=-\\infty}^{-m+1} (J_k - J_{k-1}) = J_{-m+1} - \\lim_{N\\to-\\infty} J_N$.\n$J_{-m+1} = p^{-m+1} \\mathbb{1}_{-m+1 \\le -m}$.\nIs $-m+1 \\le -m$? $1 \\le 0$? False.\nSo $J_{-m+1} = 0$.\nHence $\\widehat{f}(\\xi) = 0$ for all $m \\ge 1$.\n\n**Summary of $\\widehat{f}(\\xi)$:**\n$$ \\widehat{f}(\\xi) = \\begin{cases} 1 + p^{-1} - p^{-2}(p+1)|\\xi|_p & \\text{if } |\\xi|_p < 1, \\\\ 1 - p^{-2} & \\text{if } |\\xi|_p = 1, \\\\ 0 & \\text{if } |\\xi|_p > 1. \\end{cases} $$\nLet's check continuity at $|\\xi|_p \\to 1^-$ (i.e., $m \\to 0^-$? No, $p$-adic norm is discrete).\nAt $m=0$, formula 1 gives $1 + p^{-1} - p^{-2}(p+1) = 1 + p^{-1} - p^{-1} - p^{-2} = 1 - p^{-2}$. Matches.\nSo we can combine them:\n$$ \\widehat{f}(\\xi) = \\left( 1 + \\frac{1}{p} - \\frac{p+1}{p^2} |\\xi|_p \\right) \\mathbf{1}_{|\\xi|_p \\le 1}. $$\n\n---\n\n#### **Part 3: Verification of Plancherel Theorem**\n\nWe previously computed $\\|f\\|_2^2 = 1 + \\frac{1}{p(p^2+p+1)}$.\n\nNow we compute $\\|\\widehat{f}\\|_2^2$:\n$$ \\|\\widehat{f}\\|_2^2 = \\int_{\\mathbb{Q}_p} |\\widehat{f}(\\xi)|^2 \\, d\\mu(\\xi). $$\nSince $\\widehat{f}$ is supported on $|\\xi|_p \\le 1$ ($m \\le 0$):\n$$ \\|\\widehat{f}\\|_2^2 = \\sum_{m=-\\infty}^{0} \\int_{S_m} \\left( 1 + p^{-1} - p^{-2}(p+1)p^m \\right)^2 \\, d\\mu(\\xi). $$\nLet $A = 1 + p^{-1}$. Let $B = p^{-2}(p+1)$.\nTerm in square: $(A - B p^m)^2 = A^2 - 2AB p^m + B^2 p^{2m}$.\nMeasure of $S_m$ ($m \\le 0$): $p^m(1 - p^{-1})$.\n\n$$ \\|\\widehat{f}\\|_2^2 = \\sum_{m=-\\infty}^{0} (A^2 - 2AB p^m + B^2 p^{2m}) p^m (1 - p^{-1}). $$\nLet $C = 1 - p^{-1}$. Sum over $j = -m$ where $j \\in [0, \\infty)$. $p^m = p^{-j}$.\nTerms:\n1.  $A^2 \\sum p^m \\cdot p^m C$? No, integrand is constant on shell. Integral is Value$^2 \\times$ Volume.\n    $\\sum_{m=-\\infty}^0 (A - B p^m)^2 p^m C$.\n    Let $x = p^m$. Summing geometric series $\\sum_{k=0}^\\infty x^{k}$ with $x=1/p$.\n    Series 1: $\\sum_{m \\le 0} A^2 p^m = A^2 \\frac{1}{1-1/p} = A^2 \\frac{p}{p-1} = A^2 / C$.\n    Series 2: $\\sum_{m \\le 0} (-2AB) p^{2m} = -2AB \\frac{1}{1-1/p^2}$.\n    Series 3: $\\sum_{m \\le 0} B^2 p^{3m} = B^2 \\frac{1}{1-1/p^3}$.\n\nTotal $\\|\\widehat{f}\\|_2^2 = C \\left[ \\frac{A^2}{1-p^{-1}} - \\frac{2AB}{1-p^{-2}} + \\frac{B^2}{1-p^{-3}} \\right]$.\nNote $C = 1 - p^{-1}$.\nTerm 1: $C \\frac{A^2}{C} = A^2 = (1 + p^{-1})^2 = 1 + 2p^{-1} + p^{-2}$.\nTerm 2: $C \\frac{2AB}{C(1+p^{-1})} = \\frac{2AB}{1+p^{-1}} = \\frac{2A(p^{-2}(p+1))}{A} = 2 p^{-2}(p+1) = 2p^{-1} + 2p^{-2}$.\n    (Since $A = 1+1/p = (p+1)/p$, $2AB/(1+1/p) = 2B ((p+1)/p) / ((p+1)/p) = 2B$).\n    Wait, $1-p^{-2} = (1-p^{-1})(1+p^{-1})$. So denominator cancels $C$.\n    Left with $2AB/(1+p^{-1})$. Recall $A = 1+p^{-1}$. So it is just $2B$.\n    $2B = 2(p^{-2}(p+1)) = 2p^{-2} + 2p^{-1}$.\nTerm 3: $C \\frac{B^2}{1-p^{-3}} = (1-p^{-1}) \\frac{B^2}{(1-p^{-1})(1+p^{-1}+p^{-2})} = \\frac{B^2}{1+p^{-1}+p^{-2}}$.\n    $B = p^{-2}(p+1)$. $B^2 = p^{-4}(p+1)^2$.\n    Term 3 = $\\frac{p^{-4}(p+1)^2}{1+p^{-1}+p^{-2}} \\cdot \\frac{p^2}{p^2} = \\frac{p^{-4}(p+1)^2 p^2}{p^2+p+1} = \\frac{p^{-2}(p+1)^2}{p^2+p+1}$.\n\nSum = (Term 1) - (Term 2) + (Term 3).\n$(1 + 2p^{-1} + p^{-2}) - (2p^{-1} + 2p^{-2}) + \\text{Term 3}$\n$= 1 - p^{-2} + \\frac{p^{-2}(p+1)^2}{p^2+p+1}$\n$= 1 + p^{-2} \\left( -1 + \\frac{p^2+2p+1}{p^2+p+1} \\right)$\n$= 1 + p^{-2} \\left( \\frac{-(p^2+p+1) + p^2+2p+1}{p^2+p+1} \\right)$\n$= 1 + p^{-2} \\left( \\frac{p}{p^2+p+1} \\right)$\n$= 1 + \\frac{p^{-1}}{p^2+p+1} = 1 + \\frac{1}{p(p^2+p+1)}$.\n\nThis matches the result for $\\|f\\|_2^2$ exactly.\n\n**Conclusion:**\nThe Plancherel theorem is verified: $\\|f\\|_2^2 = \\|\\widehat{f}\\|_2^2$.",
        "raw_answer": "Based on the instruction to act as a testee generating a challenging problem in abstract harmonic analysis, here is the generated problem followed by a complete, rigorous solution.\n\n---\n\n### **Generated Problem**\n\n**Topic:** Harmonic Analysis on Local Fields ($p$-adic Analysis)\n\n**Problem Statement:**\nLet $p$ be a prime number and let $\\mathbb{Q}_p$ denote the field of $p$-adic numbers. Let $\\mu$ be the additive Haar measure on $\\mathbb{Q}_p$, normalized such that $\\mu(\\mathbb{Z}_p) = 1$, where $\\mathbb{Z}_p$ is the ring of $p$-adic integers. Let $\\psi$ be the standard additive character of $\\mathbb{Q}_p$ defined by $\\psi(x) = e^{2\\pi i \\{x\\}_p}$, where $\\{x\\}_p$ denotes the fractional part of $x$.\n\nConsider the function $f: \\mathbb{Q}_p \\to \\mathbb{R}$ defined by:\n$$ f(x) = \\begin{cases} 1 & \\text{if } |x|_p \\le 1, \\\\ |x|_p^{-2} & \\text{if } |x|_p > 1. \\end{cases} $$\n\n1.  Prove that $f \\in L^1(\\mathbb{Q}_p) \\cap L^2(\\mathbb{Q}_p)$.\n2.  Compute the Fourier transform $\\widehat{f}(\\xi)$, defined by\n    $$ \\widehat{f}(\\xi) = \\int_{\\mathbb{Q}_p} f(x) \\psi(x\\xi) \\, d\\mu(x). $$\n    Express $\\widehat{f}(\\xi)$ explicitly as a function of $|\\xi|_p$.\n3.  Verify the Plancherel theorem for this function by explicitly computing $\\|f\\|_{L^2}^2$ and $\\|\\widehat{f}\\|_{L^2}^2$.\n\n---\n\n### **Solution**\n\n#### **Part 1: Integrability of $f$**\n\nThe group $\\mathbb{Q}_p$ can be decomposed into disjoint spherical shells $S_k = \\{ x \\in \\mathbb{Q}_p : |x|_p = p^k \\}$ for $k \\in \\mathbb{Z}$.\nThe measure of a shell $S_k$ is given by:\n$$ \\mu(S_k) = \\mu(p^{-k}\\mathbb{Z}_p) - \\mu(p^{-k+1}\\mathbb{Z}_p) = p^k - p^{k-1} = p^k(1 - p^{-1}). $$\n\n**Checking $L^1$ norm:**\n$$ \\|f\\|_1 = \\int_{\\mathbb{Q}_p} |f(x)| \\, d\\mu(x) = \\int_{|x|_p \\le 1} 1 \\, d\\mu(x) + \\sum_{k=1}^{\\infty} \\int_{S_k} |x|_p^{-2} \\, d\\mu(x). $$\nThe first term is $\\mu(\\mathbb{Z}_p) = 1$.\nFor the sum, on $S_k$, $|x|_p = p^k$, so $f(x) = p^{-2k}$.\n$$ \\sum_{k=1}^{\\infty} p^{-2k} \\mu(S_k) = \\sum_{k=1}^{\\infty} p^{-2k} \\cdot p^k (1 - p^{-1}) = (1 - p^{-1}) \\sum_{k=1}^{\\infty} p^{-k}. $$\nThis is a geometric series with ratio $1/p < 1$.\n$$ \\sum_{k=1}^{\\infty} p^{-k} = \\frac{1/p}{1 - 1/p} = \\frac{1}{p-1}. $$\nThus,\n$$ \\|f\\|_1 = 1 + (1 - p^{-1}) \\frac{1}{p-1} = 1 + \\frac{p-1}{p} \\frac{1}{p-1} = 1 + \\frac{1}{p} < \\infty. $$\nSo $f \\in L^1(\\mathbb{Q}_p)$.\n\n**Checking $L^2$ norm:**\n$$ \\|f\\|_2^2 = \\int_{\\mathbb{Q}_p} |f(x)|^2 \\, d\\mu(x) = \\int_{|x|_p \\le 1} 1^2 \\, d\\mu(x) + \\sum_{k=1}^{\\infty} \\int_{S_k} (p^{-2k})^2 \\, d\\mu(x). $$\n$$ \\|f\\|_2^2 = 1 + \\sum_{k=1}^{\\infty} p^{-4k} p^k (1 - p^{-1}) = 1 + (1 - p^{-1}) \\sum_{k=1}^{\\infty} p^{-3k}. $$\nThe geometric series sum is $\\frac{p^{-3}}{1 - p^{-3}} = \\frac{1}{p^3 - 1}$.\n$$ \\|f\\|_2^2 = 1 + \\frac{p-1}{p} \\frac{1}{(p-1)(p^2+p+1)} = 1 + \\frac{1}{p(p^2+p+1)} < \\infty. $$\nSo $f \\in L^2(\\mathbb{Q}_p)$.\n\n---\n\n#### **Part 2: Computing the Fourier Transform**\n\nSince $f$ is a radial function (depends only on $|x|_p$), $\\widehat{f}$ will also be radial. Let $|\\xi|_p = p^m$ for some integer $m$.\n$$ \\widehat{f}(\\xi) = \\sum_{k=-\\infty}^{\\infty} \\int_{S_k} f(x) \\psi(x\\xi) \\, d\\mu(x). $$\nSubstituting the values of $f(x)$:\n$$ \\widehat{f}(\\xi) = \\sum_{k=-\\infty}^{0} \\int_{S_k} \\psi(x\\xi) \\, d\\mu(x) + \\sum_{k=1}^{\\infty} p^{-2k} \\int_{S_k} \\psi(x\\xi) \\, d\\mu(x). $$\nLet $I_k(\\xi) = \\int_{S_k} \\psi(x\\xi) \\, d\\mu(x)$.\nWe define the ball $B_k = \\{ x : |x|_p \\le p^k \\}$. Then $S_k = B_k \\setminus B_{k-1}$.\nThe integral of the character over a ball is a standard result:\n$$ \\int_{B_k} \\psi(x\\xi) \\, d\\mu(x) = \\begin{cases} \\mu(B_k) = p^k & \\text{if } |\\xi|_p \\le p^{-k}, \\\\ 0 & \\text{if } |\\xi|_p > p^{-k}. \\end{cases} $$\nGiven $|\\xi|_p = p^m$, the condition $|\\xi|_p \\le p^{-k}$ is equivalent to $p^m \\le p^{-k} \\iff m \\le -k \\iff k \\le -m$.\n\nThus,\n$$ I_k(\\xi) = \\int_{B_k} \\psi(x\\xi) \\, d\\mu - \\int_{B_{k-1}} \\psi(x\\xi) \\, d\\mu. $$\nWe analyze $I_k(\\xi)$ based on the relation between $k$ and $-m$:\n1.  If $k \\le -m - 1$ (so $k-1 < k \\le -m$), both integrals are non-zero.\n    $I_k(\\xi) = p^k - p^{k-1}$. (Note: In this range, the character oscillates slowly enough to be effectively 1).\n2.  If $k = -m$:\n    The integral over $B_k$ is $p^k$ (since $k \\le -m$ holds).\n    The integral over $B_{k-1}$ is $p^{k-1}$ (since $k-1 \\le -m$ holds).\n    Wait, the condition is strict for the cutoff. Let's re-evaluate carefully.\n    \n    Let $J_k = \\mathbb{1}_{k \\le -m} p^k$. Then $I_k = J_k - J_{k-1}$.\n    \n    -   **Case $k < -m$**: $k \\le -m$ and $k-1 \\le -m$.\n        $I_k = p^k - p^{k-1}$.\n    -   **Case $k = -m$**: $k \\le -m$ is true. $k-1 \\le -m$ is true.\n        $I_k = p^{-m} - p^{-m-1}$.\n    -   **Case $k = -m+1$**: $k \\le -m$ is false ($J_k=0$). $k-1 = -m \\le -m$ is true ($J_{k-1}=p^{-m}$).\n        $I_k = 0 - p^{-m} = -p^{-m}$.\n    -   **Case $k > -m+1$**: Both are 0. $I_k = 0$.\n\nSo the only non-zero terms in the sum for $\\widehat{f}(\\xi)$ are for $k \\le -m+1$.\nWe split the calculation into cases for $m$ (the magnitude of $\\xi$).\n\n**Case A: $|\\xi|_p \\le 1$ (i.e., $m \\le 0$).**\nThe critical index is $-m+1 \\ge 1$.\nThe sum has two parts: $k \\le 0$ where $f(x)=1$, and $k \\ge 1$ where $f(x)=p^{-2k}$.\nSince $m \\le 0$, the \"cutoff\" $k = -m+1$ occurs in the $k \\ge 1$ region.\n\n1.  Sum for $k \\le 0$:\n    $$ \\sum_{k=-\\infty}^{0} 1 \\cdot (p^k - p^{k-1}) = \\lim_{N\\to-\\infty} (p^0 - p^{N-1}) = 1. $$\n    (This is simply $\\int_{|x|_p \\le 1} \\psi(x\\xi) d\\mu$, which is 1 since $|\\xi|_p \\le 1$).\n\n2.  Sum for $1 \\le k \\le -m$:\n    Here $I_k = p^k - p^{k-1}$.\n    $$ \\sum_{k=1}^{-m} p^{-2k} (p^k - p^{k-1}) = (1 - p^{-1}) \\sum_{k=1}^{-m} p^{-k} = (1 - p^{-1}) \\frac{p^{-1}(1 - p^{m})}{1 - p^{-1}} = p^{-1} - p^{m-1}. $$\n    If $-m < 1$ (i.e., $m=0$), this sum is empty (equals 0).\n\n3.  Term for $k = -m+1$:\n    $I_k = -p^{-m}$. $f(p^k) = p^{-2(-m+1)} = p^{2m-2}$.\n    $$ \\text{Term} = p^{2m-2} (-p^{-m}) = -p^{m-2}. $$\n\n**Subcase A1: $m=0$ ($|\\xi|_p=1$).**\nSum 1 (for $k \\le 0$) = 1.\nSum 2 (empty) = 0.\nTerm $k=1$: $I_1 = -1$ (since $m=0, -m+1=1$). $f(p)=p^{-2}$.\nContribution: $-p^{-2}$.\n$$ \\widehat{f}(\\xi) = 1 - p^{-2}. $$\n\n**Subcase A2: $m < 0$ ($|\\xi|_p < 1$).**\nTotal = (Sum 1) + (Sum 2) + (Term $k=-m+1$).\n$$ \\widehat{f}(\\xi) = 1 + (p^{-1} - p^{m-1}) - p^{m-2} = 1 + p^{-1} - p^{m-2}(p+1). $$\n\n**Case B: $|\\xi|_p > 1$ (i.e., $m \\ge 1$).**\nHere $-m+1 \\le 0$. The cutoff happens within the region where $f(x)=1$.\nThe terms for $k \\ge 1$ are all zero because $I_k=0$ there.\nWe only sum for $k \\le -m+1 \\le 0$.\nIn this range $f(p^k) = 1$.\nThe integral is over the ball $B_{-m+1}$.\n$$ \\widehat{f}(\\xi) = \\int_{|x|_p \\le p^{-m+1}} 1 \\cdot \\psi(x\\xi) \\, d\\mu(x). $$\nWe use the ball integral formula with radius $R = p^{-m+1}$.\nCondition for non-zero: $|\\xi|_p \\le 1/R = p^{m-1}$.\nBut we know $|\\xi|_p = p^m$.\nSince $p^m > p^{m-1}$, the condition is **false**.\nThus, $\\int_{B_{-m+1}} \\psi(x\\xi) \\, d\\mu(x) = 0$.\nWait, let's re-verify the cutoff summation logic.\n$\\widehat{f}(\\xi) = \\sum_{k=-\\infty}^{-m+1} f(p^k) I_k$.\nSince $k \\le -m+1 \\le 0$, $f(p^k) = 1$ everywhere in the sum.\nSo $\\widehat{f}(\\xi) = \\sum_{k=-\\infty}^{-m+1} (J_k - J_{k-1}) = J_{-m+1} - \\lim_{N\\to-\\infty} J_N$.\n$J_{-m+1} = p^{-m+1} \\mathbb{1}_{-m+1 \\le -m}$.\nIs $-m+1 \\le -m$? $1 \\le 0$? False.\nSo $J_{-m+1} = 0$.\nHence $\\widehat{f}(\\xi) = 0$ for all $m \\ge 1$.\n\n**Summary of $\\widehat{f}(\\xi)$:**\n$$ \\widehat{f}(\\xi) = \\begin{cases} 1 + p^{-1} - p^{-2}(p+1)|\\xi|_p & \\text{if } |\\xi|_p < 1, \\\\ 1 - p^{-2} & \\text{if } |\\xi|_p = 1, \\\\ 0 & \\text{if } |\\xi|_p > 1. \\end{cases} $$\nLet's check continuity at $|\\xi|_p \\to 1^-$ (i.e., $m \\to 0^-$? No, $p$-adic norm is discrete).\nAt $m=0$, formula 1 gives $1 + p^{-1} - p^{-2}(p+1) = 1 + p^{-1} - p^{-1} - p^{-2} = 1 - p^{-2}$. Matches.\nSo we can combine them:\n$$ \\widehat{f}(\\xi) = \\left( 1 + \\frac{1}{p} - \\frac{p+1}{p^2} |\\xi|_p \\right) \\mathbf{1}_{|\\xi|_p \\le 1}. $$\n\n---\n\n#### **Part 3: Verification of Plancherel Theorem**\n\nWe previously computed $\\|f\\|_2^2 = 1 + \\frac{1}{p(p^2+p+1)}$.\n\nNow we compute $\\|\\widehat{f}\\|_2^2$:\n$$ \\|\\widehat{f}\\|_2^2 = \\int_{\\mathbb{Q}_p} |\\widehat{f}(\\xi)|^2 \\, d\\mu(\\xi). $$\nSince $\\widehat{f}$ is supported on $|\\xi|_p \\le 1$ ($m \\le 0$):\n$$ \\|\\widehat{f}\\|_2^2 = \\sum_{m=-\\infty}^{0} \\int_{S_m} \\left( 1 + p^{-1} - p^{-2}(p+1)p^m \\right)^2 \\, d\\mu(\\xi). $$\nLet $A = 1 + p^{-1}$. Let $B = p^{-2}(p+1)$.\nTerm in square: $(A - B p^m)^2 = A^2 - 2AB p^m + B^2 p^{2m}$.\nMeasure of $S_m$ ($m \\le 0$): $p^m(1 - p^{-1})$.\n\n$$ \\|\\widehat{f}\\|_2^2 = \\sum_{m=-\\infty}^{0} (A^2 - 2AB p^m + B^2 p^{2m}) p^m (1 - p^{-1}). $$\nLet $C = 1 - p^{-1}$. Sum over $j = -m$ where $j \\in [0, \\infty)$. $p^m = p^{-j}$.\nTerms:\n1.  $A^2 \\sum p^m \\cdot p^m C$? No, integrand is constant on shell. Integral is Value$^2 \\times$ Volume.\n    $\\sum_{m=-\\infty}^0 (A - B p^m)^2 p^m C$.\n    Let $x = p^m$. Summing geometric series $\\sum_{k=0}^\\infty x^{k}$ with $x=1/p$.\n    Series 1: $\\sum_{m \\le 0} A^2 p^m = A^2 \\frac{1}{1-1/p} = A^2 \\frac{p}{p-1} = A^2 / C$.\n    Series 2: $\\sum_{m \\le 0} (-2AB) p^{2m} = -2AB \\frac{1}{1-1/p^2}$.\n    Series 3: $\\sum_{m \\le 0} B^2 p^{3m} = B^2 \\frac{1}{1-1/p^3}$.\n\nTotal $\\|\\widehat{f}\\|_2^2 = C \\left[ \\frac{A^2}{1-p^{-1}} - \\frac{2AB}{1-p^{-2}} + \\frac{B^2}{1-p^{-3}} \\right]$.\nNote $C = 1 - p^{-1}$.\nTerm 1: $C \\frac{A^2}{C} = A^2 = (1 + p^{-1})^2 = 1 + 2p^{-1} + p^{-2}$.\nTerm 2: $C \\frac{2AB}{C(1+p^{-1})} = \\frac{2AB}{1+p^{-1}} = \\frac{2A(p^{-2}(p+1))}{A} = 2 p^{-2}(p+1) = 2p^{-1} + 2p^{-2}$.\n    (Since $A = 1+1/p = (p+1)/p$, $2AB/(1+1/p) = 2B ((p+1)/p) / ((p+1)/p) = 2B$).\n    Wait, $1-p^{-2} = (1-p^{-1})(1+p^{-1})$. So denominator cancels $C$.\n    Left with $2AB/(1+p^{-1})$. Recall $A = 1+p^{-1}$. So it is just $2B$.\n    $2B = 2(p^{-2}(p+1)) = 2p^{-2} + 2p^{-1}$.\nTerm 3: $C \\frac{B^2}{1-p^{-3}} = (1-p^{-1}) \\frac{B^2}{(1-p^{-1})(1+p^{-1}+p^{-2})} = \\frac{B^2}{1+p^{-1}+p^{-2}}$.\n    $B = p^{-2}(p+1)$. $B^2 = p^{-4}(p+1)^2$.\n    Term 3 = $\\frac{p^{-4}(p+1)^2}{1+p^{-1}+p^{-2}} \\cdot \\frac{p^2}{p^2} = \\frac{p^{-4}(p+1)^2 p^2}{p^2+p+1} = \\frac{p^{-2}(p+1)^2}{p^2+p+1}$.\n\nSum = (Term 1) - (Term 2) + (Term 3).\n$(1 + 2p^{-1} + p^{-2}) - (2p^{-1} + 2p^{-2}) + \\text{Term 3}$\n$= 1 - p^{-2} + \\frac{p^{-2}(p+1)^2}{p^2+p+1}$\n$= 1 + p^{-2} \\left( -1 + \\frac{p^2+2p+1}{p^2+p+1} \\right)$\n$= 1 + p^{-2} \\left( \\frac{-(p^2+p+1) + p^2+2p+1}{p^2+p+1} \\right)$\n$= 1 + p^{-2} \\left( \\frac{p}{p^2+p+1} \\right)$\n$= 1 + \\frac{p^{-1}}{p^2+p+1} = 1 + \\frac{1}{p(p^2+p+1)}$.\n\nThis matches the result for $\\|f\\|_2^2$ exactly.\n\n**Conclusion:**\nThe Plancherel theorem is verified: $\\|f\\|_2^2 = \\|\\widehat{f}\\|_2^2$.",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ],
    "outer_attempt": 1
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "## Step 1: Understand the task and the domain\nThe task is to generate a challenging mathematics problem in the domain of \"Integral transforms, operational calculus\" along with a complete and verifiable solution. The problem should be well-posed, clear, non-trivial, and solvable with established mathematical methods.\n\n## Step 2: Choose a specific topic within the domain\nLet's focus on the Laplace transform, a key concept within integral transforms and operational calculus. The Laplace transform is defined as $\\mathcal{L}\\{f(t)\\} = \\int_{0}^{\\infty} e^{-st}f(t)dt = F(s)$, where $f(t)$ is a function defined for $t \\geq 0$.\n\n## Step 3: Formulate a challenging problem\nTo create a challenging problem, we can consider applying the Laplace transform to solve a differential equation, as this involves understanding both the transform and its inverse. A suitable problem could involve solving an initial value problem (IVP) using the Laplace transform.\n\n## Step 4: Specify the problem\nLet's consider the following IVP: $y'' + 4y = \\sin(2t)$, with initial conditions $y(0) = 0$ and $y'(0) = 0$. The task is to solve for $y(t)$ using the Laplace transform.\n\n## Step 5: Write the problem statement\n\nUse the Laplace transform to solve the initial value problem $y'' + 4y = \\sin(2t)$, where $y(0) = 0$ and $y'(0) = 0$.\n\n## Step 6: Provide the solution\nTo solve the given IVP using the Laplace transform, we first apply the Laplace transform to both sides of the differential equation.\n\n## Step 7: Apply the Laplace transform to the differential equation\n$\\mathcal{L}\\{y'' + 4y\\} = \\mathcal{L}\\{\\sin(2t)\\}$. Using linearity, we get $\\mathcal{L}\\{y''\\} + 4\\mathcal{L}\\{y\\} = \\mathcal{L}\\{\\sin(2t)\\}$.\n\n## Step 8: Evaluate the Laplace transforms\n$\\mathcal{L}\\{y''\\} = s^2Y(s) - sy(0) - y'(0)$ and $\\mathcal{L}\\{y\\} = Y(s)$, where $Y(s) = \\mathcal{L}\\{y(t)\\}$. Given $y(0) = 0$ and $y'(0) = 0$, $\\mathcal{L}\\{y''\\} = s^2Y(s)$. $\\mathcal{L}\\{\\sin(2t)\\} = \\frac{2}{s^2 + 4}$.\n\n## Step 9: Substitute the known values and simplify\n$s^2Y(s) + 4Y(s) = \\frac{2}{s^2 + 4}$. Thus, $Y(s)(s^2 + 4) = \\frac{2}{s^2 + 4}$, which simplifies to $Y(s) = \\frac{2}{(s^2 + 4)^2}$.\n\n## Step 10: Find the inverse Laplace transform of $Y(s)$\nTo find $y(t)$, we need $\\mathcal{L}^{-1}\\{Y(s)\\} = \\mathcal{L}^{-1}\\left\\{\\frac{2}{(s^2 + 4)^2}\\right\\}$. Recall that $\\mathcal{L}\\{\\sin(at)\\} = \\frac{a}{s^2 + a^2}$, and use the property that $\\mathcal{L}^{-1}\\{F(s)\\} = f(t)$ implies $\\mathcal{L}^{-1}\\left\\{\\frac{d}{ds}F(s)\\right\\} = -tf(t)$, or more directly, utilize the convolution or known transforms.\n\n## Step 11: Evaluate the inverse Laplace transform\nUsing the formula for $\\mathcal{L}^{-1}\\left\\{\\frac{2a^3}{(s^2 + a^2)^2}\\right\\} = \\sin(at) - at\\cos(at)$ with $a = 2$, we adjust for our case: $\\mathcal{L}^{-1}\\left\\{\\frac{2}{(s^2 + 4)^2}\\right\\} = \\frac{1}{4}\\mathcal{L}^{-1}\\left\\{\\frac{2 \\cdot 2^3}{(s^2 + 2^2)^2}\\right\\} = \\frac{1}{4}(\\sin(2t) - 2t\\cos(2t))$.\n\n## Step 12: Write the complete solution",
    "run_id": "30",
    "topic_slug": "integral_transforms_and_operational_calculus",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "To solve the initial value problem (IVP) given by\n$$y'' + 4y = \\sin(2t)$$\nwith initial conditions $y(0) = 0$ and $y'(0) = 0$, we will use the Laplace transform method.\n\n### Step 1: Apply the Laplace Transform\n\nLet $Y(s) = \\mathcal{L}\\{y(t)\\}$ be the Laplace transform of the unknown function $y(t)$. Applying the Laplace transform to both sides of the differential equation, we utilize the linearity property:\n$$ \\mathcal{L}\\{y''\\} + 4\\mathcal{L}\\{y\\} = \\mathcal{L}\\{\\sin(2t)\\} $$\n\nRecall the derivative property for the Laplace transform:\n$$ \\mathcal{L}\\{y''\\} = s^2 Y(s) - s y(0) - y'(0) $$\n\nSubstitute the given initial conditions $y(0) = 0$ and $y'(0) = 0$:\n$$ \\mathcal{L}\\{y''\\} = s^2 Y(s) - s(0) - 0 = s^2 Y(s) $$\n\nNext, recall the standard transform for sine:\n$$ \\mathcal{L}\\{\\sin(at)\\} = \\frac{a}{s^2 + a^2} $$\nFor $\\sin(2t)$, where $a=2$:\n$$ \\mathcal{L}\\{\\sin(2t)\\} = \\frac{2}{s^2 + 4} $$\n\nSubstituting these back into the transformed equation yields:\n$$ s^2 Y(s) + 4 Y(s) = \\frac{2}{s^2 + 4} $$\n\n### Step 2: Solve for $Y(s)$\n\nFactor out $Y(s)$ on the left side:\n$$ (s^2 + 4) Y(s) = \\frac{2}{s^2 + 4} $$\n\nDivide by $(s^2 + 4)$ to isolate $Y(s)$:\n$$ Y(s) = \\frac{2}{(s^2 + 4)^2} $$\n\n### Step 3: Compute the Inverse Laplace Transform\n\nWe now need to find $y(t) = \\mathcal{L}^{-1}\\{Y(s)\\}$. We are looking for the inverse transform of:\n$$ Y(s) = \\frac{2}{(s^2 + 4)^2} $$\n\nWe can derive this inverse using the property of the derivative of a transform, specifically regarding the term $t \\cos(at)$.\nRecall that if $F(s) = \\mathcal{L}\\{f(t)\\}$, then $F'(s) = \\mathcal{L}\\{-t f(t)\\}$.\n\nLet us consider the transform of $\\cos(2t)$:\n$$ \\mathcal{L}\\{\\cos(2t)\\} = \\frac{s}{s^2 + 4} $$\nDifferentiating with respect to $s$:\n$$ \\frac{d}{ds} \\left( \\frac{s}{s^2 + 4} \\right) = \\frac{1 \\cdot (s^2 + 4) - s \\cdot (2s)}{(s^2 + 4)^2} = \\frac{4 - s^2}{(s^2 + 4)^2} $$\nUsing the property $\\mathcal{L}\\{t f(t)\\} = -F'(s)$, we have:\n$$ \\mathcal{L}\\{t \\cos(2t)\\} = - \\frac{4 - s^2}{(s^2 + 4)^2} = \\frac{s^2 - 4}{(s^2 + 4)^2} $$\n\nThis does not immediately match our target $Y(s)$. However, we can also look at the transform of $\\sin(2t)$:\n$$ \\mathcal{L}\\{\\sin(2t)\\} = \\frac{2}{s^2 + 4} = \\frac{2(s^2 + 4)}{(s^2 + 4)^2} = \\frac{2s^2 + 8}{(s^2 + 4)^2} $$\n\nWe seek a linear combination of these known transforms that equals $\\frac{2}{(s^2 + 4)^2}$. Let us check the expression $\\sin(2t) - 2t \\cos(2t)$.\n$$ \\mathcal{L}\\{\\sin(2t) - 2t \\cos(2t)\\} = \\mathcal{L}\\{\\sin(2t)\\} - 2\\mathcal{L}\\{t \\cos(2t)\\} $$\n$$ = \\frac{2s^2 + 8}{(s^2 + 4)^2} - 2 \\left( \\frac{s^2 - 4}{(s^2 + 4)^2} \\right) $$\n$$ = \\frac{2s^2 + 8 - 2s^2 + 8}{(s^2 + 4)^2} $$\n$$ = \\frac{16}{(s^2 + 4)^2} $$\n\nWe have obtained a numerator of 16. Our target numerator is 2. Therefore, we must divide by 8:\n$$ \\mathcal{L}^{-1} \\left\\{ \\frac{2}{(s^2 + 4)^2} \\right\\} = \\frac{1}{8} \\mathcal{L}^{-1} \\left\\{ \\frac{16}{(s^2 + 4)^2} \\right\\} $$\n$$ y(t) = \\frac{1}{8} \\left( \\sin(2t) - 2t \\cos(2t) \\right) $$\n\n### Step 4: Final Solution and Verification\n\nSimplifying the expression:\n$$ y(t) = \\frac{1}{8}\\sin(2t) - \\frac{1}{4}t\\cos(2t) $$\n\n**Verification:**\nLet us verify this solution satisfies the original IVP.\n1.  **Initial Conditions:**\n    *   $y(0) = \\frac{1}{8}(0) - 0 = 0$. (Check)\n    *   $y'(t) = \\frac{1}{8}(2\\cos(2t)) - \\frac{1}{4}(\\cos(2t) - 2t\\sin(2t)) = \\frac{1}{4}\\cos(2t) - \\frac{1}{4}\\cos(2t) + \\frac{1}{2}t\\sin(2t) = \\frac{1}{2}t\\sin(2t)$.\n    *   $y'(0) = 0$. (Check)\n\n2.  **Differential Equation:**\n    *   Calculate $y''(t)$:\n        $y''(t) = \\frac{d}{dt} \\left( \\frac{1}{2}t\\sin(2t) \\right) = \\frac{1}{2}(\\sin(2t) + 2t\\cos(2t)) = \\frac{1}{2}\\sin(2t) + t\\cos(2t)$.\n    *   Substitute into $y'' + 4y$:\n        $$ \\left( \\frac{1}{2}\\sin(2t) + t\\cos(2t) \\right) + 4 \\left( \\frac{1}{8}\\sin(2t) - \\frac{1}{4}t\\cos(2t) \\right) $$\n        $$ = \\frac{1}{2}\\sin(2t) + t\\cos(2t) + \\frac{1}{2}\\sin(2t) - t\\cos(2t) $$\n        $$ = \\sin(2t) $$\n    The solution satisfies the differential equation.\n\n**Conclusion:**\nThe solution to the initial value problem is:\n$$ y(t) = \\frac{1}{8}\\sin(2t) - \\frac{1}{4}t\\cos(2t) $$",
        "raw_answer": "To solve the initial value problem (IVP) given by\n$$y'' + 4y = \\sin(2t)$$\nwith initial conditions $y(0) = 0$ and $y'(0) = 0$, we will use the Laplace transform method.\n\n### Step 1: Apply the Laplace Transform\n\nLet $Y(s) = \\mathcal{L}\\{y(t)\\}$ be the Laplace transform of the unknown function $y(t)$. Applying the Laplace transform to both sides of the differential equation, we utilize the linearity property:\n$$ \\mathcal{L}\\{y''\\} + 4\\mathcal{L}\\{y\\} = \\mathcal{L}\\{\\sin(2t)\\} $$\n\nRecall the derivative property for the Laplace transform:\n$$ \\mathcal{L}\\{y''\\} = s^2 Y(s) - s y(0) - y'(0) $$\n\nSubstitute the given initial conditions $y(0) = 0$ and $y'(0) = 0$:\n$$ \\mathcal{L}\\{y''\\} = s^2 Y(s) - s(0) - 0 = s^2 Y(s) $$\n\nNext, recall the standard transform for sine:\n$$ \\mathcal{L}\\{\\sin(at)\\} = \\frac{a}{s^2 + a^2} $$\nFor $\\sin(2t)$, where $a=2$:\n$$ \\mathcal{L}\\{\\sin(2t)\\} = \\frac{2}{s^2 + 4} $$\n\nSubstituting these back into the transformed equation yields:\n$$ s^2 Y(s) + 4 Y(s) = \\frac{2}{s^2 + 4} $$\n\n### Step 2: Solve for $Y(s)$\n\nFactor out $Y(s)$ on the left side:\n$$ (s^2 + 4) Y(s) = \\frac{2}{s^2 + 4} $$\n\nDivide by $(s^2 + 4)$ to isolate $Y(s)$:\n$$ Y(s) = \\frac{2}{(s^2 + 4)^2} $$\n\n### Step 3: Compute the Inverse Laplace Transform\n\nWe now need to find $y(t) = \\mathcal{L}^{-1}\\{Y(s)\\}$. We are looking for the inverse transform of:\n$$ Y(s) = \\frac{2}{(s^2 + 4)^2} $$\n\nWe can derive this inverse using the property of the derivative of a transform, specifically regarding the term $t \\cos(at)$.\nRecall that if $F(s) = \\mathcal{L}\\{f(t)\\}$, then $F'(s) = \\mathcal{L}\\{-t f(t)\\}$.\n\nLet us consider the transform of $\\cos(2t)$:\n$$ \\mathcal{L}\\{\\cos(2t)\\} = \\frac{s}{s^2 + 4} $$\nDifferentiating with respect to $s$:\n$$ \\frac{d}{ds} \\left( \\frac{s}{s^2 + 4} \\right) = \\frac{1 \\cdot (s^2 + 4) - s \\cdot (2s)}{(s^2 + 4)^2} = \\frac{4 - s^2}{(s^2 + 4)^2} $$\nUsing the property $\\mathcal{L}\\{t f(t)\\} = -F'(s)$, we have:\n$$ \\mathcal{L}\\{t \\cos(2t)\\} = - \\frac{4 - s^2}{(s^2 + 4)^2} = \\frac{s^2 - 4}{(s^2 + 4)^2} $$\n\nThis does not immediately match our target $Y(s)$. However, we can also look at the transform of $\\sin(2t)$:\n$$ \\mathcal{L}\\{\\sin(2t)\\} = \\frac{2}{s^2 + 4} = \\frac{2(s^2 + 4)}{(s^2 + 4)^2} = \\frac{2s^2 + 8}{(s^2 + 4)^2} $$\n\nWe seek a linear combination of these known transforms that equals $\\frac{2}{(s^2 + 4)^2}$. Let us check the expression $\\sin(2t) - 2t \\cos(2t)$.\n$$ \\mathcal{L}\\{\\sin(2t) - 2t \\cos(2t)\\} = \\mathcal{L}\\{\\sin(2t)\\} - 2\\mathcal{L}\\{t \\cos(2t)\\} $$\n$$ = \\frac{2s^2 + 8}{(s^2 + 4)^2} - 2 \\left( \\frac{s^2 - 4}{(s^2 + 4)^2} \\right) $$\n$$ = \\frac{2s^2 + 8 - 2s^2 + 8}{(s^2 + 4)^2} $$\n$$ = \\frac{16}{(s^2 + 4)^2} $$\n\nWe have obtained a numerator of 16. Our target numerator is 2. Therefore, we must divide by 8:\n$$ \\mathcal{L}^{-1} \\left\\{ \\frac{2}{(s^2 + 4)^2} \\right\\} = \\frac{1}{8} \\mathcal{L}^{-1} \\left\\{ \\frac{16}{(s^2 + 4)^2} \\right\\} $$\n$$ y(t) = \\frac{1}{8} \\left( \\sin(2t) - 2t \\cos(2t) \\right) $$\n\n### Step 4: Final Solution and Verification\n\nSimplifying the expression:\n$$ y(t) = \\frac{1}{8}\\sin(2t) - \\frac{1}{4}t\\cos(2t) $$\n\n**Verification:**\nLet us verify this solution satisfies the original IVP.\n1.  **Initial Conditions:**\n    *   $y(0) = \\frac{1}{8}(0) - 0 = 0$. (Check)\n    *   $y'(t) = \\frac{1}{8}(2\\cos(2t)) - \\frac{1}{4}(\\cos(2t) - 2t\\sin(2t)) = \\frac{1}{4}\\cos(2t) - \\frac{1}{4}\\cos(2t) + \\frac{1}{2}t\\sin(2t) = \\frac{1}{2}t\\sin(2t)$.\n    *   $y'(0) = 0$. (Check)\n\n2.  **Differential Equation:**\n    *   Calculate $y''(t)$:\n        $y''(t) = \\frac{d}{dt} \\left( \\frac{1}{2}t\\sin(2t) \\right) = \\frac{1}{2}(\\sin(2t) + 2t\\cos(2t)) = \\frac{1}{2}\\sin(2t) + t\\cos(2t)$.\n    *   Substitute into $y'' + 4y$:\n        $$ \\left( \\frac{1}{2}\\sin(2t) + t\\cos(2t) \\right) + 4 \\left( \\frac{1}{8}\\sin(2t) - \\frac{1}{4}t\\cos(2t) \\right) $$\n        $$ = \\frac{1}{2}\\sin(2t) + t\\cos(2t) + \\frac{1}{2}\\sin(2t) - t\\cos(2t) $$\n        $$ = \\sin(2t) $$\n    The solution satisfies the differential equation.\n\n**Conclusion:**\nThe solution to the initial value problem is:\n$$ y(t) = \\frac{1}{8}\\sin(2t) - \\frac{1}{4}t\\cos(2t) $$",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ],
    "outer_attempt": 1
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "## Step 1: Understand the task and the domain\nThe task is to generate a challenging but solvable mathematics problem in the domain of integral equations, along with a complete and verifiable solution. The problem should be well-posed, clear, non-trivial, and solvable with established mathematical methods.\n\n## Step 2: Identify the specific domain within integral equations\nIntegral equations involve equations where the unknown function appears under an integral sign. A common type is the Fredholm integral equation of the second kind, given by $f(x) = g(x) + \\lambda \\int_{a}^{b} K(x, t)f(t)dt$, where $f(x)$ is the unknown function, $g(x)$ and $K(x, t)$ are known functions, and $\\lambda$ is a parameter.\n\n## Step 3: Formulate a challenging problem\nTo create a challenging problem, we can consider a specific case of the Fredholm integral equation of the second kind with a given kernel $K(x, t)$ and a function $g(x)$, and ask to solve for $f(x)$.\n\n## Step 4: Specify the problem\nLet's consider the integral equation $f(x) = x + \\int_{0}^{1} (xt + x + t)f(t)dt$. This is a Fredholm integral equation of the second kind. We need to find $f(x)$.\n\n## Step 5: Provide the solution\nTo solve $f(x) = x + \\int_{0}^{1} (xt + x + t)f(t)dt$, let's first simplify the integral.\n$f(x) = x + x\\int_{0}^{1} tf(t)dt + x\\int_{0}^{1} f(t)dt + \\int_{0}^{1} tf(t)dt$.\nLet $A = \\int_{0}^{1} f(t)dt$ and $B = \\int_{0}^{1} tf(t)dt$. Then, $f(x) = x + xB + xA + B = x(1 + A + B) + B$.\n\n## Step 6: Calculate A and B using the definitions\nSubstitute $f(x)$ from Step 5 into the definitions of $A$ and $B$.\n$A = \\int_{0}^{1} [t(1 + A + B) + B]dt = \\left[\\frac{t^2}{2}(1 + A + B) + Bt\\right]_{0}^{1} = \\frac{1}{2}(1 + A + B) + B$.\n$B = \\int_{0}^{1} t[t(1 + A + B) + B]dt = \\left[\\frac{t^3}{3}(1 + A + B) + \\frac{Bt^2}{2}\\right]_{0}^{1} = \\frac{1}{3}(1 + A + B) + \\frac{B}{2}$.\n\n## Step 7: Solve for A and B\nFrom Step 6, we have:\n1. $A = \\frac{1}{2}(1 + A + B) + B$\n2. $B = \\frac{1}{3}(1 + A + B) + \\frac{B}{2}$.\n\nSimplifying these equations:\n1. $A = \\frac{1}{2} + \\frac{A}{2} + \\frac{B}{2} + B$ implies $\\frac{A}{2} = \\frac{1}{2} + \\frac{3B}{2}$, so $A = 1 + 3B$.\n2. $B = \\frac{1}{3} + \\frac{A}{3} + \\frac{B}{3} + \\frac{B}{2}$ implies $B - \\frac{B}{2} - \\frac{B}{3} = \\frac{1}{3} + \\frac{A}{3}$, so $\\frac{B}{6} = \\frac{1}{3} + \\frac{A}{3}$.\n\n## Step 8: Continue solving for A and B\nFrom equation 2 in Step 7: $\\frac{B}{6} = \\frac{1}{3} + \\frac{A}{3}$, we get $B = 2 + 2A$.\nSubstitute $A = 1 + 3B$ from Step 7 into $B = 2 + 2A$:\n$B = 2 + 2(1 + 3B)$, so $B = 2 + 2 + 6B$, which simplifies to $B = 4 + 6B$, hence $-5B = 4$, and $B = -\\frac{4}{5}$.\n\n## Step 9: Find A using B\nSubstitute $B = -\\frac{4}{5}$ into $A = 1 + 3B$:\n$A = 1 + 3(-\\frac{4}{5}) = 1 - \\frac{12}{5} = -\\frac{7}{5}$.\n\n## Step 10: Substitute A and B back into f(x)\n$f(x) = x(1 + A + B) + B = x(1 - \\frac{7}{5} - \\frac{4}{5}) - \\frac{4}{5}$.\n$f(x) = x(1 - \\frac{11}{5}) - \\frac{4}{5} = x(-\\frac{6}{5}) - \\frac{4}{5}$.\n$f(x) = -\\frac{6}{5}x - \\frac{4}{5}$.\n\nThe final answer is: \n\nSolve the integral equation $f(x) = x + \\int_{0}^{1} (xt + x + t)f(t)dt$ for $f(x)$.",
    "run_id": "31",
    "topic_slug": "integral_equations",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "To find the function $f(x)$ that satisfies the integral equation\n$$ f(x) = x + \\int_{0}^{1} (xt + x + t)f(t) \\, dt, $$\nwe identify this as a Fredholm integral equation of the second kind with a separable (degenerate) kernel. The kernel is $K(x, t) = xt + x + t$.\n\n### Step 1: Separate the Kernel\nWe can expand the integral term to separate the components that depend on $x$ from those that do not.\n$$ \\int_{0}^{1} (xt + x + t)f(t) \\, dt = \\int_{0}^{1} (xt + x)f(t) \\, dt + \\int_{0}^{1} t f(t) \\, dt $$\nFactor out $x$ from the first integral:\n$$ \\int_{0}^{1} (xt + x + t)f(t) \\, dt = x \\int_{0}^{1} (t + 1)f(t) \\, dt + \\int_{0}^{1} t f(t) \\, dt $$\n\nSubstitute this back into the original equation:\n$$ f(x) = x + x \\int_{0}^{1} (t + 1)f(t) \\, dt + \\int_{0}^{1} t f(t) \\, dt $$\n$$ f(x) = x \\left( 1 + \\int_{0}^{1} (t + 1)f(t) \\, dt \\right) + \\int_{0}^{1} t f(t) \\, dt $$\n\n### Step 2: Define Constants\nSince the definite integrals result in constant values, we can define them as unknown constants $A$ and $B$. Let:\n$$ A = \\int_{0}^{1} f(t) \\, dt $$\n$$ B = \\int_{0}^{1} t f(t) \\, dt $$\n\nUsing these definitions, we can rewrite the expression for $f(x)$. First, note that $\\int_{0}^{1} (t + 1)f(t) \\, dt = \\int_{0}^{1} t f(t) \\, dt + \\int_{0}^{1} f(t) \\, dt = B + A$.\nSubstituting this into the equation for $f(x)$:\n$$ f(x) = x(1 + A + B) + B $$\n\nLet $C = 1 + A + B$. Then the form of the solution is linear:\n$$ f(x) = Cx + B $$\n\n### Step 3: Determine the Constants $A$ and $B$\nWe now substitute the form $f(t) = Ct + B$ back into the definitions of $A$ and $B$ to create a system of linear equations.\n\n**Equation for $A$:**\n$$ A = \\int_{0}^{1} (Ct + B) \\, dt $$\n$$ A = \\left[ \\frac{C t^2}{2} + Bt \\right]_{0}^{1} $$\n$$ A = \\frac{C}{2} + B $$\n\n**Equation for $B$:**\n$$ B = \\int_{0}^{1} t(Ct + B) \\, dt = \\int_{0}^{1} (Ct^2 + Bt) \\, dt $$\n$$ B = \\left[ \\frac{C t^3}{3} + \\frac{B t^2}{2} \\right]_{0}^{1} $$\n$$ B = \\frac{C}{3} + \\frac{B}{2} $$\n\nNow we have a system of equations. Recall that $C = 1 + A + B$. We can substitute $C$ into the equations or solve for $A$ and $B$ first. Let's simplify the equation for $B$ first.\n$$ B = \\frac{C}{3} + \\frac{B}{2} $$\nMultiply by 6 to clear denominators:\n$$ 6B = 2C + 3B $$\n$$ 3B = 2C $$\n$$ C = \\frac{3}{2}B $$\n\nNow substitute $C = \\frac{3}{2}B$ into the equation for $A$:\n$$ A = \\frac{1}{2}\\left(\\frac{3}{2}B\\right) + B $$\n$$ A = \\frac{3}{4}B + B $$\n$$ A = \\frac{7}{4}B $$\n\nWe also have the definition $C = 1 + A + B$. Substitute expressions for $A$ and $C$ in terms of $B$ into this equation:\n$$ \\frac{3}{2}B = 1 + \\frac{7}{4}B + B $$\n$$ \\frac{3}{2}B = 1 + \\frac{11}{4}B $$\nMultiply by 4 to clear denominators:\n$$ 6B = 4 + 11B $$\n$$ -5B = 4 $$\n$$ B = -\\frac{4}{5} $$\n\nNow find $A$ and $C$:\n$$ A = \\frac{7}{4}B = \\frac{7}{4}\\left(-\\frac{4}{5}\\right) = -\\frac{7}{5} $$\n$$ C = \\frac{3}{2}B = \\frac{3}{2}\\left(-\\frac{4}{5}\\right) = -\\frac{6}{5} $$\n\nAlternatively, verify $C = 1 + A + B$:\n$$ C = 1 + \\left(-\\frac{7}{5}\\right) + \\left(-\\frac{4}{5}\\right) = 1 - \\frac{11}{5} = -\\frac{6}{5} $$\nThe values are consistent.\n\n### Step 4: Construct the Final Solution\nThe function is $f(x) = Cx + B$. Substituting the values we found:\n$$ f(x) = -\\frac{6}{5}x - \\frac{4}{5} $$\n\n### Step 5: Verification\nWe verify the solution by substituting $f(x) = -\\frac{1}{5}(6x + 4)$ back into the original integral equation.\n**Right Hand Side (RHS):**\n$$ RHS = x + \\int_{0}^{1} (xt + x + t)\\left(-\\frac{6}{5}t - \\frac{4}{5}\\right) \\, dt $$\n$$ = x - \\frac{1}{5} \\int_{0}^{1} (xt + x + t)(6t + 4) \\, dt $$\nSeparate terms with $x$ and constant terms:\n$$ = x - \\frac{x}{5} \\int_{0}^{1} (t+1)(6t+4) \\, dt - \\frac{1}{5} \\int_{0}^{1} t(6t+4) \\, dt $$\n\nCalculate the first integral:\n$$ \\int_{0}^{1} (6t^2 + 4t + 6t + 4) \\, dt = \\int_{0}^{1} (6t^2 + 10t + 4) \\, dt $$\n$$ = \\left[ 2t^3 + 5t^2 + 4t \\right]_{0}^{1} = 2 + 5 + 4 = 11 $$\n\nCalculate the second integral:\n$$ \\int_{0}^{1} (6t^2 + 4t) \\, dt = \\left[ 2t^3 + 2t^2 \\right]_{0}^{1} = 2 + 2 = 4 $$\n\nSubstitute back into the RHS expression:\n$$ RHS = x - \\frac{x}{5}(11) - \\frac{1}{5}(4) $$\n$$ = x - \\frac{11}{5}x - \\frac{4}{5} $$\n$$ = -\\frac{6}{5}x - \\frac{4}{5} $$\n\n**Left Hand Side (LHS):**\n$$ LHS = f(x) = -\\frac{6}{5}x - \\frac{4}{5} $$\n\nSince $LHS = RHS$, the solution is correct.\n\n### Final Answer\nThe solution to the integral equation is:\n$$ f(x) = -\\frac{6}{5}x - \\frac{4}{5} $$",
        "raw_answer": "To find the function $f(x)$ that satisfies the integral equation\n$$ f(x) = x + \\int_{0}^{1} (xt + x + t)f(t) \\, dt, $$\nwe identify this as a Fredholm integral equation of the second kind with a separable (degenerate) kernel. The kernel is $K(x, t) = xt + x + t$.\n\n### Step 1: Separate the Kernel\nWe can expand the integral term to separate the components that depend on $x$ from those that do not.\n$$ \\int_{0}^{1} (xt + x + t)f(t) \\, dt = \\int_{0}^{1} (xt + x)f(t) \\, dt + \\int_{0}^{1} t f(t) \\, dt $$\nFactor out $x$ from the first integral:\n$$ \\int_{0}^{1} (xt + x + t)f(t) \\, dt = x \\int_{0}^{1} (t + 1)f(t) \\, dt + \\int_{0}^{1} t f(t) \\, dt $$\n\nSubstitute this back into the original equation:\n$$ f(x) = x + x \\int_{0}^{1} (t + 1)f(t) \\, dt + \\int_{0}^{1} t f(t) \\, dt $$\n$$ f(x) = x \\left( 1 + \\int_{0}^{1} (t + 1)f(t) \\, dt \\right) + \\int_{0}^{1} t f(t) \\, dt $$\n\n### Step 2: Define Constants\nSince the definite integrals result in constant values, we can define them as unknown constants $A$ and $B$. Let:\n$$ A = \\int_{0}^{1} f(t) \\, dt $$\n$$ B = \\int_{0}^{1} t f(t) \\, dt $$\n\nUsing these definitions, we can rewrite the expression for $f(x)$. First, note that $\\int_{0}^{1} (t + 1)f(t) \\, dt = \\int_{0}^{1} t f(t) \\, dt + \\int_{0}^{1} f(t) \\, dt = B + A$.\nSubstituting this into the equation for $f(x)$:\n$$ f(x) = x(1 + A + B) + B $$\n\nLet $C = 1 + A + B$. Then the form of the solution is linear:\n$$ f(x) = Cx + B $$\n\n### Step 3: Determine the Constants $A$ and $B$\nWe now substitute the form $f(t) = Ct + B$ back into the definitions of $A$ and $B$ to create a system of linear equations.\n\n**Equation for $A$:**\n$$ A = \\int_{0}^{1} (Ct + B) \\, dt $$\n$$ A = \\left[ \\frac{C t^2}{2} + Bt \\right]_{0}^{1} $$\n$$ A = \\frac{C}{2} + B $$\n\n**Equation for $B$:**\n$$ B = \\int_{0}^{1} t(Ct + B) \\, dt = \\int_{0}^{1} (Ct^2 + Bt) \\, dt $$\n$$ B = \\left[ \\frac{C t^3}{3} + \\frac{B t^2}{2} \\right]_{0}^{1} $$\n$$ B = \\frac{C}{3} + \\frac{B}{2} $$\n\nNow we have a system of equations. Recall that $C = 1 + A + B$. We can substitute $C$ into the equations or solve for $A$ and $B$ first. Let's simplify the equation for $B$ first.\n$$ B = \\frac{C}{3} + \\frac{B}{2} $$\nMultiply by 6 to clear denominators:\n$$ 6B = 2C + 3B $$\n$$ 3B = 2C $$\n$$ C = \\frac{3}{2}B $$\n\nNow substitute $C = \\frac{3}{2}B$ into the equation for $A$:\n$$ A = \\frac{1}{2}\\left(\\frac{3}{2}B\\right) + B $$\n$$ A = \\frac{3}{4}B + B $$\n$$ A = \\frac{7}{4}B $$\n\nWe also have the definition $C = 1 + A + B$. Substitute expressions for $A$ and $C$ in terms of $B$ into this equation:\n$$ \\frac{3}{2}B = 1 + \\frac{7}{4}B + B $$\n$$ \\frac{3}{2}B = 1 + \\frac{11}{4}B $$\nMultiply by 4 to clear denominators:\n$$ 6B = 4 + 11B $$\n$$ -5B = 4 $$\n$$ B = -\\frac{4}{5} $$\n\nNow find $A$ and $C$:\n$$ A = \\frac{7}{4}B = \\frac{7}{4}\\left(-\\frac{4}{5}\\right) = -\\frac{7}{5} $$\n$$ C = \\frac{3}{2}B = \\frac{3}{2}\\left(-\\frac{4}{5}\\right) = -\\frac{6}{5} $$\n\nAlternatively, verify $C = 1 + A + B$:\n$$ C = 1 + \\left(-\\frac{7}{5}\\right) + \\left(-\\frac{4}{5}\\right) = 1 - \\frac{11}{5} = -\\frac{6}{5} $$\nThe values are consistent.\n\n### Step 4: Construct the Final Solution\nThe function is $f(x) = Cx + B$. Substituting the values we found:\n$$ f(x) = -\\frac{6}{5}x - \\frac{4}{5} $$\n\n### Step 5: Verification\nWe verify the solution by substituting $f(x) = -\\frac{1}{5}(6x + 4)$ back into the original integral equation.\n**Right Hand Side (RHS):**\n$$ RHS = x + \\int_{0}^{1} (xt + x + t)\\left(-\\frac{6}{5}t - \\frac{4}{5}\\right) \\, dt $$\n$$ = x - \\frac{1}{5} \\int_{0}^{1} (xt + x + t)(6t + 4) \\, dt $$\nSeparate terms with $x$ and constant terms:\n$$ = x - \\frac{x}{5} \\int_{0}^{1} (t+1)(6t+4) \\, dt - \\frac{1}{5} \\int_{0}^{1} t(6t+4) \\, dt $$\n\nCalculate the first integral:\n$$ \\int_{0}^{1} (6t^2 + 4t + 6t + 4) \\, dt = \\int_{0}^{1} (6t^2 + 10t + 4) \\, dt $$\n$$ = \\left[ 2t^3 + 5t^2 + 4t \\right]_{0}^{1} = 2 + 5 + 4 = 11 $$\n\nCalculate the second integral:\n$$ \\int_{0}^{1} (6t^2 + 4t) \\, dt = \\left[ 2t^3 + 2t^2 \\right]_{0}^{1} = 2 + 2 = 4 $$\n\nSubstitute back into the RHS expression:\n$$ RHS = x - \\frac{x}{5}(11) - \\frac{1}{5}(4) $$\n$$ = x - \\frac{11}{5}x - \\frac{4}{5} $$\n$$ = -\\frac{6}{5}x - \\frac{4}{5} $$\n\n**Left Hand Side (LHS):**\n$$ LHS = f(x) = -\\frac{6}{5}x - \\frac{4}{5} $$\n\nSince $LHS = RHS$, the solution is correct.\n\n### Final Answer\nThe solution to the integral equation is:\n$$ f(x) = -\\frac{6}{5}x - \\frac{4}{5} $$",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ],
    "outer_attempt": 1
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "Let $H$ be a Hilbert space and $T: H \\to H$ be a bounded linear operator. Suppose that $T$ is self-adjoint and satisfies the condition $\\|T\\| \\leq 1$. Prove that the operator $I - T^2$ is positive, where $I$ denotes the identity operator on $H$.",
    "run_id": "32",
    "topic_slug": "functional_analysis",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "**Proof**\n\nLet $H$ be a Hilbert space and let $T: H \\to H$ be a bounded linear operator. We are given that $T$ is self-adjoint ($T = T^*$) and its operator norm satisfies $\\|T\\| \\leq 1$. We wish to prove that the operator $A = I - T^2$ is positive.\n\n### 1. Definition of a Positive Operator\nAn operator $A$ on a Hilbert space $H$ is defined to be **positive** (denoted $A \\geq 0$) if it satisfies two conditions:\n1. $A$ is self-adjoint ($A = A^*$).\n2. For every vector $x \\in H$, the inner product $\\langle Ax, x \\rangle$ is non-negative, i.e., $\\langle Ax, x \\rangle \\geq 0$.\n\n### 2. Verification of Self-Adjointness\nFirst, we show that $A = I - T^2$ is self-adjoint.\nUsing the properties of the adjoint operation (linearity and the reverse order law for products):\n$$\nA^* = (I - T^2)^* = I^* - (T^2)^* = I - (TT)^*.\n$$\nSince $I$ is self-adjoint, $I^* = I$. Applying the reverse order law $(ST)^* = T^* S^*$:\n$$\n(TT)^* = T^* T^*.\n$$\nBy the hypothesis that $T$ is self-adjoint, $T^* = T$. Therefore:\n$$\nT^* T^* = T T = T^2.\n$$\nSubstituting this back into the expression for $A^*$:\n$$\nA^* = I - T^2 = A.\n$$\nThus, $I - T^2$ is a self-adjoint operator.\n\n### 3. Verification of the Positivity Condition\nNext, we examine the quadratic form associated with $A$. Let $x \\in H$ be an arbitrary vector. We compute $\\langle (I - T^2)x, x \\rangle$:\n$$\n\\begin{aligned}\n\\langle (I - T^2)x, x \\rangle &= \\langle Ix - T^2x, x \\rangle \\\\\n&= \\langle x, x \\rangle - \\langle T^2x, x \\rangle \\quad &&(\\text{Linearity in the first argument}) \\\\\n&= \\|x\\|^2 - \\langle T(Tx), x \\rangle.\n\\end{aligned}\n$$\nBy the definition of the adjoint operator, $\\langle Ty, x \\rangle = \\langle y, T^*x \\rangle$. Applying this with $y = Tx$:\n$$\n\\langle T(Tx), x \\rangle = \\langle Tx, T^*x \\rangle.\n$$\nSince $T$ is self-adjoint ($T^* = T$), this becomes:\n$$\n\\langle Tx, Tx \\rangle = \\|Tx\\|^2.\n$$\nSubstituting this back into our expression for the quadratic form:\n$$\n\\langle (I - T^2)x, x \\rangle = \\|x\\|^2 - \\|Tx\\|^2.\n$$\n\n### 4. Application of the Norm Bound\nWe are given that $\\|T\\| \\leq 1$. By the definition of the operator norm, for any $x \\in H$:\n$$\n\\|Tx\\| \\leq \\|T\\| \\|x\\|.\n$$\nSubstituting $\\|T\\| \\leq 1$:\n$$\n\\|Tx\\| \\leq 1 \\cdot \\|x\\| = \\|x\\|.\n$$\nSince norms are non-negative, we can square both sides of the inequality:\n$$\n\\|Tx\\|^2 \\leq \\|x\\|^2.\n$$\nConsequently:\n$$\n\\|x\\|^2 - \\|Tx\\|^2 \\geq 0.\n$$\n\n### Conclusion\nCombining the results from steps 3 and 4, we have shown that for all $x \\in H$:\n$$\n\\langle (I - T^2)x, x \\rangle \\geq 0.\n$$\nSince $I - T^2$ is self-adjoint and satisfies the non-negativity condition for the inner product, the operator $I - T^2$ is positive.",
        "raw_answer": "**Proof**\n\nLet $H$ be a Hilbert space and let $T: H \\to H$ be a bounded linear operator. We are given that $T$ is self-adjoint ($T = T^*$) and its operator norm satisfies $\\|T\\| \\leq 1$. We wish to prove that the operator $A = I - T^2$ is positive.\n\n### 1. Definition of a Positive Operator\nAn operator $A$ on a Hilbert space $H$ is defined to be **positive** (denoted $A \\geq 0$) if it satisfies two conditions:\n1. $A$ is self-adjoint ($A = A^*$).\n2. For every vector $x \\in H$, the inner product $\\langle Ax, x \\rangle$ is non-negative, i.e., $\\langle Ax, x \\rangle \\geq 0$.\n\n### 2. Verification of Self-Adjointness\nFirst, we show that $A = I - T^2$ is self-adjoint.\nUsing the properties of the adjoint operation (linearity and the reverse order law for products):\n$$\nA^* = (I - T^2)^* = I^* - (T^2)^* = I - (TT)^*.\n$$\nSince $I$ is self-adjoint, $I^* = I$. Applying the reverse order law $(ST)^* = T^* S^*$:\n$$\n(TT)^* = T^* T^*.\n$$\nBy the hypothesis that $T$ is self-adjoint, $T^* = T$. Therefore:\n$$\nT^* T^* = T T = T^2.\n$$\nSubstituting this back into the expression for $A^*$:\n$$\nA^* = I - T^2 = A.\n$$\nThus, $I - T^2$ is a self-adjoint operator.\n\n### 3. Verification of the Positivity Condition\nNext, we examine the quadratic form associated with $A$. Let $x \\in H$ be an arbitrary vector. We compute $\\langle (I - T^2)x, x \\rangle$:\n$$\n\\begin{aligned}\n\\langle (I - T^2)x, x \\rangle &= \\langle Ix - T^2x, x \\rangle \\\\\n&= \\langle x, x \\rangle - \\langle T^2x, x \\rangle \\quad &&(\\text{Linearity in the first argument}) \\\\\n&= \\|x\\|^2 - \\langle T(Tx), x \\rangle.\n\\end{aligned}\n$$\nBy the definition of the adjoint operator, $\\langle Ty, x \\rangle = \\langle y, T^*x \\rangle$. Applying this with $y = Tx$:\n$$\n\\langle T(Tx), x \\rangle = \\langle Tx, T^*x \\rangle.\n$$\nSince $T$ is self-adjoint ($T^* = T$), this becomes:\n$$\n\\langle Tx, Tx \\rangle = \\|Tx\\|^2.\n$$\nSubstituting this back into our expression for the quadratic form:\n$$\n\\langle (I - T^2)x, x \\rangle = \\|x\\|^2 - \\|Tx\\|^2.\n$$\n\n### 4. Application of the Norm Bound\nWe are given that $\\|T\\| \\leq 1$. By the definition of the operator norm, for any $x \\in H$:\n$$\n\\|Tx\\| \\leq \\|T\\| \\|x\\|.\n$$\nSubstituting $\\|T\\| \\leq 1$:\n$$\n\\|Tx\\| \\leq 1 \\cdot \\|x\\| = \\|x\\|.\n$$\nSince norms are non-negative, we can square both sides of the inequality:\n$$\n\\|Tx\\|^2 \\leq \\|x\\|^2.\n$$\nConsequently:\n$$\n\\|x\\|^2 - \\|Tx\\|^2 \\geq 0.\n$$\n\n### Conclusion\nCombining the results from steps 3 and 4, we have shown that for all $x \\in H$:\n$$\n\\langle (I - T^2)x, x \\rangle \\geq 0.\n$$\nSince $I - T^2$ is self-adjoint and satisfies the non-negativity condition for the inner product, the operator $I - T^2$ is positive.",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ],
    "outer_attempt": 1
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "Let $T: L^2[0,1] \\to L^2[0,1]$ be defined by $Tf(x) = \\int_{0}^{x} f(t) dt$. Prove that $T$ is compact and find its spectrum.",
    "run_id": "33",
    "topic_slug": "operator_theory",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "To solve the problem regarding the Volterra operator $T: L^2[0,1] \\to L^2[0,1]$ defined by \n$$ (Tf)(x) = \\int_{0}^{x} f(t) \\, dt, $$\nwe proceed in two parts: first proving that $T$ is a compact operator, and then determining its spectrum $\\sigma(T)$. We treat $L^2[0,1]$ as a complex Hilbert space.\n\n---\n\n### Part 1: Proof of Compactness\n\nWe prove that $T$ is compact by showing it is a Hilbert-Schmidt operator.\n\n**Step 1: Express $T$ as an Integral Operator**\nThe operator $T$ can be written in the form of a Fredholm integral operator:\n$$ (Tf)(x) = \\int_{0}^{1} K(x,t) f(t) \\, dt, $$\nwhere the kernel $K(x,t)$ is defined by the characteristic function of the region $0 \\le t \\le x \\le 1$:\n$$ K(x,t) = \\begin{cases} \n1 & \\text{if } 0 \\le t \\le x, \\\\\n0 & \\text{if } x < t \\le 1.\n\\end{cases} $$\n\n**Step 2: Verify the Hilbert-Schmidt Condition**\nA linear integral operator on $L^2[0,1]$ is a Hilbert-Schmidt operator if its kernel $K$ belongs to $L^2([0,1] \\times [0,1])$. We compute the $L^2$ norm of the kernel:\n$$ \\|K\\|_{L^2}^2 = \\int_{0}^{1} \\int_{0}^{1} |K(x,t)|^2 \\, dt \\, dx. $$\nSubstituting the definition of $K(x,t)$:\n$$ \\|K\\|_{L^2}^2 = \\int_{0}^{1} \\left( \\int_{0}^{x} 1^2 \\, dt \\right) dx = \\int_{0}^{1} [t]_{0}^{x} \\, dx = \\int_{0}^{1} x \\, dx. $$\nEvaluating the final integral:\n$$ \\int_{0}^{1} x \\, dx = \\left[ \\frac{1}{2}x^2 \\right]_{0}^{1} = \\frac{1}{2}. $$\nSince $\\|K\\|_{L^2}^2 = \\frac{1}{2} < \\infty$, the kernel $K$ is square-integrable.\n\n**Conclusion for Part 1**\nEvery Hilbert-Schmidt operator on a separable Hilbert space is a compact operator. Therefore, $T$ is compact.\n\n---\n\n### Part 2: Determination of the Spectrum\n\nLet $\\sigma(T)$ denote the spectrum of $T$. Since $T$ is a compact operator on an infinite-dimensional Banach space, the Riesz-Schauder theorem applies. This theorem states:\n1. $0 \\in \\sigma(T)$.\n2. The non-zero elements of the spectrum, $\\sigma(T) \\setminus \\{0\\}$, consist entirely of eigenvalues (the point spectrum $\\sigma_p(T)$).\n\nThus, to find $\\sigma(T)$, we must determine if $0$ is an eigenvalue and find all non-zero eigenvalues.\n\n**Step 1: Check if $\\lambda = 0$ is an eigenvalue**\nWe check for non-trivial solutions to $Tf = 0$:\n$$ \\int_{0}^{x} f(t) \\, dt = 0 \\quad \\text{for almost every } x \\in [0,1]. $$\nSince $f \\in L^2[0,1] \\subset L^1[0,1]$, the integral $F(x) = \\int_0^x f(t) dt$ is absolutely continuous. Differentiating both sides with respect to $x$ (using the Lebesgue differentiation theorem) gives:\n$$ f(x) = 0 \\quad \\text{almost everywhere}. $$\nThus, the kernel of $T$ is trivial ($\\text{Ker}(T) = \\{0\\}$), so $\\lambda = 0$ is **not** an eigenvalue.\nHowever, as noted above, $0 \\in \\sigma(T)$ because $T$ is a compact operator on an infinite-dimensional space (implying $T$ cannot be boundedly invertible).\n\n**Step 2: Check for eigenvalues $\\lambda \\neq 0$**\nSuppose there exists $\\lambda \\in \\mathbb{C} \\setminus \\{0\\}$ and a non-zero function $f \\in L^2[0,1]$ such that $Tf = \\lambda f$.\n$$ \\int_{0}^{x} f(t) \\, dt = \\lambda f(x). \\quad (*)$$\n\n*Regularity Argument:*\nSince $f \\in L^2[0,1]$, the left-hand side of $(*)$ is an absolutely continuous function of $x$. Therefore, $\\lambda f(x)$ is continuous. Since $\\lambda \\neq 0$, $f(x)$ must be continuous.\nSince $f$ is continuous, the integrand in the left-hand side is continuous, making the integral continuously differentiable ($C^1$). Therefore, $f$ must be a $C^1$ function.\n\n*Differential Equation:*\nWe can now differentiate $(*)$ with respect to $x$:\n$$ \\frac{d}{dx} \\int_{0}^{x} f(t) \\, dt = \\frac{d}{dx} (\\lambda f(x)) $$\n$$ f(x) = \\lambda f'(x). $$\nThis can be rewritten as a first-order linear ordinary differential equation:\n$$ f'(x) - \\frac{1}{\\lambda} f(x) = 0. $$\nThe general solution is:\n$$ f(x) = C e^{x/\\lambda}, $$\nwhere $C$ is an arbitrary constant.\n\n*Boundary Condition:*\nWe must ensure the solution satisfies the original integral equation $(*)$ at a specific point, typically $x=0$, to determine $C$.\nEvaluating $(*)$ at $x=0$:\n$$ \\text{LHS} = \\int_{0}^{0} f(t) \\, dt = 0, $$\n$$ \\text{RHS} = \\lambda f(0). $$\nSince $\\lambda \\neq 0$, we must have $f(0) = 0$.\n\nSubstituting the general solution into this boundary condition:\n$$ f(0) = C e^{0/\\lambda} = C \\cdot 1 = C. $$\nThus, $C = 0$.\n\nThis implies $f(x) = 0$ for all $x \\in [0,1]$. Since an eigenvector must be non-zero, there are no eigenvalues $\\lambda \\neq 0$.\n\n**Conclusion for Part 2**\nThe point spectrum of $T$ is empty ($\\sigma_p(T) = \\emptyset$). Since $T$ is compact and the space is infinite-dimensional, the spectrum contains $0$ but no other values.\n\n### Final Answer\n\n1. The operator $T$ is **compact** because it is a Hilbert-Schmidt operator with kernel norm $\\|K\\|_{L^2} = 1/\\sqrt{2}$.\n2. The spectrum of $T$ is **$\\sigma(T) = \\{0\\}$**.",
        "raw_answer": "To solve the problem regarding the Volterra operator $T: L^2[0,1] \\to L^2[0,1]$ defined by \n$$ (Tf)(x) = \\int_{0}^{x} f(t) \\, dt, $$\nwe proceed in two parts: first proving that $T$ is a compact operator, and then determining its spectrum $\\sigma(T)$. We treat $L^2[0,1]$ as a complex Hilbert space.\n\n---\n\n### Part 1: Proof of Compactness\n\nWe prove that $T$ is compact by showing it is a Hilbert-Schmidt operator.\n\n**Step 1: Express $T$ as an Integral Operator**\nThe operator $T$ can be written in the form of a Fredholm integral operator:\n$$ (Tf)(x) = \\int_{0}^{1} K(x,t) f(t) \\, dt, $$\nwhere the kernel $K(x,t)$ is defined by the characteristic function of the region $0 \\le t \\le x \\le 1$:\n$$ K(x,t) = \\begin{cases} \n1 & \\text{if } 0 \\le t \\le x, \\\\\n0 & \\text{if } x < t \\le 1.\n\\end{cases} $$\n\n**Step 2: Verify the Hilbert-Schmidt Condition**\nA linear integral operator on $L^2[0,1]$ is a Hilbert-Schmidt operator if its kernel $K$ belongs to $L^2([0,1] \\times [0,1])$. We compute the $L^2$ norm of the kernel:\n$$ \\|K\\|_{L^2}^2 = \\int_{0}^{1} \\int_{0}^{1} |K(x,t)|^2 \\, dt \\, dx. $$\nSubstituting the definition of $K(x,t)$:\n$$ \\|K\\|_{L^2}^2 = \\int_{0}^{1} \\left( \\int_{0}^{x} 1^2 \\, dt \\right) dx = \\int_{0}^{1} [t]_{0}^{x} \\, dx = \\int_{0}^{1} x \\, dx. $$\nEvaluating the final integral:\n$$ \\int_{0}^{1} x \\, dx = \\left[ \\frac{1}{2}x^2 \\right]_{0}^{1} = \\frac{1}{2}. $$\nSince $\\|K\\|_{L^2}^2 = \\frac{1}{2} < \\infty$, the kernel $K$ is square-integrable.\n\n**Conclusion for Part 1**\nEvery Hilbert-Schmidt operator on a separable Hilbert space is a compact operator. Therefore, $T$ is compact.\n\n---\n\n### Part 2: Determination of the Spectrum\n\nLet $\\sigma(T)$ denote the spectrum of $T$. Since $T$ is a compact operator on an infinite-dimensional Banach space, the Riesz-Schauder theorem applies. This theorem states:\n1. $0 \\in \\sigma(T)$.\n2. The non-zero elements of the spectrum, $\\sigma(T) \\setminus \\{0\\}$, consist entirely of eigenvalues (the point spectrum $\\sigma_p(T)$).\n\nThus, to find $\\sigma(T)$, we must determine if $0$ is an eigenvalue and find all non-zero eigenvalues.\n\n**Step 1: Check if $\\lambda = 0$ is an eigenvalue**\nWe check for non-trivial solutions to $Tf = 0$:\n$$ \\int_{0}^{x} f(t) \\, dt = 0 \\quad \\text{for almost every } x \\in [0,1]. $$\nSince $f \\in L^2[0,1] \\subset L^1[0,1]$, the integral $F(x) = \\int_0^x f(t) dt$ is absolutely continuous. Differentiating both sides with respect to $x$ (using the Lebesgue differentiation theorem) gives:\n$$ f(x) = 0 \\quad \\text{almost everywhere}. $$\nThus, the kernel of $T$ is trivial ($\\text{Ker}(T) = \\{0\\}$), so $\\lambda = 0$ is **not** an eigenvalue.\nHowever, as noted above, $0 \\in \\sigma(T)$ because $T$ is a compact operator on an infinite-dimensional space (implying $T$ cannot be boundedly invertible).\n\n**Step 2: Check for eigenvalues $\\lambda \\neq 0$**\nSuppose there exists $\\lambda \\in \\mathbb{C} \\setminus \\{0\\}$ and a non-zero function $f \\in L^2[0,1]$ such that $Tf = \\lambda f$.\n$$ \\int_{0}^{x} f(t) \\, dt = \\lambda f(x). \\quad (*)$$\n\n*Regularity Argument:*\nSince $f \\in L^2[0,1]$, the left-hand side of $(*)$ is an absolutely continuous function of $x$. Therefore, $\\lambda f(x)$ is continuous. Since $\\lambda \\neq 0$, $f(x)$ must be continuous.\nSince $f$ is continuous, the integrand in the left-hand side is continuous, making the integral continuously differentiable ($C^1$). Therefore, $f$ must be a $C^1$ function.\n\n*Differential Equation:*\nWe can now differentiate $(*)$ with respect to $x$:\n$$ \\frac{d}{dx} \\int_{0}^{x} f(t) \\, dt = \\frac{d}{dx} (\\lambda f(x)) $$\n$$ f(x) = \\lambda f'(x). $$\nThis can be rewritten as a first-order linear ordinary differential equation:\n$$ f'(x) - \\frac{1}{\\lambda} f(x) = 0. $$\nThe general solution is:\n$$ f(x) = C e^{x/\\lambda}, $$\nwhere $C$ is an arbitrary constant.\n\n*Boundary Condition:*\nWe must ensure the solution satisfies the original integral equation $(*)$ at a specific point, typically $x=0$, to determine $C$.\nEvaluating $(*)$ at $x=0$:\n$$ \\text{LHS} = \\int_{0}^{0} f(t) \\, dt = 0, $$\n$$ \\text{RHS} = \\lambda f(0). $$\nSince $\\lambda \\neq 0$, we must have $f(0) = 0$.\n\nSubstituting the general solution into this boundary condition:\n$$ f(0) = C e^{0/\\lambda} = C \\cdot 1 = C. $$\nThus, $C = 0$.\n\nThis implies $f(x) = 0$ for all $x \\in [0,1]$. Since an eigenvector must be non-zero, there are no eigenvalues $\\lambda \\neq 0$.\n\n**Conclusion for Part 2**\nThe point spectrum of $T$ is empty ($\\sigma_p(T) = \\emptyset$). Since $T$ is compact and the space is infinite-dimensional, the spectrum contains $0$ but no other values.\n\n### Final Answer\n\n1. The operator $T$ is **compact** because it is a Hilbert-Schmidt operator with kernel norm $\\|K\\|_{L^2} = 1/\\sqrt{2}$.\n2. The spectrum of $T$ is **$\\sigma(T) = \\{0\\}$**.",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ],
    "outer_attempt": 1
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "## Step 1: Understand the task\nThe task is to generate a challenging mathematical problem in the domain of calculus of variations and optimal control, along with a complete solution. The problem should be solvable, verifiable, and adhere to the specified format.",
    "run_id": "34",
    "topic_slug": "calculus_of_variations_and_optimization",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "Here is a challenging mathematical problem in the domain of **Calculus of Variations**, followed by a complete and rigorous solution.\n\n***\n\n### **Problem Statement**\n\nFind the function $y(x)$ continuously differentiable on the interval $[0, 1]$ that minimizes the functional:\n$$ J[y] = \\int_{0}^{1} \\left( (y'(x))^2 + y(x)^2 \\right) \\, dx $$\nsubject to the boundary conditions:\n$$ y(0) = 0, \\quad y(1) = 0 $$\nand the isoperimetric constraint:\n$$ \\int_{0}^{1} y(x) \\, dx = 1 $$\nProvide the explicit analytical expression for the optimal curve $y(x)$ and calculate the minimum value of the functional $J[y]$.\n\n***\n\n### **Solution**\n\nTo solve this constrained optimization problem, we employ the method of Lagrange multipliers in the calculus of variations.\n\n#### **1. Formulation of the Auxiliary Functional**\n\nWe introduce a Lagrange multiplier constant $\\lambda$ associated with the integral constraint. We define the auxiliary functional $L[y]$:\n$$ L[y] = \\int_{0}^{1} \\left( (y')^2 + y^2 \\right) \\, dx + \\lambda \\left( \\int_{0}^{1} y \\, dx - 1 \\right) $$\nThis can be rewritten as maximizing/minimizing the integral of the auxiliary Lagrangian $\\mathcal{F}$:\n$$ L[y] = \\int_{0}^{1} \\mathcal{F}(x, y, y') \\, dx - \\lambda $$\nwhere $\\mathcal{F}(x, y, y') = (y')^2 + y^2 + \\lambda y$.\n\n#### **2. The Euler-Lagrange Equation**\n\nA necessary condition for $y(x)$ to be an extremal is that it satisfies the Euler-Lagrange equation:\n$$ \\frac{\\partial \\mathcal{F}}{\\partial y} - \\frac{d}{dx} \\left( \\frac{\\partial \\mathcal{F}}{\\partial y'} \\right) = 0 $$\nCalculating the partial derivatives:\n$$ \\frac{\\partial \\mathcal{F}}{\\partial y} = 2y + \\lambda $$\n$$ \\frac{\\partial \\mathcal{F}}{\\partial y'} = 2y' $$\nSubstitute these into the equation:\n$$ (2y + \\lambda) - \\frac{d}{dx}(2y') = 0 $$\n$$ 2y + \\lambda - 2y'' = 0 $$\nDividing by 2, we obtain the non-homogeneous linear differential equation:\n$$ y'' - y = \\frac{\\lambda}{2} $$\n\n#### **3. Solving the Differential Equation**\n\nThe general solution to the differential equation $y'' - y = \\frac{\\lambda}{2}$ consists of the homogeneous solution $y_h$ and a particular solution $y_p$.\n\n**Homogeneous solution** ($y'' - y = 0$):\n$$ y_h(x) = c_1 \\cosh(x) + c_2 \\sinh(x) $$\n\n**Particular solution**:\nSince the right-hand side is a constant, we posit a constant particular solution $y_p = A$.\n$$ 0 - A = \\frac{\\lambda}{2} \\implies A = -\\frac{\\lambda}{2} $$\n\nThus, the general solution is:\n$$ y(x) = c_1 \\cosh(x) + c_2 \\sinh(x) - \\frac{\\lambda}{2} $$\n\n#### **4. Applying Boundary Conditions**\n\nWe apply the boundary conditions $y(0)=0$ and $y(1)=0$ to determine constants $c_1$ and $c_2$ in terms of $\\lambda$.\n\n**At $x=0$:**\n$$ y(0) = c_1 \\cosh(0) + c_2 \\sinh(0) - \\frac{\\lambda}{2} = 0 $$\n$$ c_1 (1) + 0 - \\frac{\\lambda}{2} = 0 \\implies c_1 = \\frac{\\lambda}{2} $$\n\n**At $x=1$:**\n$$ y(1) = c_1 \\cosh(1) + c_2 \\sinh(1) - \\frac{\\lambda}{2} = 0 $$\nSubstitute $c_1 = \\frac{\\lambda}{2}$:\n$$ \\frac{\\lambda}{2} \\cosh(1) + c_2 \\sinh(1) - \\frac{\\lambda}{2} = 0 $$\n$$ c_2 \\sinh(1) = \\frac{\\lambda}{2} (1 - \\cosh(1)) $$\n$$ c_2 = \\frac{\\lambda}{2} \\frac{1 - \\cosh(1)}{\\sinh(1)} $$\n\nUsing the half-angle hyperbolic identities $\\cosh(x) - 1 = 2\\sinh^2(x/2)$ and $\\sinh(x) = 2\\sinh(x/2)\\cosh(x/2)$:\n$$ \\frac{1 - \\cosh(1)}{\\sinh(1)} = \\frac{-2\\sinh^2(1/2)}{2\\sinh(1/2)\\cosh(1/2)} = -\\tanh(1/2) $$\nSo,\n$$ c_2 = -\\frac{\\lambda}{2} \\tanh(1/2) $$\n\nSubstituting $c_1$ and $c_2$ back into the expression for $y(x)$:\n$$ y(x) = \\frac{\\lambda}{2} \\left[ \\cosh(x) - \\tanh(1/2)\\sinh(x) - 1 \\right] $$\n\n#### **5. Determining the Lagrange Multiplier**\n\nWe use the isoperimetric constraint $\\int_{0}^{1} y(x) \\, dx = 1$ to solve for $\\lambda$.\n$$ \\int_{0}^{1} \\frac{\\lambda}{2} \\left[ \\cosh(x) - \\tanh(1/2)\\sinh(x) - 1 \\right] dx = 1 $$\nFactor out $\\frac{\\lambda}{2}$:\n$$ \\frac{\\lambda}{2} \\left( \\left[ \\sinh(x) \\right]_0^1 - \\tanh(1/2) \\left[ \\cosh(x) \\right]_0^1 - \\left[ x \\right]_0^1 \\right) = 1 $$\nEvaluate the terms:\n1.  $\\left[ \\sinh(x) \\right]_0^1 = \\sinh(1)$\n2.  $\\left[ \\cosh(x) \\right]_0^1 = \\cosh(1) - 1$\n3.  $\\left[ x \\right]_0^1 = 1$\n\nThe equation becomes:\n$$ \\frac{\\lambda}{2} \\left( \\sinh(1) - \\frac{\\cosh(1)-1}{\\sinh(1)}(\\cosh(1)-1) - 1 \\right) = 1 $$\nSimplify the term inside the parenthesis:\n$$ \\sinh(1) - 1 - \\frac{(\\cosh(1)-1)^2}{\\sinh(1)} $$\nFind a common denominator ($\\sinh(1)$):\n$$ \\frac{\\sinh^2(1) - \\sinh(1) - (\\cosh^2(1) - 2\\cosh(1) + 1)}{\\sinh(1)} $$\nRecall the identity $\\cosh^2(x) - \\sinh^2(x) = 1$, so $\\sinh^2(x) - \\cosh^2(x) = -1$:\n$$ \\frac{-1 - \\sinh(1) + 2\\cosh(1) - 1}{\\sinh(1)} = \\frac{2\\cosh(1) - \\sinh(1) - 2}{\\sinh(1)} $$\n\nThus, the constraint equation is:\n$$ \\frac{\\lambda}{2} \\left( \\frac{2\\cosh(1) - \\sinh(1) - 2}{\\sinh(1)} \\right) = 1 $$\n$$ \\lambda = \\frac{2\\sinh(1)}{2\\cosh(1) - \\sinh(1) - 2} $$\n\n#### **6. Final Expression for $y(x)$**\n\nSubstitute $\\lambda$ back into the expression for $y(x)$:\n$$ y(x) = \\frac{\\sinh(1)}{2\\cosh(1) - \\sinh(1) - 2} \\left[ \\cosh(x) - \\frac{\\cosh(1)-1}{\\sinh(1)}\\sinh(x) - 1 \\right] $$\nThis simplifies slightly by grouping the hyperbolic terms. Let $K = \\frac{\\lambda}{2}$.\n$$ y(x) = K \\left[ \\cosh(x) - \\tanh(1/2)\\sinh(x) - 1 \\right] $$\nwhere $K = \\frac{\\sinh(1)}{2\\cosh(1) - \\sinh(1) - 2}$.\n\n#### **7. Calculation of the Minimum Functional Value**\n\nWe calculate $J[y]$. Note that the Euler-Lagrange equation implies $y'' = y + \\frac{\\lambda}{2}$.\nIntegration by parts on the first term of $J$:\n$$ \\int_0^1 (y')^2 dx = [y y']_0^1 - \\int_0^1 y y'' dx $$\nSince $y(0)=y(1)=0$, the boundary term $[y y']_0^1$ vanishes.\n$$ \\int_0^1 (y')^2 dx = - \\int_0^1 y (y + \\frac{\\lambda}{2}) dx = - \\int_0^1 (y^2 + \\frac{\\lambda}{2} y) dx $$\nSubstitute this back into $J[y]$:\n$$ J[y] = \\int_0^1 \\left( -y^2 - \\frac{\\lambda}{2} y + y^2 \\right) dx = -\\frac{\\lambda}{2} \\int_0^1 y \\, dx $$\nUsing the constraint $\\int_0^1 y \\, dx = 1$:\n$$ J[y] = -\\frac{\\lambda}{2} $$\nSubstituting the value of $\\lambda$:\n$$ J_{min} = -K = -\\frac{\\sinh(1)}{2\\cosh(1) - \\sinh(1) - 2} $$\nUsing numerical values ($\\sinh(1) \\approx 1.1752$, $\\cosh(1) \\approx 1.5431$):\n$$ 2(1.5431) - 1.1752 - 2 = 3.0862 - 3.1752 = -0.089 $$\n$$ J_{min} \\approx -\\frac{1.1752}{-0.089} \\approx 13.2 $$\n(Exact values should be preserved).\n\n**Check for Minimality:**\nThe integrand $\\mathcal{F} = y'^2 + y^2 + \\lambda y$ is convex with respect to $y$ and $y'$ (Hessian with respect to $y, y'$ is positive definite diagonal matrix $\\text{diag}(2, 2)$). Therefore, the extremal found is a global minimum.\n\n### **Final Answer**\n\nThe optimal curve is:\n$$ y(x) = \\frac{\\sinh(1)}{2\\cosh(1) - \\sinh(1) - 2} \\left( \\cosh(x) - \\tanh\\left(\\frac{1}{2}\\right)\\sinh(x) - 1 \\right) $$\nThe minimum value of the functional is:\n$$ J_{min} = \\frac{\\sinh(1)}{2 + \\sinh(1) - 2\\cosh(1)} $$",
        "raw_answer": "Here is a challenging mathematical problem in the domain of **Calculus of Variations**, followed by a complete and rigorous solution.\n\n***\n\n### **Problem Statement**\n\nFind the function $y(x)$ continuously differentiable on the interval $[0, 1]$ that minimizes the functional:\n$$ J[y] = \\int_{0}^{1} \\left( (y'(x))^2 + y(x)^2 \\right) \\, dx $$\nsubject to the boundary conditions:\n$$ y(0) = 0, \\quad y(1) = 0 $$\nand the isoperimetric constraint:\n$$ \\int_{0}^{1} y(x) \\, dx = 1 $$\nProvide the explicit analytical expression for the optimal curve $y(x)$ and calculate the minimum value of the functional $J[y]$.\n\n***\n\n### **Solution**\n\nTo solve this constrained optimization problem, we employ the method of Lagrange multipliers in the calculus of variations.\n\n#### **1. Formulation of the Auxiliary Functional**\n\nWe introduce a Lagrange multiplier constant $\\lambda$ associated with the integral constraint. We define the auxiliary functional $L[y]$:\n$$ L[y] = \\int_{0}^{1} \\left( (y')^2 + y^2 \\right) \\, dx + \\lambda \\left( \\int_{0}^{1} y \\, dx - 1 \\right) $$\nThis can be rewritten as maximizing/minimizing the integral of the auxiliary Lagrangian $\\mathcal{F}$:\n$$ L[y] = \\int_{0}^{1} \\mathcal{F}(x, y, y') \\, dx - \\lambda $$\nwhere $\\mathcal{F}(x, y, y') = (y')^2 + y^2 + \\lambda y$.\n\n#### **2. The Euler-Lagrange Equation**\n\nA necessary condition for $y(x)$ to be an extremal is that it satisfies the Euler-Lagrange equation:\n$$ \\frac{\\partial \\mathcal{F}}{\\partial y} - \\frac{d}{dx} \\left( \\frac{\\partial \\mathcal{F}}{\\partial y'} \\right) = 0 $$\nCalculating the partial derivatives:\n$$ \\frac{\\partial \\mathcal{F}}{\\partial y} = 2y + \\lambda $$\n$$ \\frac{\\partial \\mathcal{F}}{\\partial y'} = 2y' $$\nSubstitute these into the equation:\n$$ (2y + \\lambda) - \\frac{d}{dx}(2y') = 0 $$\n$$ 2y + \\lambda - 2y'' = 0 $$\nDividing by 2, we obtain the non-homogeneous linear differential equation:\n$$ y'' - y = \\frac{\\lambda}{2} $$\n\n#### **3. Solving the Differential Equation**\n\nThe general solution to the differential equation $y'' - y = \\frac{\\lambda}{2}$ consists of the homogeneous solution $y_h$ and a particular solution $y_p$.\n\n**Homogeneous solution** ($y'' - y = 0$):\n$$ y_h(x) = c_1 \\cosh(x) + c_2 \\sinh(x) $$\n\n**Particular solution**:\nSince the right-hand side is a constant, we posit a constant particular solution $y_p = A$.\n$$ 0 - A = \\frac{\\lambda}{2} \\implies A = -\\frac{\\lambda}{2} $$\n\nThus, the general solution is:\n$$ y(x) = c_1 \\cosh(x) + c_2 \\sinh(x) - \\frac{\\lambda}{2} $$\n\n#### **4. Applying Boundary Conditions**\n\nWe apply the boundary conditions $y(0)=0$ and $y(1)=0$ to determine constants $c_1$ and $c_2$ in terms of $\\lambda$.\n\n**At $x=0$:**\n$$ y(0) = c_1 \\cosh(0) + c_2 \\sinh(0) - \\frac{\\lambda}{2} = 0 $$\n$$ c_1 (1) + 0 - \\frac{\\lambda}{2} = 0 \\implies c_1 = \\frac{\\lambda}{2} $$\n\n**At $x=1$:**\n$$ y(1) = c_1 \\cosh(1) + c_2 \\sinh(1) - \\frac{\\lambda}{2} = 0 $$\nSubstitute $c_1 = \\frac{\\lambda}{2}$:\n$$ \\frac{\\lambda}{2} \\cosh(1) + c_2 \\sinh(1) - \\frac{\\lambda}{2} = 0 $$\n$$ c_2 \\sinh(1) = \\frac{\\lambda}{2} (1 - \\cosh(1)) $$\n$$ c_2 = \\frac{\\lambda}{2} \\frac{1 - \\cosh(1)}{\\sinh(1)} $$\n\nUsing the half-angle hyperbolic identities $\\cosh(x) - 1 = 2\\sinh^2(x/2)$ and $\\sinh(x) = 2\\sinh(x/2)\\cosh(x/2)$:\n$$ \\frac{1 - \\cosh(1)}{\\sinh(1)} = \\frac{-2\\sinh^2(1/2)}{2\\sinh(1/2)\\cosh(1/2)} = -\\tanh(1/2) $$\nSo,\n$$ c_2 = -\\frac{\\lambda}{2} \\tanh(1/2) $$\n\nSubstituting $c_1$ and $c_2$ back into the expression for $y(x)$:\n$$ y(x) = \\frac{\\lambda}{2} \\left[ \\cosh(x) - \\tanh(1/2)\\sinh(x) - 1 \\right] $$\n\n#### **5. Determining the Lagrange Multiplier**\n\nWe use the isoperimetric constraint $\\int_{0}^{1} y(x) \\, dx = 1$ to solve for $\\lambda$.\n$$ \\int_{0}^{1} \\frac{\\lambda}{2} \\left[ \\cosh(x) - \\tanh(1/2)\\sinh(x) - 1 \\right] dx = 1 $$\nFactor out $\\frac{\\lambda}{2}$:\n$$ \\frac{\\lambda}{2} \\left( \\left[ \\sinh(x) \\right]_0^1 - \\tanh(1/2) \\left[ \\cosh(x) \\right]_0^1 - \\left[ x \\right]_0^1 \\right) = 1 $$\nEvaluate the terms:\n1.  $\\left[ \\sinh(x) \\right]_0^1 = \\sinh(1)$\n2.  $\\left[ \\cosh(x) \\right]_0^1 = \\cosh(1) - 1$\n3.  $\\left[ x \\right]_0^1 = 1$\n\nThe equation becomes:\n$$ \\frac{\\lambda}{2} \\left( \\sinh(1) - \\frac{\\cosh(1)-1}{\\sinh(1)}(\\cosh(1)-1) - 1 \\right) = 1 $$\nSimplify the term inside the parenthesis:\n$$ \\sinh(1) - 1 - \\frac{(\\cosh(1)-1)^2}{\\sinh(1)} $$\nFind a common denominator ($\\sinh(1)$):\n$$ \\frac{\\sinh^2(1) - \\sinh(1) - (\\cosh^2(1) - 2\\cosh(1) + 1)}{\\sinh(1)} $$\nRecall the identity $\\cosh^2(x) - \\sinh^2(x) = 1$, so $\\sinh^2(x) - \\cosh^2(x) = -1$:\n$$ \\frac{-1 - \\sinh(1) + 2\\cosh(1) - 1}{\\sinh(1)} = \\frac{2\\cosh(1) - \\sinh(1) - 2}{\\sinh(1)} $$\n\nThus, the constraint equation is:\n$$ \\frac{\\lambda}{2} \\left( \\frac{2\\cosh(1) - \\sinh(1) - 2}{\\sinh(1)} \\right) = 1 $$\n$$ \\lambda = \\frac{2\\sinh(1)}{2\\cosh(1) - \\sinh(1) - 2} $$\n\n#### **6. Final Expression for $y(x)$**\n\nSubstitute $\\lambda$ back into the expression for $y(x)$:\n$$ y(x) = \\frac{\\sinh(1)}{2\\cosh(1) - \\sinh(1) - 2} \\left[ \\cosh(x) - \\frac{\\cosh(1)-1}{\\sinh(1)}\\sinh(x) - 1 \\right] $$\nThis simplifies slightly by grouping the hyperbolic terms. Let $K = \\frac{\\lambda}{2}$.\n$$ y(x) = K \\left[ \\cosh(x) - \\tanh(1/2)\\sinh(x) - 1 \\right] $$\nwhere $K = \\frac{\\sinh(1)}{2\\cosh(1) - \\sinh(1) - 2}$.\n\n#### **7. Calculation of the Minimum Functional Value**\n\nWe calculate $J[y]$. Note that the Euler-Lagrange equation implies $y'' = y + \\frac{\\lambda}{2}$.\nIntegration by parts on the first term of $J$:\n$$ \\int_0^1 (y')^2 dx = [y y']_0^1 - \\int_0^1 y y'' dx $$\nSince $y(0)=y(1)=0$, the boundary term $[y y']_0^1$ vanishes.\n$$ \\int_0^1 (y')^2 dx = - \\int_0^1 y (y + \\frac{\\lambda}{2}) dx = - \\int_0^1 (y^2 + \\frac{\\lambda}{2} y) dx $$\nSubstitute this back into $J[y]$:\n$$ J[y] = \\int_0^1 \\left( -y^2 - \\frac{\\lambda}{2} y + y^2 \\right) dx = -\\frac{\\lambda}{2} \\int_0^1 y \\, dx $$\nUsing the constraint $\\int_0^1 y \\, dx = 1$:\n$$ J[y] = -\\frac{\\lambda}{2} $$\nSubstituting the value of $\\lambda$:\n$$ J_{min} = -K = -\\frac{\\sinh(1)}{2\\cosh(1) - \\sinh(1) - 2} $$\nUsing numerical values ($\\sinh(1) \\approx 1.1752$, $\\cosh(1) \\approx 1.5431$):\n$$ 2(1.5431) - 1.1752 - 2 = 3.0862 - 3.1752 = -0.089 $$\n$$ J_{min} \\approx -\\frac{1.1752}{-0.089} \\approx 13.2 $$\n(Exact values should be preserved).\n\n**Check for Minimality:**\nThe integrand $\\mathcal{F} = y'^2 + y^2 + \\lambda y$ is convex with respect to $y$ and $y'$ (Hessian with respect to $y, y'$ is positive definite diagonal matrix $\\text{diag}(2, 2)$). Therefore, the extremal found is a global minimum.\n\n### **Final Answer**\n\nThe optimal curve is:\n$$ y(x) = \\frac{\\sinh(1)}{2\\cosh(1) - \\sinh(1) - 2} \\left( \\cosh(x) - \\tanh\\left(\\frac{1}{2}\\right)\\sinh(x) - 1 \\right) $$\nThe minimum value of the functional is:\n$$ J_{min} = \\frac{\\sinh(1)}{2 + \\sinh(1) - 2\\cosh(1)} $$",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ],
    "outer_attempt": 1
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "## Step 1: Understand the task\nThe task is to generate a challenging but solvable mathematics problem in the domain of Geometry, along with a complete and verifiable solution. The problem must be well-posed, clear, non-trivial, and solvable with established mathematical methods.\n\n## Step 2: Choose a geometry topic\nTo create a challenging problem, we need to select a topic within geometry that is rich enough to allow for a non-trivial question. Let's consider problems related to geometric shapes, such as triangles, circles, or more complex figures.\n\n## Step 3: Formulate a problem\nLet's formulate a problem involving a triangle and a circle, as these are fundamental geometric objects. A potential problem could involve the properties of inscribed angles, the relationship between a triangle and its circumcircle, or the application of geometric theorems like Ptolemy's theorem.\n\n## Step 4: Create a specific problem\nConsider the following problem: \"Let $ABC$ be a triangle with $AB = 5$, $AC = 6$, and $BC = 7$. The circumcircle of triangle $ABC$ has center $O$. Let $D$ be the point on $BC$ such that $AD$ is the angle bisector of $\\angle BAC$. Find the length of $AD$.\"\n\n## Step 5: Verify the problem meets the criteria\n- **Well-Posedness and Completeness**: The triangle $ABC$ is well-defined by its side lengths (5, 6, 7), and $D$ is defined as the point on $BC$ where $AD$ bisects $\\angle BAC$. The circumcircle and its center $O$ are also well-defined.\n- **Clarity**: The problem statement is clear and unambiguous.\n- **Non-Triviality**: The problem requires applying geometric principles, such as the Angle Bisector Theorem and possibly properties of the circumcircle.\n- **Solvability and Verification**: The problem is solvable using established geometric methods.\n\n## Step 6: Solve the problem\nTo solve this, we can use the Angle Bisector Theorem to find $BD$ and $DC$, and then apply the Stewart's Theorem to find $AD$.\n\n### Step 6.1: Apply the Angle Bisector Theorem\nThe Angle Bisector Theorem states that $\\frac{BD}{DC} = \\frac{AB}{AC}$. So, $\\frac{BD}{DC} = \\frac{5}{6}$.\n\n### Step 6.2: Calculate $BD$ and $DC$\nGiven $BC = 7$, we have $BD + DC = 7$. From the Angle Bisector Theorem, we can write $BD = \\frac{5}{11} \\times 7$ and $DC = \\frac{6}{11} \\times 7$. Thus, $BD = \\frac{35}{11}$ and $DC = \\frac{42}{11}$.\n\n### Step 6.3: Apply Stewart's Theorem to find $AD$\nStewart's Theorem states that $b^2n + c^2m = a(d^2 + m*n)$, where $a = BC = 7$, $b = AC = 6$, $c = AB = 5$, $d = AD$, $m = BD = \\frac{35}{11}$, and $n = DC = \\frac{42}{11}$. Substituting these values gives us an equation to solve for $d = AD$.\n\n## Step 7: Perform Stewart's Theorem calculation\nLet's substitute the given values into Stewart's Theorem:\n$6^2(\\frac{35}{11}) + 5^2(\\frac{42}{11}) = 7(AD^2 + \\frac{35}{11}*\\frac{42}{11})$.\nSimplifying yields:\n$36(\\frac{35}{11}) + 25(\\frac{42}{11}) = 7(AD^2 + \\frac{1470}{121})$.\nThis simplifies to:\n$\\frac{1260}{11} + \\frac{1050}{11} = 7AD^2 + \\frac{7*1470}{121}$.\nFurther simplification gives:\n$\\frac{2310}{11} = 7AD^2 + \\frac{10290}{121}$.\nConverting to a common denominator:\n$\\frac{2310*11}{121} = 7AD^2 + \\frac{10290}{121}$,\n$\\frac{25410}{121} = 7AD^2 + \\frac{10290}{121}$.\nSimplifying:\n$7AD^2 = \\frac{25410 - 10290}{121}$,\n$7AD^2 = \\frac{15120}{121}$,\n$AD^2 = \\frac{15120}{7*121}$,\n$AD^2 = \\frac{2160}{121}$,\n$AD = \\sqrt{\\frac{2160}{121}}$.\n\n## Step 8: Simplify $AD$\n$AD = \\sqrt{\\frac{2160}{121}} = \\sqrt{\\frac{16*135}{121}} = \\sqrt{\\frac{16*9*15}{121}} = \\frac{4*3}{11}\\sqrt{15} = \\frac{12\\sqrt{15}}{11}$.\n\nThe final answer is: \n\nLet $ABC$ be a triangle with $AB = 5$, $AC = 6$, and $BC = 7$. The circumcircle of triangle $ABC$ has center $O$. Let $D$ be the point on $BC$ such that $AD$ is the angle bisector of $\\angle BAC$. Find the length of $AD$.",
    "run_id": "35",
    "topic_slug": "geometry",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "To find the length of the segment $AD$, we will utilize the properties of the triangle defined by the side lengths provided, applying the Angle Bisector Theorem and the formula for the length of an internal angle bisector derived from Stewart's Theorem.\n\n### 1. Problem Setup and Definitions\n\nLet the vertices of the triangle be $A$, $B$, and $C$. The side lengths are given as:\n*   $c = AB = 5$\n*   $b = AC = 6$\n*   $a = BC = 7$\n\nPoint $D$ lies on the segment $BC$ such that $AD$ is the internal angle bisector of $\\angle BAC$. We aim to find the length $AD$.\n\n*(Note: The problem statement mentions the circumcenter $O$. While this establishes the geometric context, the position of the circumcenter is not required to determine the length of the internal angle bisector given the side lengths.)*\n\n### 2. Determining the Segments of BC\n\nAccording to the **Angle Bisector Theorem**, the internal bisector of an angle of a triangle divides the opposite side into two segments that are proportional to the other two sides of the triangle.\n\nLet $BD = m$ and $DC = n$. The theorem states:\n$$ \\frac{m}{n} = \\frac{c}{b} $$\n\nSubstituting the known values:\n$$ \\frac{m}{n} = \\frac{5}{6} $$\n\nSince $D$ lies on $BC$, we know that $m + n = a = 7$. We can set up the system of equations:\n1.  $6m = 5n$\n2.  $m + n = 7 \\implies n = 7 - m$\n\nSubstitute (2) into (1):\n$$ 6m = 5(7 - m) $$\n$$ 6m = 35 - 5m $$\n$$ 11m = 35 $$\n$$ m = BD = \\frac{35}{11} $$\n\nConsequently:\n$$ n = 7 - \\frac{35}{11} = \\frac{77 - 35}{11} = \\frac{42}{11} $$\nSo, $DC = \\frac{42}{11}$.\n\n### 3. Calculating the Length of AD\n\nWe can calculate the length of the angle bisector $AD$ using the standard formula derived from Stewart's Theorem. For an internal angle bisector of length $d_a$, the relationship is:\n$$ d_a^2 = bc - mn $$\n\n**Verification of the formula:**\nStewart's Theorem states that for a cevian of length $d$ dividing side $a$ into segments $m$ and $n$ (where $m$ is adjacent to $c$ and $n$ is adjacent to $b$):\n$$ b^2m + c^2n = a(d^2 + mn) $$\nSubstituting $m = \\frac{ac}{b+c}$ and $n = \\frac{ab}{b+c}$ leads to the simplified form $d^2 = bc - mn$.\n\n**Calculation:**\nSubstitute the values $b=6$, $c=5$, $m=\\frac{35}{11}$, and $n=\\frac{42}{11}$ into the equation:\n\n$$ AD^2 = (6)(5) - \\left(\\frac{35}{11}\\right)\\left(\\frac{42}{11}\\right) $$\n$$ AD^2 = 30 - \\frac{1470}{121} $$\n\nTo combine these terms, find a common denominator:\n$$ AD^2 = \\frac{30 \\times 121}{121} - \\frac{1470}{121} $$\n$$ AD^2 = \\frac{3630 - 1470}{121} $$\n$$ AD^2 = \\frac{2160}{121} $$\n\n### 4. Simplifying the Result\n\nNow, we take the square root of both sides to find $AD$:\n$$ AD = \\sqrt{\\frac{2160}{121}} $$\n$$ AD = \\frac{\\sqrt{2160}}{11} $$\n\nWe simplify the numerator $\\sqrt{2160}$ by prime factorization:\n$$ 2160 = 10 \\times 216 = (2 \\times 5) \\times (6^3) = 2 \\times 5 \\times 2^3 \\times 3^3 = 2^4 \\times 3^3 \\times 5 $$\nGroup perfect squares:\n$$ 2160 = (2^4 \\times 3^2) \\times (3 \\times 5) $$\n$$ 2160 = (16 \\times 9) \\times 15 = 144 \\times 15 $$\n\nThus:\n$$ \\sqrt{2160} = \\sqrt{144 \\times 15} = 12\\sqrt{15} $$\n\nSubstituting this back into the expression for $AD$:\n$$ AD = \\frac{12\\sqrt{15}}{11} $$\n\n### Conclusion\n\nThe length of the angle bisector $AD$ is:\n$$ \\frac{12\\sqrt{15}}{11} $$",
        "raw_answer": "To find the length of the segment $AD$, we will utilize the properties of the triangle defined by the side lengths provided, applying the Angle Bisector Theorem and the formula for the length of an internal angle bisector derived from Stewart's Theorem.\n\n### 1. Problem Setup and Definitions\n\nLet the vertices of the triangle be $A$, $B$, and $C$. The side lengths are given as:\n*   $c = AB = 5$\n*   $b = AC = 6$\n*   $a = BC = 7$\n\nPoint $D$ lies on the segment $BC$ such that $AD$ is the internal angle bisector of $\\angle BAC$. We aim to find the length $AD$.\n\n*(Note: The problem statement mentions the circumcenter $O$. While this establishes the geometric context, the position of the circumcenter is not required to determine the length of the internal angle bisector given the side lengths.)*\n\n### 2. Determining the Segments of BC\n\nAccording to the **Angle Bisector Theorem**, the internal bisector of an angle of a triangle divides the opposite side into two segments that are proportional to the other two sides of the triangle.\n\nLet $BD = m$ and $DC = n$. The theorem states:\n$$ \\frac{m}{n} = \\frac{c}{b} $$\n\nSubstituting the known values:\n$$ \\frac{m}{n} = \\frac{5}{6} $$\n\nSince $D$ lies on $BC$, we know that $m + n = a = 7$. We can set up the system of equations:\n1.  $6m = 5n$\n2.  $m + n = 7 \\implies n = 7 - m$\n\nSubstitute (2) into (1):\n$$ 6m = 5(7 - m) $$\n$$ 6m = 35 - 5m $$\n$$ 11m = 35 $$\n$$ m = BD = \\frac{35}{11} $$\n\nConsequently:\n$$ n = 7 - \\frac{35}{11} = \\frac{77 - 35}{11} = \\frac{42}{11} $$\nSo, $DC = \\frac{42}{11}$.\n\n### 3. Calculating the Length of AD\n\nWe can calculate the length of the angle bisector $AD$ using the standard formula derived from Stewart's Theorem. For an internal angle bisector of length $d_a$, the relationship is:\n$$ d_a^2 = bc - mn $$\n\n**Verification of the formula:**\nStewart's Theorem states that for a cevian of length $d$ dividing side $a$ into segments $m$ and $n$ (where $m$ is adjacent to $c$ and $n$ is adjacent to $b$):\n$$ b^2m + c^2n = a(d^2 + mn) $$\nSubstituting $m = \\frac{ac}{b+c}$ and $n = \\frac{ab}{b+c}$ leads to the simplified form $d^2 = bc - mn$.\n\n**Calculation:**\nSubstitute the values $b=6$, $c=5$, $m=\\frac{35}{11}$, and $n=\\frac{42}{11}$ into the equation:\n\n$$ AD^2 = (6)(5) - \\left(\\frac{35}{11}\\right)\\left(\\frac{42}{11}\\right) $$\n$$ AD^2 = 30 - \\frac{1470}{121} $$\n\nTo combine these terms, find a common denominator:\n$$ AD^2 = \\frac{30 \\times 121}{121} - \\frac{1470}{121} $$\n$$ AD^2 = \\frac{3630 - 1470}{121} $$\n$$ AD^2 = \\frac{2160}{121} $$\n\n### 4. Simplifying the Result\n\nNow, we take the square root of both sides to find $AD$:\n$$ AD = \\sqrt{\\frac{2160}{121}} $$\n$$ AD = \\frac{\\sqrt{2160}}{11} $$\n\nWe simplify the numerator $\\sqrt{2160}$ by prime factorization:\n$$ 2160 = 10 \\times 216 = (2 \\times 5) \\times (6^3) = 2 \\times 5 \\times 2^3 \\times 3^3 = 2^4 \\times 3^3 \\times 5 $$\nGroup perfect squares:\n$$ 2160 = (2^4 \\times 3^2) \\times (3 \\times 5) $$\n$$ 2160 = (16 \\times 9) \\times 15 = 144 \\times 15 $$\n\nThus:\n$$ \\sqrt{2160} = \\sqrt{144 \\times 15} = 12\\sqrt{15} $$\n\nSubstituting this back into the expression for $AD$:\n$$ AD = \\frac{12\\sqrt{15}}{11} $$\n\n### Conclusion\n\nThe length of the angle bisector $AD$ is:\n$$ \\frac{12\\sqrt{15}}{11} $$",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ],
    "outer_attempt": 1
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "## Step 1: Understand the task and the domain\nThe task is to generate a challenging but solvable mathematics problem in the domain of Convex and discrete geometry, along with a complete and verifiable solution. The problem should be well-posed, clear, non-trivial, and solvable with established mathematical methods.\n\n## Step 2: Identify a suitable topic within Convex and discrete geometry\nConvex and discrete geometry encompasses a broad range of topics including the geometry of convex sets, polytopes, and discrete geometric structures. A suitable topic could involve properties of convex polytopes, such as their faces, volumes, or the relationships between their vertices and facets.\n\n## Step 3: Formulate a specific problem\nLet's consider a problem related to the properties of a specific convex polytope. A well-known result in convex geometry is the characterization of regular polytopes. However, a more challenging and less straightforward problem could involve understanding the relationship between the number of vertices, edges, and faces of a polytope.\n\n## Step 4: Generate a problem statement\nConsider a 3-dimensional convex polytope $P$ that is simple (every vertex is incident to exactly 3 edges) and has $V$ vertices, $E$ edges, and $F$ faces. The problem is to prove that for such a polytope, Euler's formula ($V - E + F = 2$) holds, and then use this to derive a relationship between $V$ and $F$.\n\n## Step 5: Write down the problem in the required format\n\nLet $P$ be a simple 3-dimensional convex polytope with $V$ vertices, $E$ edges, and $F$ faces. Prove that $V - E + F = 2$ and derive a formula relating $V$ and $F$ using the fact that $P$ is simple.",
    "run_id": "36",
    "topic_slug": "convex_and_discrete_geometry",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "Here is the complete, rigorous solution to the problem presented.\n\n### Problem Statement\n\nLet $P$ be a simple 3-dimensional convex polytope with $V$ vertices, $E$ edges, and $F$ faces.\n1. Prove that $V - E + F = 2$.\n2. Derive a formula relating $V$ and $F$ using the fact that $P$ is simple.\n\n---\n\n### Part 1: Proof of Euler's Formula for 3-Polytopes\n\n**Theorem:** For any convex 3-dimensional polytope, the number of vertices ($V$), edges ($E$), and faces ($F$) satisfy $V - E + F = 2$.\n\n**Proof:**\n\n**1. Reduction to Planar Graphs**\nThe boundary of a convex 3-polytope $P$ is homeomorphic to the 2-sphere $S^2$. We can project this boundary onto the Euclidean plane $\\mathbb{R}^2$ to form a planar graph $G$. A standard method is using a Schlegel diagram: we choose a face of the polytope to act as the \"window\" and project the remaining structure through this face onto the plane.\n- The vertices and edges of $P$ correspond one-to-one with the vertices and edges of $G$. Thus, $V_G = V$ and $E_G = E$.\n- The faces of $P$ correspond to the bounded regions of the plane graph $G$, plus the one unbounded region (the exterior) which corresponds to the face we projected through. Thus, $F_G = F$.\n- Since $P$ is a convex polytope, its 1-skeleton is connected; thus $G$ is a connected planar graph.\n\nThe problem reduces to proving Euler's formula for a connected planar graph $G$: $V_G - E_G + F_G = 2$.\n\n**2. Inductive Proof for Planar Graphs**\nWe proceed by induction on the number of faces, $F_G$.\n\n**Base Case:** $F_G = 1$.\nIf the graph has only one face (the unbounded region), it cannot contain any cycles (a cycle would enclose a bounded face). Since $G$ is connected and has no cycles, it is a tree.\nFor a tree with $V_G$ vertices and $E_G$ edges, we know that $E_G = V_G - 1$.\nSubstituting into Euler's expression:\n$$V_G - E_G + F_G = V_G - (V_G - 1) + 1 = 1 + 1 = 2$$\nThe formula holds for the base case.\n\n**Inductive Step:**\nAssume that for any connected planar graph with $k$ faces, $V - E + k = 2$.\nConsider a connected planar graph $G$ with $k+1$ faces ($k \\geq 1$).\nSince $k+1 \\geq 2$, the graph is not a tree and must contain at least one cycle. Furthermore, there must exist an edge $e$ that lies on the boundary between two distinct faces (an internal edge).\nRemove this edge $e$.\n- The number of edges becomes $E' = E - 1$.\n- The number of vertices remains $V' = V$.\n- Removing the boundary between two faces merges them into one. Thus, the number of faces becomes $F' = (k+1) - 1 = k$.\n- Removing an edge from a cycle does not disconnect the graph, so the resulting graph $G'$ is still connected.\n\nBy the inductive hypothesis, Euler's formula holds for $G'$:\n$$V' - E' + F' = 2$$\nSubstitute the relations back in terms of $G$:\n$$V - (E - 1) + (F - 1) = 2$$\n$$V - E + 1 + F - 1 = 2$$\n$$V - E + F = 2$$\n\n**Conclusion:**\nBy the principle of mathematical induction, $V - E + F = 2$ holds for all connected planar graphs, and consequently for all convex 3-polytopes.\n\n---\n\n### Part 2: Derivation of the Relation for Simple Polytopes\n\n**Definition:** A 3-dimensional polytope is **simple** if every vertex is incident to exactly 3 edges.\n\n**1. Establish the Vertex-Edge Relation**\nLet $\\deg(v)$ denote the degree of a vertex $v$ (the number of edges incident to it).\nSince $P$ is simple, for every vertex $v_i$, $\\deg(v_i) = 3$.\nWe sum the degrees of all vertices:\n$$\\sum_{i=1}^{V} \\deg(v_i) = 3V$$\n\nBy the Handshaking Lemma (graph theory), the sum of the degrees of the vertices in a graph is equal to twice the number of edges, because each edge connects two vertices and is counted exactly twice in the sum.\n$$2E = \\sum_{i=1}^{V} \\deg(v_i)$$\n\nEquating the two expressions:\n$$2E = 3V \\implies E = \\frac{3}{2}V$$\n\n**2. Substitute into Euler's Formula**\nWe start with Euler's formula proven in Part 1:\n$$V - E + F = 2$$\n\nSubstitute $E = \\frac{3}{2}V$ into the equation:\n$$V - \\frac{3}{2}V + F = 2$$\n\nSimplify the terms with $V$:\n$$-\\frac{1}{2}V + F = 2$$\n\n**3. Final Formula**\nTo express the relationship cleanly, we can solve for $V$ in terms of $F$:\n$$\\frac{1}{2}V = F - 2$$\n$$V = 2F - 4$$\n\nAlternatively, we can write it as $2F - V = 4$.\n\n### Final Answer\n\n1.  **Proof**: The relation $V - E + F = 2$ is proven by mapping the polytope to a connected planar graph and using induction on the number of faces.\n2.  **Relation**: For a simple 3-dimensional convex polytope, the relationship between the number of vertices and faces is:\n    $$V = 2F - 4$$",
        "raw_answer": "Here is the complete, rigorous solution to the problem presented.\n\n### Problem Statement\n\nLet $P$ be a simple 3-dimensional convex polytope with $V$ vertices, $E$ edges, and $F$ faces.\n1. Prove that $V - E + F = 2$.\n2. Derive a formula relating $V$ and $F$ using the fact that $P$ is simple.\n\n---\n\n### Part 1: Proof of Euler's Formula for 3-Polytopes\n\n**Theorem:** For any convex 3-dimensional polytope, the number of vertices ($V$), edges ($E$), and faces ($F$) satisfy $V - E + F = 2$.\n\n**Proof:**\n\n**1. Reduction to Planar Graphs**\nThe boundary of a convex 3-polytope $P$ is homeomorphic to the 2-sphere $S^2$. We can project this boundary onto the Euclidean plane $\\mathbb{R}^2$ to form a planar graph $G$. A standard method is using a Schlegel diagram: we choose a face of the polytope to act as the \"window\" and project the remaining structure through this face onto the plane.\n- The vertices and edges of $P$ correspond one-to-one with the vertices and edges of $G$. Thus, $V_G = V$ and $E_G = E$.\n- The faces of $P$ correspond to the bounded regions of the plane graph $G$, plus the one unbounded region (the exterior) which corresponds to the face we projected through. Thus, $F_G = F$.\n- Since $P$ is a convex polytope, its 1-skeleton is connected; thus $G$ is a connected planar graph.\n\nThe problem reduces to proving Euler's formula for a connected planar graph $G$: $V_G - E_G + F_G = 2$.\n\n**2. Inductive Proof for Planar Graphs**\nWe proceed by induction on the number of faces, $F_G$.\n\n**Base Case:** $F_G = 1$.\nIf the graph has only one face (the unbounded region), it cannot contain any cycles (a cycle would enclose a bounded face). Since $G$ is connected and has no cycles, it is a tree.\nFor a tree with $V_G$ vertices and $E_G$ edges, we know that $E_G = V_G - 1$.\nSubstituting into Euler's expression:\n$$V_G - E_G + F_G = V_G - (V_G - 1) + 1 = 1 + 1 = 2$$\nThe formula holds for the base case.\n\n**Inductive Step:**\nAssume that for any connected planar graph with $k$ faces, $V - E + k = 2$.\nConsider a connected planar graph $G$ with $k+1$ faces ($k \\geq 1$).\nSince $k+1 \\geq 2$, the graph is not a tree and must contain at least one cycle. Furthermore, there must exist an edge $e$ that lies on the boundary between two distinct faces (an internal edge).\nRemove this edge $e$.\n- The number of edges becomes $E' = E - 1$.\n- The number of vertices remains $V' = V$.\n- Removing the boundary between two faces merges them into one. Thus, the number of faces becomes $F' = (k+1) - 1 = k$.\n- Removing an edge from a cycle does not disconnect the graph, so the resulting graph $G'$ is still connected.\n\nBy the inductive hypothesis, Euler's formula holds for $G'$:\n$$V' - E' + F' = 2$$\nSubstitute the relations back in terms of $G$:\n$$V - (E - 1) + (F - 1) = 2$$\n$$V - E + 1 + F - 1 = 2$$\n$$V - E + F = 2$$\n\n**Conclusion:**\nBy the principle of mathematical induction, $V - E + F = 2$ holds for all connected planar graphs, and consequently for all convex 3-polytopes.\n\n---\n\n### Part 2: Derivation of the Relation for Simple Polytopes\n\n**Definition:** A 3-dimensional polytope is **simple** if every vertex is incident to exactly 3 edges.\n\n**1. Establish the Vertex-Edge Relation**\nLet $\\deg(v)$ denote the degree of a vertex $v$ (the number of edges incident to it).\nSince $P$ is simple, for every vertex $v_i$, $\\deg(v_i) = 3$.\nWe sum the degrees of all vertices:\n$$\\sum_{i=1}^{V} \\deg(v_i) = 3V$$\n\nBy the Handshaking Lemma (graph theory), the sum of the degrees of the vertices in a graph is equal to twice the number of edges, because each edge connects two vertices and is counted exactly twice in the sum.\n$$2E = \\sum_{i=1}^{V} \\deg(v_i)$$\n\nEquating the two expressions:\n$$2E = 3V \\implies E = \\frac{3}{2}V$$\n\n**2. Substitute into Euler's Formula**\nWe start with Euler's formula proven in Part 1:\n$$V - E + F = 2$$\n\nSubstitute $E = \\frac{3}{2}V$ into the equation:\n$$V - \\frac{3}{2}V + F = 2$$\n\nSimplify the terms with $V$:\n$$-\\frac{1}{2}V + F = 2$$\n\n**3. Final Formula**\nTo express the relationship cleanly, we can solve for $V$ in terms of $F$:\n$$\\frac{1}{2}V = F - 2$$\n$$V = 2F - 4$$\n\nAlternatively, we can write it as $2F - V = 4$.\n\n### Final Answer\n\n1.  **Proof**: The relation $V - E + F = 2$ is proven by mapping the polytope to a connected planar graph and using induction on the number of faces.\n2.  **Relation**: For a simple 3-dimensional convex polytope, the relationship between the number of vertices and faces is:\n    $$V = 2F - 4$$",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ],
    "outer_attempt": 1
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "## Step 1: Understand the task and the domain\nThe task is to generate a challenging but solvable mathematics problem in the domain of differential geometry, along with a complete and verifiable solution. The problem should be well-posed, clear, non-trivial, and solvable with established mathematical methods.\n\n## Step 2: Identify a suitable topic within differential geometry\nDifferential geometry is a broad field that includes the study of curves and surfaces, Riemannian geometry, and more. A suitable topic could involve the properties of curves or surfaces, such as curvature, geodesics, or the application of differential geometry to other areas like physics.\n\n## Step 3: Formulate a specific problem\nLet's consider a problem involving the curvature of a surface. A classic result in differential geometry is the Theorema Egregium of Gauss, which states that the Gaussian curvature of a surface is invariant under isometric deformations. A challenging problem could involve calculating or utilizing this curvature.\n\n## Step 4: Construct the problem statement\nTo create a problem, let's consider a specific surface and ask a question about its geometric properties. For example, we could examine the surface given by $z = x^2 + y^2$ and ask to compute its Gaussian curvature at a given point.\n\n## Step 5: Write the problem statement\nLet's define the problem: \"Let $S$ be the surface given by $z = x^2 + y^2$. Compute the Gaussian curvature $K$ of $S$ at the point $(1,1,2)$.\"\n\n## Step 6: Provide the solution\nTo solve this, we first need to parameterize the surface. The surface $S$ can be parameterized by $\\vec{r}(x,y) = (x, y, x^2 + y^2)$. The Gaussian curvature $K$ can be found using the formula $K = \\frac{LN - M^2}{EG - F^2}$, where $E, F, G$ are coefficients of the first fundamental form, and $L, M, N$ are coefficients of the second fundamental form.\n\n## Step 7: Calculate the first and second fundamental forms\nFirst, compute $\\vec{r}_x = (1, 0, 2x)$ and $\\vec{r}_y = (0, 1, 2y)$. Then, $E = \\vec{r}_x \\cdot \\vec{r}_x = 1 + 4x^2$, $F = \\vec{r}_x \\cdot \\vec{r}_y = 4xy$, and $G = \\vec{r}_y \\cdot \\vec{r}_y = 1 + 4y^2$. The normal vector $\\vec{n} = \\frac{\\vec{r}_x \\times \\vec{r}_y}{\\|\\vec{r}_x \\times \\vec{r}_y\\|}$, and $\\vec{r}_x \\times \\vec{r}_y = (-2x, -2y, 1)$, so $\\|\\vec{r}_x \\times \\vec{r}_y\\| = \\sqrt{4x^2 + 4y^2 + 1}$. Thus, $\\vec{n} = \\frac{1}{\\sqrt{4x^2 + 4y^2 + 1}}(-2x, -2y, 1)$.\n\n## Step 8: Continue with the calculation of $L, M, N$\n$L = \\vec{r}_{xx} \\cdot \\vec{n}$, $M = \\vec{r}_{xy} \\cdot \\vec{n}$, and $N = \\vec{r}_{yy} \\cdot \\vec{n}$. We have $\\vec{r}_{xx} = (0, 0, 2)$, $\\vec{r}_{xy} = (0, 0, 0)$, and $\\vec{r}_{yy} = (0, 0, 2)$. So, $L = \\frac{2}{\\sqrt{4x^2 + 4y^2 + 1}}$, $M = 0$, and $N = \\frac{2}{\\sqrt{4x^2 + 4y^2 + 1}}$.\n\n## Step 9: Evaluate $K$ at $(1,1,2)$\nAt $(1,1,2)$, $x = 1$, $y = 1$, so $E = 5$, $F = 4$, $G = 5$, $L = \\frac{2}{3}$, $M = 0$, and $N = \\frac{2}{3}$. Thus, $EG - F^2 = 25 - 16 = 9$ and $LN - M^2 = \\frac{4}{9}$.\n\n## Step 10: Calculate $K$\n$K = \\frac{LN - M^2}{EG - F^2} = \\frac{\\frac{4}{9}}{9} = \\frac{4}{81}$.\n\nThe final answer is: \n\nLet $S$ be the surface given by $z = x^2 + y^2$. Compute the Gaussian curvature $K$ of $S$ at the point $(1,1,2)$.",
    "run_id": "37",
    "topic_slug": "differential_geometry",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "To solve the problem, we will compute the Gaussian curvature $K$ of the surface $S$ defined by $z = x^2 + y^2$ at the specific point $P = (1, 1, 2)$.\n\n### 1. Parametrization of the Surface\n\nThe surface $S$ is the graph of the function $f(x, y) = x^2 + y^2$. We can parametrize this surface globally using the coordinates $u = x$ and $v = y$. The position vector $\\mathbf{r}: \\mathbb{R}^2 \\to \\mathbb{R}^3$ is given by:\n$$ \\mathbf{r}(u, v) = (u, v, u^2 + v^2) $$\nThe point $P = (1, 1, 2)$ corresponds to the parameter values $u = 1$ and $v = 1$.\n\n### 2. The First Fundamental Form\n\nWe calculate the tangent vectors by taking the partial derivatives of $\\mathbf{r}$ with respect to $u$ and $v$:\n$$ \\mathbf{r}_u = \\frac{\\partial \\mathbf{r}}{\\partial u} = (1, 0, 2u) $$\n$$ \\mathbf{r}_v = \\frac{\\partial \\mathbf{r}}{\\partial v} = (0, 1, 2v) $$\n\nWe evaluate these vectors at the point $(u, v) = (1, 1)$:\n$$ \\mathbf{r}_u|_{(1,1)} = (1, 0, 2) $$\n$$ \\mathbf{r}_v|_{(1,1)} = (0, 1, 2) $$\n\nThe coefficients of the first fundamental form, $E, F, G$, are given by the inner products of these tangent vectors:\n$$ E = \\mathbf{r}_u \\cdot \\mathbf{r}_u = 1^2 + 0^2 + 2^2 = 5 $$\n$$ F = \\mathbf{r}_u \\cdot \\mathbf{r}_v = (1)(0) + (0)(1) + (2)(2) = 4 $$\n$$ G = \\mathbf{r}_v \\cdot \\mathbf{r}_v = 0^2 + 1^2 + 2^2 = 5 $$\n\nWe calculate the determinant of the first fundamental form matrix:\n$$ EG - F^2 = (5)(5) - (4)^2 = 25 - 16 = 9 $$\nSince $EG - F^2 \\neq 0$, the parametrization is regular at this point.\n\n### 3. The Unit Normal Vector\n\nTo find the unit normal vector $\\mathbf{n}$, we first compute the cross product of the tangent vectors:\n$$ \\mathbf{r}_u \\times \\mathbf{r}_v = \\begin{vmatrix} \\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\ 1 & 0 & 2 \\\\ 0 & 1 & 2 \\end{vmatrix} = (0(2) - 2(1)) \\mathbf{i} - (1(2) - 2(0)) \\mathbf{j} + (1(1) - 0(0)) \\mathbf{k} $$\n$$ \\mathbf{r}_u \\times \\mathbf{r}_v = (-2, -2, 1) $$\n\nNext, we find the magnitude of this vector:\n$$ \\| \\mathbf{r}_u \\times \\mathbf{r}_v \\| = \\sqrt{(-2)^2 + (-2)^2 + 1^2} = \\sqrt{4 + 4 + 1} = \\sqrt{9} = 3 $$\n\nThe unit normal vector $\\mathbf{n}$ is:\n$$ \\mathbf{n} = \\frac{\\mathbf{r}_u \\times \\mathbf{r}_v}{\\| \\mathbf{r}_u \\times \\mathbf{r}_v \\|} = \\frac{1}{3}(-2, -2, 1) $$\n\n### 4. The Second Fundamental Form\n\nWe compute the second partial derivatives of $\\mathbf{r}$:\n$$ \\mathbf{r}_{uu} = \\frac{\\partial}{\\partial u}(1, 0, 2u) = (0, 0, 2) $$\n$$ \\mathbf{r}_{uv} = \\frac{\\partial}{\\partial v}(1, 0, 2u) = (0, 0, 0) $$\n$$ \\mathbf{r}_{vv} = \\frac{\\partial}{\\partial v}(0, 1, 2v) = (0, 0, 2) $$\n\nThe coefficients of the second fundamental form, $L, M, N$, are the projections of the second derivatives onto the normal vector $\\mathbf{n}$:\n$$ L = \\mathbf{r}_{uu} \\cdot \\mathbf{n} = (0, 0, 2) \\cdot \\frac{1}{3}(-2, -2, 1) = \\frac{1}{3}(0 + 0 + 2) = \\frac{2}{3} $$\n$$ M = \\mathbf{r}_{uv} \\cdot \\mathbf{n} = (0, 0, 0) \\cdot \\mathbf{n} = 0 $$\n$$ N = \\mathbf{r}_{vv} \\cdot \\mathbf{n} = (0, 0, 2) \\cdot \\frac{1}{3}(-2, -2, 1) = \\frac{1}{3}(0 + 0 + 2) = \\frac{2}{3} $$\n\nThe determinant of the second fundamental form matrix is:\n$$ LN - M^2 = \\left(\\frac{2}{3}\\right)\\left(\\frac{2}{3}\\right) - 0^2 = \\frac{4}{9} $$\n\n### 5. Calculation of Gaussian Curvature\n\nThe Gaussian curvature $K$ is defined as the ratio of the determinants of the second and first fundamental forms:\n$$ K = \\frac{LN - M^2}{EG - F^2} $$\n\nSubstituting the calculated values:\n$$ K = \\frac{4/9}{9} = \\frac{4}{81} $$\n\n### 6. Verification using Explicit Formula\n\nAlternatively, for a surface defined explicitly by $z = f(x,y)$, the Gaussian curvature is given by:\n$$ K = \\frac{f_{xx}f_{yy} - f_{xy}^2}{(1 + f_x^2 + f_y^2)^2} $$\nFor $f(x,y) = x^2 + y^2$:\n- $f_x = 2x, \\quad f_y = 2y$\n- $f_{xx} = 2, \\quad f_{yy} = 2, \\quad f_{xy} = 0$\n\nAt point $(1,1)$:\n- $f_x = 2, \\quad f_y = 2$\n- Numerator: $(2)(2) - 0^2 = 4$\n- Denominator: $(1 + 2^2 + 2^2)^2 = (1 + 4 + 4)^2 = 9^2 = 81$\n\nThus, $K = \\frac{4}{81}$. This confirms our previous result.\n\n### Final Answer\nThe Gaussian curvature of the surface $S$ at the point $(1,1,2)$ is $\\frac{4}{81}$.",
        "raw_answer": "To solve the problem, we will compute the Gaussian curvature $K$ of the surface $S$ defined by $z = x^2 + y^2$ at the specific point $P = (1, 1, 2)$.\n\n### 1. Parametrization of the Surface\n\nThe surface $S$ is the graph of the function $f(x, y) = x^2 + y^2$. We can parametrize this surface globally using the coordinates $u = x$ and $v = y$. The position vector $\\mathbf{r}: \\mathbb{R}^2 \\to \\mathbb{R}^3$ is given by:\n$$ \\mathbf{r}(u, v) = (u, v, u^2 + v^2) $$\nThe point $P = (1, 1, 2)$ corresponds to the parameter values $u = 1$ and $v = 1$.\n\n### 2. The First Fundamental Form\n\nWe calculate the tangent vectors by taking the partial derivatives of $\\mathbf{r}$ with respect to $u$ and $v$:\n$$ \\mathbf{r}_u = \\frac{\\partial \\mathbf{r}}{\\partial u} = (1, 0, 2u) $$\n$$ \\mathbf{r}_v = \\frac{\\partial \\mathbf{r}}{\\partial v} = (0, 1, 2v) $$\n\nWe evaluate these vectors at the point $(u, v) = (1, 1)$:\n$$ \\mathbf{r}_u|_{(1,1)} = (1, 0, 2) $$\n$$ \\mathbf{r}_v|_{(1,1)} = (0, 1, 2) $$\n\nThe coefficients of the first fundamental form, $E, F, G$, are given by the inner products of these tangent vectors:\n$$ E = \\mathbf{r}_u \\cdot \\mathbf{r}_u = 1^2 + 0^2 + 2^2 = 5 $$\n$$ F = \\mathbf{r}_u \\cdot \\mathbf{r}_v = (1)(0) + (0)(1) + (2)(2) = 4 $$\n$$ G = \\mathbf{r}_v \\cdot \\mathbf{r}_v = 0^2 + 1^2 + 2^2 = 5 $$\n\nWe calculate the determinant of the first fundamental form matrix:\n$$ EG - F^2 = (5)(5) - (4)^2 = 25 - 16 = 9 $$\nSince $EG - F^2 \\neq 0$, the parametrization is regular at this point.\n\n### 3. The Unit Normal Vector\n\nTo find the unit normal vector $\\mathbf{n}$, we first compute the cross product of the tangent vectors:\n$$ \\mathbf{r}_u \\times \\mathbf{r}_v = \\begin{vmatrix} \\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\ 1 & 0 & 2 \\\\ 0 & 1 & 2 \\end{vmatrix} = (0(2) - 2(1)) \\mathbf{i} - (1(2) - 2(0)) \\mathbf{j} + (1(1) - 0(0)) \\mathbf{k} $$\n$$ \\mathbf{r}_u \\times \\mathbf{r}_v = (-2, -2, 1) $$\n\nNext, we find the magnitude of this vector:\n$$ \\| \\mathbf{r}_u \\times \\mathbf{r}_v \\| = \\sqrt{(-2)^2 + (-2)^2 + 1^2} = \\sqrt{4 + 4 + 1} = \\sqrt{9} = 3 $$\n\nThe unit normal vector $\\mathbf{n}$ is:\n$$ \\mathbf{n} = \\frac{\\mathbf{r}_u \\times \\mathbf{r}_v}{\\| \\mathbf{r}_u \\times \\mathbf{r}_v \\|} = \\frac{1}{3}(-2, -2, 1) $$\n\n### 4. The Second Fundamental Form\n\nWe compute the second partial derivatives of $\\mathbf{r}$:\n$$ \\mathbf{r}_{uu} = \\frac{\\partial}{\\partial u}(1, 0, 2u) = (0, 0, 2) $$\n$$ \\mathbf{r}_{uv} = \\frac{\\partial}{\\partial v}(1, 0, 2u) = (0, 0, 0) $$\n$$ \\mathbf{r}_{vv} = \\frac{\\partial}{\\partial v}(0, 1, 2v) = (0, 0, 2) $$\n\nThe coefficients of the second fundamental form, $L, M, N$, are the projections of the second derivatives onto the normal vector $\\mathbf{n}$:\n$$ L = \\mathbf{r}_{uu} \\cdot \\mathbf{n} = (0, 0, 2) \\cdot \\frac{1}{3}(-2, -2, 1) = \\frac{1}{3}(0 + 0 + 2) = \\frac{2}{3} $$\n$$ M = \\mathbf{r}_{uv} \\cdot \\mathbf{n} = (0, 0, 0) \\cdot \\mathbf{n} = 0 $$\n$$ N = \\mathbf{r}_{vv} \\cdot \\mathbf{n} = (0, 0, 2) \\cdot \\frac{1}{3}(-2, -2, 1) = \\frac{1}{3}(0 + 0 + 2) = \\frac{2}{3} $$\n\nThe determinant of the second fundamental form matrix is:\n$$ LN - M^2 = \\left(\\frac{2}{3}\\right)\\left(\\frac{2}{3}\\right) - 0^2 = \\frac{4}{9} $$\n\n### 5. Calculation of Gaussian Curvature\n\nThe Gaussian curvature $K$ is defined as the ratio of the determinants of the second and first fundamental forms:\n$$ K = \\frac{LN - M^2}{EG - F^2} $$\n\nSubstituting the calculated values:\n$$ K = \\frac{4/9}{9} = \\frac{4}{81} $$\n\n### 6. Verification using Explicit Formula\n\nAlternatively, for a surface defined explicitly by $z = f(x,y)$, the Gaussian curvature is given by:\n$$ K = \\frac{f_{xx}f_{yy} - f_{xy}^2}{(1 + f_x^2 + f_y^2)^2} $$\nFor $f(x,y) = x^2 + y^2$:\n- $f_x = 2x, \\quad f_y = 2y$\n- $f_{xx} = 2, \\quad f_{yy} = 2, \\quad f_{xy} = 0$\n\nAt point $(1,1)$:\n- $f_x = 2, \\quad f_y = 2$\n- Numerator: $(2)(2) - 0^2 = 4$\n- Denominator: $(1 + 2^2 + 2^2)^2 = (1 + 4 + 4)^2 = 9^2 = 81$\n\nThus, $K = \\frac{4}{81}$. This confirms our previous result.\n\n### Final Answer\nThe Gaussian curvature of the surface $S$ at the point $(1,1,2)$ is $\\frac{4}{81}$.",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ],
    "outer_attempt": 1
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "## Step 1: Understand the task\nThe task is to generate a challenging mathematical problem in the domain of general topology, along with a complete solution, following specific guidelines.\n[/QUESTION]",
    "run_id": "38",
    "topic_slug": "general_topology",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "Here is a challenging problem in general topology concerning metrization theorems for compact spaces, followed by a complete and rigorous solution.\n\n***\n\n# Problem: Metrizability of Compact Hausdorff Spaces via the Diagonal\n\n**Problem Statement:**\nLet $X$ be a compact Hausdorff topological space. Let $\\Delta = \\{(x, x) \\in X \\times X \\mid x \\in X\\}$ denote the diagonal subset of the product space $X \\times X$.\n\nProve that $X$ is metrizable if and only if $\\Delta$ is a $G_\\delta$ set in $X \\times X$.\n\n(Note: A set is a $G_\\delta$ set if it can be written as a countable intersection of open sets.)\n\n***\n\n# Solution\n\nWe must prove the equivalence of two statements for a compact Hausdorff space $X$:\n1. $X$ is metrizable.\n2. The diagonal $\\Delta$ is a $G_\\delta$ set in $X \\times X$ (with the product topology).\n\n## Part 1: Forward Implication ($\\implies$)\n\n**Hypothesis:** Assume $X$ is metrizable.\n**Goal:** Show that $\\Delta$ is a $G_\\delta$ set in $X \\times X$.\n\n1.  Since $X$ is metrizable, let $d: X \\times X \\to [0, \\infty)$ be a metric that induces the topology of $X$.\n2.  The metric function $d$ is continuous on $X \\times X$ (with respect to the product topology induced by the metric on $X$).\n3.  The diagonal $\\Delta$ is precisely the zero set of the metric:\n    $$ \\Delta = \\{(x, y) \\in X \\times X \\mid d(x, y) = 0 \\} = d^{-1}(\\{0\\}). $$\n4.  Consider the sets $U_n$ defined by:\n    $$ U_n = \\{(x, y) \\in X \\times X \\mid d(x, y) < \\frac{1}{n} \\} = d^{-1}\\left(\\left[0, \\frac{1}{n}\\right)\\right). $$\n5.  Since $d$ is continuous and $[0, 1/n)$ is open in $[0, \\infty)$, each set $U_n$ is open in $X \\times X$.\n6.  The intersection of these sets is:\n    $$ \\bigcap_{n=1}^{\\infty} U_n = \\{(x, y) \\in X \\times X \\mid d(x, y) < \\frac{1}{n} \\text{ for all } n \\in \\mathbb{Z}^+\\} = \\{(x, y) \\mid d(x, y) = 0\\} = \\Delta. $$\n7.  Therefore, $\\Delta$ is the intersection of a countable collection of open sets. By definition, $\\Delta$ is a $G_\\delta$ set.\n\nThis direction holds for any metrizable space (compactness is not required here).\n\n## Part 2: Reverse Implication ($\\impliedby$)\n\n**Hypothesis:** Assume $X$ is compact Hausdorff and $\\Delta$ is a $G_\\delta$ set in $X \\times X$.\n**Goal:** Show that $X$ is metrizable.\n\n**Strategy:** A standard metrization theorem states that a compact Hausdorff space is metrizable if and only if it is second-countable. Since we are given that $X$ is compact Hausdorff, it suffices to prove that $X$ is second-countable (i.e., has a countable basis).\n\n### Step 2.1: Properties of the Diagonal and Covers\n\n1.  Since $\\Delta$ is a $G_\\delta$ set, there exist open sets $W_1, W_2, \\dots$ in $X \\times X$ such that $\\Delta = \\bigcap_{n=1}^\\infty W_n$.\n2.  Since $X$ is Hausdorff, $\\Delta$ is closed in $X \\times X$.\n3.  Since $X$ is compact Hausdorff, the product space $X \\times X$ is also compact Hausdorff.\n4.  Compact Hausdorff spaces are normal ($T_4$). In a normal space, for any closed $G_\\delta$ set $F$, there exists a sequence of open sets $G_n$ such that $F = \\bigcap G_n$ and $\\overline{G_{n+1}} \\subseteq G_n$ for all $n$.\n    *Justification:* We have $\\Delta = \\bigcap W_n$. Since $X \\times X$ is normal, we can use Urysohn's Lemma or simple normality properties to construct the sequence $G_n$ inductively. Let $G_1 = W_1$. Since $\\Delta \\subseteq G_1$ is closed and $G_1$ is open, normality implies there exists an open $V$ such that $\\Delta \\subseteq V \\subseteq \\overline{V} \\subseteq G_1$. Intersect with $W_2$ to maintain the limit, i.e., set $G_2 = V \\cap W_2$. Proceeding inductively, we construct a sequence $G_n$ such that $\\Delta = \\bigcap_{n=1}^\\infty G_n$ and $\\overline{G_{n+1}} \\subseteq G_n$.\n\n### Step 2.2: Constructing Countable Covers of $X$\n\nFor each $n \\in \\mathbb{Z}^+$, we construct a finite open cover $\\mathcal{U}_n$ of $X$ with a specific \"smallness\" property relative to $G_n$.\n\n1.  Fix $n$. For each $x \\in X$, we have $(x, x) \\in \\Delta \\subseteq G_n$.\n2.  Since $G_n$ is open in the product topology, there exists an open neighborhood $V_x$ of $x$ in $X$ such that $V_x \\times V_x \\subseteq G_n$.\n3.  The collection $\\{V_x \\mid x \\in X\\}$ covers $X$. Since $X$ is compact, there exists a finite subcover. Let us denote this finite cover by $\\mathcal{U}_n$.\n4.  Property of $\\mathcal{U}_n$: For every $U \\in \\mathcal{U}_n$, $U \\times U \\subseteq G_n$.\n\nLet $\\mathcal{B} = \\bigcup_{n=1}^\\infty \\mathcal{U}_n$. Since each $\\mathcal{U}_n$ is finite, $\\mathcal{B}$ is a countable collection of open sets.\n\n### Step 2.3: Proving $\\mathcal{B}$ forms a Basis\n\nWe claim that for every $x \\in X$ and every open neighborhood $O$ of $x$, there exists a set $U \\in \\mathcal{B}$ such that $x \\in U \\subseteq O$. If this holds, $\\mathcal{B}$ is a basis for $X$.\n\nLet's define the star of $x$ with respect to a cover $\\mathcal{C}$ as $St(x, \\mathcal{C}) = \\bigcup \\{ U \\in \\mathcal{C} \\mid x \\in U \\}$.\n\n**Claim:** The collection $\\{ St(x, \\mathcal{U}_n) \\}_{n \\ge 1}$ forms a neighborhood basis for $x$. That is, for any open $O$ containing $x$, there exists $N$ such that $St(x, \\mathcal{U}_N) \\subseteq O$.\n\n**Proof of Claim (by contradiction):**\n1.  Suppose there exists an open neighborhood $O$ of $x$ such that for all $n$, $St(x, \\mathcal{U}_n) \\not\\subseteq O$.\n2.  Then for each $n$, there exists a point $y_n \\in St(x, \\mathcal{U}_n) \\setminus O$.\n3.  The sequence $(y_n)$ lies in the set $X \\setminus O$, which is closed in the compact space $X$. Therefore, $X \\setminus O$ is compact.\n4.  The sequence $(y_n)$ must have a cluster point $z \\in X \\setminus O$.\n5.  Since $x \\in O$ and $z \\in X \\setminus O$, we have $x \\neq z$. Thus $(x, z) \\notin \\Delta$.\n6.  Consider the relation established by the covers. Since $y_n \\in St(x, \\mathcal{U}_n)$, there exists some $U \\in \\mathcal{U}_n$ such that $x \\in U$ and $y_n \\in U$.\n7.  By the construction of $\\mathcal{U}_n$, $U \\times U \\subseteq G_n$. Therefore, $(x, y_n) \\in G_n$.\n8.  Now, fix an integer $k$. For all $n \\geq k$, we have $(x, y_n) \\in G_n \\subseteq G_k$.\n9.  Since $z$ is a cluster point of $(y_n)$, the point $(x, z)$ is a cluster point of the sequence $((x, y_n))$ in $X \\times X$.\n10. Since all terms $(x, y_n)$ for $n \\ge k$ lie in $G_k$, the cluster point $(x, z)$ must lie in the closure $\\overline{G_k}$.\n11. Recall our construction in Step 2.1: $\\overline{G_{k+1}} \\subseteq G_k$.\n12. Since $(x, z) \\in \\overline{G_{k+1}}$ (applying the logic from step 10 to $k+1$), we have $(x, z) \\in G_k$.\n13. This holds for *all* $k$. Therefore, $(x, z) \\in \\bigcap_{k=1}^\\infty G_k = \\Delta$.\n14. This implies $x = z$.\n15. This contradicts the fact that $x \\in O$ and $z \\in X \\setminus O$.\n\nTherefore, the assumption was false. There exists some $N$ such that $St(x, \\mathcal{U}_N) \\subseteq O$.\n\n**Conclusion of Step 2.3:**\nSince $St(x, \\mathcal{U}_N)$ is a union of finitely many sets from $\\mathcal{U}_N$ (all of which contain $x$), any single $U \\in \\mathcal{U}_N$ containing $x$ satisfies $x \\in U \\subseteq St(x, \\mathcal{U}_N) \\subseteq O$. Since $U \\in \\mathcal{U}_N \\subset \\mathcal{B}$, we have found a basis element inside $O$ containing $x$. Thus, $\\mathcal{B}$ is a countable basis for the topology of $X$.\n\n### Step 2.4: Final Conclusion\n\n1.  $X$ is a compact Hausdorff space.\n2.  We have shown that if $\\Delta$ is a $G_\\delta$ set, then $X$ admits a countable basis (is second-countable).\n3.  By the Urysohn Metrization Theorem (or the specific version for compact spaces: A compact Hausdorff space is metrizable iff it is second-countable), $X$ is metrizable.\n\nThus, both directions are proven.\n\n**Q.E.D.**",
        "raw_answer": "Here is a challenging problem in general topology concerning metrization theorems for compact spaces, followed by a complete and rigorous solution.\n\n***\n\n# Problem: Metrizability of Compact Hausdorff Spaces via the Diagonal\n\n**Problem Statement:**\nLet $X$ be a compact Hausdorff topological space. Let $\\Delta = \\{(x, x) \\in X \\times X \\mid x \\in X\\}$ denote the diagonal subset of the product space $X \\times X$.\n\nProve that $X$ is metrizable if and only if $\\Delta$ is a $G_\\delta$ set in $X \\times X$.\n\n(Note: A set is a $G_\\delta$ set if it can be written as a countable intersection of open sets.)\n\n***\n\n# Solution\n\nWe must prove the equivalence of two statements for a compact Hausdorff space $X$:\n1. $X$ is metrizable.\n2. The diagonal $\\Delta$ is a $G_\\delta$ set in $X \\times X$ (with the product topology).\n\n## Part 1: Forward Implication ($\\implies$)\n\n**Hypothesis:** Assume $X$ is metrizable.\n**Goal:** Show that $\\Delta$ is a $G_\\delta$ set in $X \\times X$.\n\n1.  Since $X$ is metrizable, let $d: X \\times X \\to [0, \\infty)$ be a metric that induces the topology of $X$.\n2.  The metric function $d$ is continuous on $X \\times X$ (with respect to the product topology induced by the metric on $X$).\n3.  The diagonal $\\Delta$ is precisely the zero set of the metric:\n    $$ \\Delta = \\{(x, y) \\in X \\times X \\mid d(x, y) = 0 \\} = d^{-1}(\\{0\\}). $$\n4.  Consider the sets $U_n$ defined by:\n    $$ U_n = \\{(x, y) \\in X \\times X \\mid d(x, y) < \\frac{1}{n} \\} = d^{-1}\\left(\\left[0, \\frac{1}{n}\\right)\\right). $$\n5.  Since $d$ is continuous and $[0, 1/n)$ is open in $[0, \\infty)$, each set $U_n$ is open in $X \\times X$.\n6.  The intersection of these sets is:\n    $$ \\bigcap_{n=1}^{\\infty} U_n = \\{(x, y) \\in X \\times X \\mid d(x, y) < \\frac{1}{n} \\text{ for all } n \\in \\mathbb{Z}^+\\} = \\{(x, y) \\mid d(x, y) = 0\\} = \\Delta. $$\n7.  Therefore, $\\Delta$ is the intersection of a countable collection of open sets. By definition, $\\Delta$ is a $G_\\delta$ set.\n\nThis direction holds for any metrizable space (compactness is not required here).\n\n## Part 2: Reverse Implication ($\\impliedby$)\n\n**Hypothesis:** Assume $X$ is compact Hausdorff and $\\Delta$ is a $G_\\delta$ set in $X \\times X$.\n**Goal:** Show that $X$ is metrizable.\n\n**Strategy:** A standard metrization theorem states that a compact Hausdorff space is metrizable if and only if it is second-countable. Since we are given that $X$ is compact Hausdorff, it suffices to prove that $X$ is second-countable (i.e., has a countable basis).\n\n### Step 2.1: Properties of the Diagonal and Covers\n\n1.  Since $\\Delta$ is a $G_\\delta$ set, there exist open sets $W_1, W_2, \\dots$ in $X \\times X$ such that $\\Delta = \\bigcap_{n=1}^\\infty W_n$.\n2.  Since $X$ is Hausdorff, $\\Delta$ is closed in $X \\times X$.\n3.  Since $X$ is compact Hausdorff, the product space $X \\times X$ is also compact Hausdorff.\n4.  Compact Hausdorff spaces are normal ($T_4$). In a normal space, for any closed $G_\\delta$ set $F$, there exists a sequence of open sets $G_n$ such that $F = \\bigcap G_n$ and $\\overline{G_{n+1}} \\subseteq G_n$ for all $n$.\n    *Justification:* We have $\\Delta = \\bigcap W_n$. Since $X \\times X$ is normal, we can use Urysohn's Lemma or simple normality properties to construct the sequence $G_n$ inductively. Let $G_1 = W_1$. Since $\\Delta \\subseteq G_1$ is closed and $G_1$ is open, normality implies there exists an open $V$ such that $\\Delta \\subseteq V \\subseteq \\overline{V} \\subseteq G_1$. Intersect with $W_2$ to maintain the limit, i.e., set $G_2 = V \\cap W_2$. Proceeding inductively, we construct a sequence $G_n$ such that $\\Delta = \\bigcap_{n=1}^\\infty G_n$ and $\\overline{G_{n+1}} \\subseteq G_n$.\n\n### Step 2.2: Constructing Countable Covers of $X$\n\nFor each $n \\in \\mathbb{Z}^+$, we construct a finite open cover $\\mathcal{U}_n$ of $X$ with a specific \"smallness\" property relative to $G_n$.\n\n1.  Fix $n$. For each $x \\in X$, we have $(x, x) \\in \\Delta \\subseteq G_n$.\n2.  Since $G_n$ is open in the product topology, there exists an open neighborhood $V_x$ of $x$ in $X$ such that $V_x \\times V_x \\subseteq G_n$.\n3.  The collection $\\{V_x \\mid x \\in X\\}$ covers $X$. Since $X$ is compact, there exists a finite subcover. Let us denote this finite cover by $\\mathcal{U}_n$.\n4.  Property of $\\mathcal{U}_n$: For every $U \\in \\mathcal{U}_n$, $U \\times U \\subseteq G_n$.\n\nLet $\\mathcal{B} = \\bigcup_{n=1}^\\infty \\mathcal{U}_n$. Since each $\\mathcal{U}_n$ is finite, $\\mathcal{B}$ is a countable collection of open sets.\n\n### Step 2.3: Proving $\\mathcal{B}$ forms a Basis\n\nWe claim that for every $x \\in X$ and every open neighborhood $O$ of $x$, there exists a set $U \\in \\mathcal{B}$ such that $x \\in U \\subseteq O$. If this holds, $\\mathcal{B}$ is a basis for $X$.\n\nLet's define the star of $x$ with respect to a cover $\\mathcal{C}$ as $St(x, \\mathcal{C}) = \\bigcup \\{ U \\in \\mathcal{C} \\mid x \\in U \\}$.\n\n**Claim:** The collection $\\{ St(x, \\mathcal{U}_n) \\}_{n \\ge 1}$ forms a neighborhood basis for $x$. That is, for any open $O$ containing $x$, there exists $N$ such that $St(x, \\mathcal{U}_N) \\subseteq O$.\n\n**Proof of Claim (by contradiction):**\n1.  Suppose there exists an open neighborhood $O$ of $x$ such that for all $n$, $St(x, \\mathcal{U}_n) \\not\\subseteq O$.\n2.  Then for each $n$, there exists a point $y_n \\in St(x, \\mathcal{U}_n) \\setminus O$.\n3.  The sequence $(y_n)$ lies in the set $X \\setminus O$, which is closed in the compact space $X$. Therefore, $X \\setminus O$ is compact.\n4.  The sequence $(y_n)$ must have a cluster point $z \\in X \\setminus O$.\n5.  Since $x \\in O$ and $z \\in X \\setminus O$, we have $x \\neq z$. Thus $(x, z) \\notin \\Delta$.\n6.  Consider the relation established by the covers. Since $y_n \\in St(x, \\mathcal{U}_n)$, there exists some $U \\in \\mathcal{U}_n$ such that $x \\in U$ and $y_n \\in U$.\n7.  By the construction of $\\mathcal{U}_n$, $U \\times U \\subseteq G_n$. Therefore, $(x, y_n) \\in G_n$.\n8.  Now, fix an integer $k$. For all $n \\geq k$, we have $(x, y_n) \\in G_n \\subseteq G_k$.\n9.  Since $z$ is a cluster point of $(y_n)$, the point $(x, z)$ is a cluster point of the sequence $((x, y_n))$ in $X \\times X$.\n10. Since all terms $(x, y_n)$ for $n \\ge k$ lie in $G_k$, the cluster point $(x, z)$ must lie in the closure $\\overline{G_k}$.\n11. Recall our construction in Step 2.1: $\\overline{G_{k+1}} \\subseteq G_k$.\n12. Since $(x, z) \\in \\overline{G_{k+1}}$ (applying the logic from step 10 to $k+1$), we have $(x, z) \\in G_k$.\n13. This holds for *all* $k$. Therefore, $(x, z) \\in \\bigcap_{k=1}^\\infty G_k = \\Delta$.\n14. This implies $x = z$.\n15. This contradicts the fact that $x \\in O$ and $z \\in X \\setminus O$.\n\nTherefore, the assumption was false. There exists some $N$ such that $St(x, \\mathcal{U}_N) \\subseteq O$.\n\n**Conclusion of Step 2.3:**\nSince $St(x, \\mathcal{U}_N)$ is a union of finitely many sets from $\\mathcal{U}_N$ (all of which contain $x$), any single $U \\in \\mathcal{U}_N$ containing $x$ satisfies $x \\in U \\subseteq St(x, \\mathcal{U}_N) \\subseteq O$. Since $U \\in \\mathcal{U}_N \\subset \\mathcal{B}$, we have found a basis element inside $O$ containing $x$. Thus, $\\mathcal{B}$ is a countable basis for the topology of $X$.\n\n### Step 2.4: Final Conclusion\n\n1.  $X$ is a compact Hausdorff space.\n2.  We have shown that if $\\Delta$ is a $G_\\delta$ set, then $X$ admits a countable basis (is second-countable).\n3.  By the Urysohn Metrization Theorem (or the specific version for compact spaces: A compact Hausdorff space is metrizable iff it is second-countable), $X$ is metrizable.\n\nThus, both directions are proven.\n\n**Q.E.D.**",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ],
    "outer_attempt": 1
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "Let $X = S^1 \\vee S^1$ be the wedge sum of two circles, where the wedge point is $x_0$. Compute $\\pi_1(X, x_0)$.",
    "run_id": "39",
    "topic_slug": "algebraic_topology",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "To compute the fundamental group $\\pi_1(X, x_0)$ of the wedge sum of two circles $X = S^1 \\vee S^1$, we will utilize the **Seifert-van Kampen Theorem**.\n\n### 1. Geometric Setup and Definitions\n\nLet $X$ be the wedge sum of two circles, often referred to as the \"figure-eight\" space. We can represent $X$ as the union of two circles, $C_1$ and $C_2$, which intersect at a single point, the wedge point $x_0$.\n$$X = C_1 \\cup C_2, \\quad C_1 \\cap C_2 = \\{x_0\\}$$\nSince $C_1$ and $C_2$ are homeomorphic to the standard circle $S^1$, we know that $\\pi_1(C_1, x_0) \\cong \\mathbb{Z}$ and $\\pi_1(C_2, x_0) \\cong \\mathbb{Z}$.\n\n### 2. Choosing an Open Cover\n\nTo apply the Seifert-van Kampen Theorem, we must choose open subsets $U$ and $V$ of $X$ such that:\n1.  $X = U \\cup V$.\n2.  $U$, $V$, and $U \\cap V$ are path-connected.\n3.  The base point $x_0 \\in U \\cap V$.\n\nLet $y \\in C_2$ be a point distinct from $x_0$, and let $z \\in C_1$ be a point distinct from $x_0$. We define the open sets as follows:\n*   $U = X \\setminus \\{y\\} = C_1 \\cup (C_2 \\setminus \\{y\\})$\n*   $V = X \\setminus \\{z\\} = (C_1 \\setminus \\{z\\}) \\cup C_2$\n\n**Analysis of the open sets:**\n*   **Set $U$**: The space $C_2 \\setminus \\{y\\}$ is homeomorphic to an open interval. Since this interval is attached to $C_1$ at the point $x_0$, $U$ deformation retracts onto the circle $C_1$. Therefore, $U$ is homotopy equivalent to $S^1$.\n    $$ \\pi_1(U, x_0) \\cong \\pi_1(S^1, x_0) \\cong \\mathbb{Z} $$\n    Let $a$ denote the generator of this group (a loop traversing $C_1$).\n\n*   **Set $V$**: Similarly, $C_1 \\setminus \\{z\\}$ is homeomorphic to an open interval attached to $C_2$ at $x_0$. Thus, $V$ deformation retracts onto the circle $C_2$, making it homotopy equivalent to $S^1$.\n    $$ \\pi_1(V, x_0) \\cong \\pi_1(S^1, x_0) \\cong \\mathbb{Z} $$\n    Let $b$ denote the generator of this group (a loop traversing $C_2$).\n\n*   **Intersection $U \\cap V$**: The intersection is $X \\setminus \\{y, z\\} = (C_1 \\setminus \\{z\\}) \\cup (C_2 \\setminus \\{y\\})$. This space consists of two open intervals intersecting at the single point $x_0$. This forms a space that is star-shaped with respect to $x_0$ (specifically, it is homeomorphic to an open cross). Therefore, $U \\cap V$ is contractible (homotopy equivalent to a point).\n    $$ \\pi_1(U \\cap V, x_0) \\cong \\{1\\} $$\n    (The trivial group).\n\n### 3. Applying the Seifert-van Kampen Theorem\n\nThe theorem states that the fundamental group of $X$ is the amalgamated free product of the fundamental groups of $U$ and $V$ over the fundamental group of their intersection:\n$$ \\pi_1(X, x_0) \\cong \\pi_1(U, x_0) *_{\\pi_1(U \\cap V, x_0)} \\pi_1(V, x_0) $$\n\nSubstituting the groups we derived:\n1.  $\\pi_1(U, x_0) \\cong \\langle a \\rangle \\cong \\mathbb{Z}$\n2.  $\\pi_1(V, x_0) \\cong \\langle b \\rangle \\cong \\mathbb{Z}$\n3.  $\\pi_1(U \\cap V, x_0) \\cong \\{1\\}$\n\nThe amalgamation over the trivial group is simply the free product of the groups:\n$$ \\pi_1(X, x_0) \\cong \\mathbb{Z} *_{\\{1\\}} \\mathbb{Z} \\cong \\mathbb{Z} * \\mathbb{Z} $$\n\n### 4. Conclusion\n\nThe group $\\mathbb{Z} * \\mathbb{Z}$ is the **free group on two generators**. If we denote the generators corresponding to the loops around the two circles as $a$ and $b$, the fundamental group consists of all reduced words formed by $a, b, a^{-1}, b^{-1}$.\n\n**Final Answer:**\nThe fundamental group of $X = S^1 \\vee S^1$ is the free group on two generators, denoted as:\n$$ \\pi_1(X, x_0) \\cong F_2 \\cong \\mathbb{Z} * \\mathbb{Z} $$",
        "raw_answer": "To compute the fundamental group $\\pi_1(X, x_0)$ of the wedge sum of two circles $X = S^1 \\vee S^1$, we will utilize the **Seifert-van Kampen Theorem**.\n\n### 1. Geometric Setup and Definitions\n\nLet $X$ be the wedge sum of two circles, often referred to as the \"figure-eight\" space. We can represent $X$ as the union of two circles, $C_1$ and $C_2$, which intersect at a single point, the wedge point $x_0$.\n$$X = C_1 \\cup C_2, \\quad C_1 \\cap C_2 = \\{x_0\\}$$\nSince $C_1$ and $C_2$ are homeomorphic to the standard circle $S^1$, we know that $\\pi_1(C_1, x_0) \\cong \\mathbb{Z}$ and $\\pi_1(C_2, x_0) \\cong \\mathbb{Z}$.\n\n### 2. Choosing an Open Cover\n\nTo apply the Seifert-van Kampen Theorem, we must choose open subsets $U$ and $V$ of $X$ such that:\n1.  $X = U \\cup V$.\n2.  $U$, $V$, and $U \\cap V$ are path-connected.\n3.  The base point $x_0 \\in U \\cap V$.\n\nLet $y \\in C_2$ be a point distinct from $x_0$, and let $z \\in C_1$ be a point distinct from $x_0$. We define the open sets as follows:\n*   $U = X \\setminus \\{y\\} = C_1 \\cup (C_2 \\setminus \\{y\\})$\n*   $V = X \\setminus \\{z\\} = (C_1 \\setminus \\{z\\}) \\cup C_2$\n\n**Analysis of the open sets:**\n*   **Set $U$**: The space $C_2 \\setminus \\{y\\}$ is homeomorphic to an open interval. Since this interval is attached to $C_1$ at the point $x_0$, $U$ deformation retracts onto the circle $C_1$. Therefore, $U$ is homotopy equivalent to $S^1$.\n    $$ \\pi_1(U, x_0) \\cong \\pi_1(S^1, x_0) \\cong \\mathbb{Z} $$\n    Let $a$ denote the generator of this group (a loop traversing $C_1$).\n\n*   **Set $V$**: Similarly, $C_1 \\setminus \\{z\\}$ is homeomorphic to an open interval attached to $C_2$ at $x_0$. Thus, $V$ deformation retracts onto the circle $C_2$, making it homotopy equivalent to $S^1$.\n    $$ \\pi_1(V, x_0) \\cong \\pi_1(S^1, x_0) \\cong \\mathbb{Z} $$\n    Let $b$ denote the generator of this group (a loop traversing $C_2$).\n\n*   **Intersection $U \\cap V$**: The intersection is $X \\setminus \\{y, z\\} = (C_1 \\setminus \\{z\\}) \\cup (C_2 \\setminus \\{y\\})$. This space consists of two open intervals intersecting at the single point $x_0$. This forms a space that is star-shaped with respect to $x_0$ (specifically, it is homeomorphic to an open cross). Therefore, $U \\cap V$ is contractible (homotopy equivalent to a point).\n    $$ \\pi_1(U \\cap V, x_0) \\cong \\{1\\} $$\n    (The trivial group).\n\n### 3. Applying the Seifert-van Kampen Theorem\n\nThe theorem states that the fundamental group of $X$ is the amalgamated free product of the fundamental groups of $U$ and $V$ over the fundamental group of their intersection:\n$$ \\pi_1(X, x_0) \\cong \\pi_1(U, x_0) *_{\\pi_1(U \\cap V, x_0)} \\pi_1(V, x_0) $$\n\nSubstituting the groups we derived:\n1.  $\\pi_1(U, x_0) \\cong \\langle a \\rangle \\cong \\mathbb{Z}$\n2.  $\\pi_1(V, x_0) \\cong \\langle b \\rangle \\cong \\mathbb{Z}$\n3.  $\\pi_1(U \\cap V, x_0) \\cong \\{1\\}$\n\nThe amalgamation over the trivial group is simply the free product of the groups:\n$$ \\pi_1(X, x_0) \\cong \\mathbb{Z} *_{\\{1\\}} \\mathbb{Z} \\cong \\mathbb{Z} * \\mathbb{Z} $$\n\n### 4. Conclusion\n\nThe group $\\mathbb{Z} * \\mathbb{Z}$ is the **free group on two generators**. If we denote the generators corresponding to the loops around the two circles as $a$ and $b$, the fundamental group consists of all reduced words formed by $a, b, a^{-1}, b^{-1}$.\n\n**Final Answer:**\nThe fundamental group of $X = S^1 \\vee S^1$ is the free group on two generators, denoted as:\n$$ \\pi_1(X, x_0) \\cong F_2 \\cong \\mathbb{Z} * \\mathbb{Z} $$",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ],
    "outer_attempt": 1
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "## Step 1: Understand the task and the domain\nThe task is to generate a challenging but solvable mathematics problem in the domain of \"Manifolds and cell complexes\" along with a complete and verifiable solution. The problem should be well-posed, clear, non-trivial, and solvable with established mathematical methods.\n\n## Step 2: Identify a suitable topic within the domain\nA suitable topic within \"Manifolds and cell complexes\" could involve the properties or constructions related to manifolds or cell complexes, such as their topology, geometry, or algebraic invariants.\n\n## Step 3: Formulate a specific problem\nLet's consider a problem involving the Euler characteristic of a manifold, which is a fundamental concept in algebraic topology that relates to both manifolds and cell complexes. The Euler characteristic is a topological invariant that can be computed using a cell decomposition of a manifold.\n\n## Step 4: Construct the problem statement\nConsider a 2-dimensional manifold $M$ that is homeomorphic to a sphere with $g$ handles (i.e., a surface of genus $g$). The task is to prove that the Euler characteristic $\\chi(M)$ of $M$ is given by $\\chi(M) = 2 - 2g$.\n\n## Step 5: Provide the problem statement in the required format\n\nLet $M$ be a 2-dimensional manifold homeomorphic to a sphere with $g$ handles. Prove that the Euler characteristic $\\chi(M)$ of $M$ is given by $\\chi(M) = 2 - 2g$.",
    "run_id": "40",
    "topic_slug": "manifolds_and_cell_complexes",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "Here is the comprehensive solution to the problem.\n\n### Problem Statement\nLet $M$ be a 2-dimensional manifold homeomorphic to a sphere with $g$ handles (an orientable surface of genus $g$). Prove that the Euler characteristic $\\chi(M)$ of $M$ is given by $\\chi(M) = 2 - 2g$.\n\n---\n\n### Solution\n\nTo prove this result, we will construct a specific cell decomposition (CW-complex structure) for the surface $M$ and calculate the Euler characteristic using the definition $\\chi(M) = V - E + F$, where $V, E,$ and $F$ correspond to the number of 0-cells (vertices), 1-cells (edges), and 2-cells (faces), respectively.\n\nWe treat the cases $g=0$ and $g \\geq 1$ separately to ensure rigor, although the formula holds for both.\n\n#### Case 1: The Sphere ($g=0$)\nA sphere with 0 handles is simply the 2-sphere, $S^2$.\nWe can compute its Euler characteristic using a standard triangulation, such as the boundary of a tetrahedron.\n1.  **Vertices ($V$)**: 4\n2.  **Edges ($E$)**: 6\n3.  **Faces ($F$)**: 4\n\nCalculating $\\chi(S^2)$:\n$$ \\chi(S^2) = V - E + F = 4 - 6 + 4 = 2 $$\nUsing the formula $2 - 2g$ with $g=0$:\n$$ 2 - 2(0) = 2 $$\nThe formula holds for $g=0$.\n\n#### Case 2: Surface of Genus $g \\geq 1$\nFor $g \\geq 1$, the surface $M$ can be constructed as a quotient space of a regular polygon. This is the **standard polygonal presentation** of an orientable surface.\n\n**1. The Polygonal Model**\nA surface of genus $g$ is homeomorphic to the quotient of a regular $4g$-sided polygon $P$ in the Euclidean plane. The edges of the polygon are identified in pairs according to the standard word $W$:\n$$ W = a_1 b_1 a_1^{-1} b_1^{-1} a_2 b_2 a_2^{-1} b_2^{-1} \\dots a_g b_g a_g^{-1} b_g^{-1} $$\nHere, each letter represents an edge of the polygon. The exponent $-1$ indicates that the edge is traversed in the direction opposite to the boundary orientation.\n\n**2. Cell Decomposition**\nWe define a CW-complex structure on $M$ induced by this identification:\n*   **Faces ($F$):** The interior of the polygon $P$ forms a single 2-cell. Thus, $F = 1$.\n*   **Edges ($E$):** The boundary of the polygon consists of $4g$ edges. The identification word pairs every edge $x$ with a corresponding edge $x^{-1}$. Since every edge is identified with exactly one other edge, the number of distinct 1-cells in the quotient is:\n    $$ E = \\frac{4g}{2} = 2g $$\n\n**3. Vertices ($V$):**\nWe must determine how the vertices of the polygon are identified. Let the vertices of the $4g$-gon be $v_0, v_1, \\dots, v_{4g-1}$ in counterclockwise order.\nConsider the $i$-th handle represented by the sequence of edges $a_i, b_i, a_i^{-1}, b_i^{-1}$. Let the vertices involved in this sequence locally be $p_0, p_1, p_2, p_3, p_4$ such that:\n*   $a_i$ connects $p_0 \\to p_1$\n*   $b_i$ connects $p_1 \\to p_2$\n*   $a_i^{-1}$ connects $p_2 \\to p_3$\n*   $b_i^{-1}$ connects $p_3 \\to p_4$\n\nThe identification of edges dictates the identification of their endpoints:\n1.  **Identify $a_i$ with $a_i^{-1}$:**\n    The edge $a_i$ (directed $p_0 \\to p_1$) is identified with $a_i^{-1}$ (which corresponds to the geometric edge $p_2 \\to p_3$ traversed backwards, i.e., $p_3 \\to p_2$).\n    Therefore, the start of $a_i$ is identified with the start of the traversal of $a_i^{-1}$, and the end with the end.\n    $$ p_0 \\sim p_3 \\quad \\text{and} \\quad p_1 \\sim p_2 $$\n\n2.  **Identify $b_i$ with $b_i^{-1}$:**\n    The edge $b_i$ (directed $p_1 \\to p_2$) is identified with $b_i^{-1}$ (geometric edge $p_3 \\to p_4$ traversed backwards, i.e., $p_4 \\to p_3$).\n    $$ p_1 \\sim p_4 \\quad \\text{and} \\quad p_2 \\sim p_3 $$\n\n**Analyzing the Equivalence Class:**\nCombining these identifications for the $i$-th group:\n*   From $a_i$: $p_1 \\sim p_2$ and $p_0 \\sim p_3$.\n*   From $b_i$: $p_2 \\sim p_3$ and $p_1 \\sim p_4$.\n*   Chaining these: $p_0 \\sim p_3 \\sim p_2 \\sim p_1 \\sim p_4$.\n\nThus, all vertices associated with the $i$-th handle ($p_0$ through $p_4$) are identified to a single point. Since $p_4$ is the starting vertex ($p_0$) of the $(i+1)$-th handle group (or wraps around to the start of the polygon if $i=g$), the equivalence relation propagates through all $g$ groups.\n\nConsequently, all $4g$ vertices of the polygon are identified to a single vertex in the quotient manifold.\n$$ V = 1 $$\n\n**4. Calculation**\nWe now compute the Euler characteristic $\\chi(M)$:\n$$ \\chi(M) = V - E + F $$\nSubstituting the values derived above ($V=1, E=2g, F=1$):\n$$ \\chi(M) = 1 - 2g + 1 $$\n$$ \\chi(M) = 2 - 2g $$\n\n### Alternative Approach: Connected Sums\nWe can also verify this result using the properties of the connected sum operation $\\#$.\nA surface of genus $g$ is the connected sum of $g$ tori: $M_g = T^2 \\# T^2 \\# \\dots \\# T^2$.\nThe Euler characteristic of a connected sum is given by:\n$$ \\chi(A \\# B) = \\chi(A) + \\chi(B) - 2 $$\nWe proceed by induction:\n1.  **Base Case ($g=1$):** For a torus $T^2$, we have the identification $aba^{-1}b^{-1}$. Here $V=1, E=2, F=1$, so $\\chi(T^2) = 1 - 2 + 1 = 0$.\n    Formula: $2 - 2(1) = 0$. (Verified)\n2.  **Inductive Step:** Assume $\\chi(M_{k}) = 2 - 2k$.\n    $$ M_{k+1} = M_k \\# T^2 $$\n    $$ \\chi(M_{k+1}) = \\chi(M_k) + \\chi(T^2) - 2 $$\n    $$ \\chi(M_{k+1}) = (2 - 2k) + 0 - 2 $$\n    $$ \\chi(M_{k+1}) = 2 - 2k - 2 = 2 - 2(k+1) $$\n\nThis confirms the result for all $g \\geq 1$.\n\n### Conclusion\nWe have shown through explicit cell decomposition and the connected sum formula that for a 2-dimensional manifold $M$ homeomorphic to a sphere with $g$ handles, the Euler characteristic is:\n$$ \\chi(M) = 2 - 2g $$",
        "raw_answer": "Here is the comprehensive solution to the problem.\n\n### Problem Statement\nLet $M$ be a 2-dimensional manifold homeomorphic to a sphere with $g$ handles (an orientable surface of genus $g$). Prove that the Euler characteristic $\\chi(M)$ of $M$ is given by $\\chi(M) = 2 - 2g$.\n\n---\n\n### Solution\n\nTo prove this result, we will construct a specific cell decomposition (CW-complex structure) for the surface $M$ and calculate the Euler characteristic using the definition $\\chi(M) = V - E + F$, where $V, E,$ and $F$ correspond to the number of 0-cells (vertices), 1-cells (edges), and 2-cells (faces), respectively.\n\nWe treat the cases $g=0$ and $g \\geq 1$ separately to ensure rigor, although the formula holds for both.\n\n#### Case 1: The Sphere ($g=0$)\nA sphere with 0 handles is simply the 2-sphere, $S^2$.\nWe can compute its Euler characteristic using a standard triangulation, such as the boundary of a tetrahedron.\n1.  **Vertices ($V$)**: 4\n2.  **Edges ($E$)**: 6\n3.  **Faces ($F$)**: 4\n\nCalculating $\\chi(S^2)$:\n$$ \\chi(S^2) = V - E + F = 4 - 6 + 4 = 2 $$\nUsing the formula $2 - 2g$ with $g=0$:\n$$ 2 - 2(0) = 2 $$\nThe formula holds for $g=0$.\n\n#### Case 2: Surface of Genus $g \\geq 1$\nFor $g \\geq 1$, the surface $M$ can be constructed as a quotient space of a regular polygon. This is the **standard polygonal presentation** of an orientable surface.\n\n**1. The Polygonal Model**\nA surface of genus $g$ is homeomorphic to the quotient of a regular $4g$-sided polygon $P$ in the Euclidean plane. The edges of the polygon are identified in pairs according to the standard word $W$:\n$$ W = a_1 b_1 a_1^{-1} b_1^{-1} a_2 b_2 a_2^{-1} b_2^{-1} \\dots a_g b_g a_g^{-1} b_g^{-1} $$\nHere, each letter represents an edge of the polygon. The exponent $-1$ indicates that the edge is traversed in the direction opposite to the boundary orientation.\n\n**2. Cell Decomposition**\nWe define a CW-complex structure on $M$ induced by this identification:\n*   **Faces ($F$):** The interior of the polygon $P$ forms a single 2-cell. Thus, $F = 1$.\n*   **Edges ($E$):** The boundary of the polygon consists of $4g$ edges. The identification word pairs every edge $x$ with a corresponding edge $x^{-1}$. Since every edge is identified with exactly one other edge, the number of distinct 1-cells in the quotient is:\n    $$ E = \\frac{4g}{2} = 2g $$\n\n**3. Vertices ($V$):**\nWe must determine how the vertices of the polygon are identified. Let the vertices of the $4g$-gon be $v_0, v_1, \\dots, v_{4g-1}$ in counterclockwise order.\nConsider the $i$-th handle represented by the sequence of edges $a_i, b_i, a_i^{-1}, b_i^{-1}$. Let the vertices involved in this sequence locally be $p_0, p_1, p_2, p_3, p_4$ such that:\n*   $a_i$ connects $p_0 \\to p_1$\n*   $b_i$ connects $p_1 \\to p_2$\n*   $a_i^{-1}$ connects $p_2 \\to p_3$\n*   $b_i^{-1}$ connects $p_3 \\to p_4$\n\nThe identification of edges dictates the identification of their endpoints:\n1.  **Identify $a_i$ with $a_i^{-1}$:**\n    The edge $a_i$ (directed $p_0 \\to p_1$) is identified with $a_i^{-1}$ (which corresponds to the geometric edge $p_2 \\to p_3$ traversed backwards, i.e., $p_3 \\to p_2$).\n    Therefore, the start of $a_i$ is identified with the start of the traversal of $a_i^{-1}$, and the end with the end.\n    $$ p_0 \\sim p_3 \\quad \\text{and} \\quad p_1 \\sim p_2 $$\n\n2.  **Identify $b_i$ with $b_i^{-1}$:**\n    The edge $b_i$ (directed $p_1 \\to p_2$) is identified with $b_i^{-1}$ (geometric edge $p_3 \\to p_4$ traversed backwards, i.e., $p_4 \\to p_3$).\n    $$ p_1 \\sim p_4 \\quad \\text{and} \\quad p_2 \\sim p_3 $$\n\n**Analyzing the Equivalence Class:**\nCombining these identifications for the $i$-th group:\n*   From $a_i$: $p_1 \\sim p_2$ and $p_0 \\sim p_3$.\n*   From $b_i$: $p_2 \\sim p_3$ and $p_1 \\sim p_4$.\n*   Chaining these: $p_0 \\sim p_3 \\sim p_2 \\sim p_1 \\sim p_4$.\n\nThus, all vertices associated with the $i$-th handle ($p_0$ through $p_4$) are identified to a single point. Since $p_4$ is the starting vertex ($p_0$) of the $(i+1)$-th handle group (or wraps around to the start of the polygon if $i=g$), the equivalence relation propagates through all $g$ groups.\n\nConsequently, all $4g$ vertices of the polygon are identified to a single vertex in the quotient manifold.\n$$ V = 1 $$\n\n**4. Calculation**\nWe now compute the Euler characteristic $\\chi(M)$:\n$$ \\chi(M) = V - E + F $$\nSubstituting the values derived above ($V=1, E=2g, F=1$):\n$$ \\chi(M) = 1 - 2g + 1 $$\n$$ \\chi(M) = 2 - 2g $$\n\n### Alternative Approach: Connected Sums\nWe can also verify this result using the properties of the connected sum operation $\\#$.\nA surface of genus $g$ is the connected sum of $g$ tori: $M_g = T^2 \\# T^2 \\# \\dots \\# T^2$.\nThe Euler characteristic of a connected sum is given by:\n$$ \\chi(A \\# B) = \\chi(A) + \\chi(B) - 2 $$\nWe proceed by induction:\n1.  **Base Case ($g=1$):** For a torus $T^2$, we have the identification $aba^{-1}b^{-1}$. Here $V=1, E=2, F=1$, so $\\chi(T^2) = 1 - 2 + 1 = 0$.\n    Formula: $2 - 2(1) = 0$. (Verified)\n2.  **Inductive Step:** Assume $\\chi(M_{k}) = 2 - 2k$.\n    $$ M_{k+1} = M_k \\# T^2 $$\n    $$ \\chi(M_{k+1}) = \\chi(M_k) + \\chi(T^2) - 2 $$\n    $$ \\chi(M_{k+1}) = (2 - 2k) + 0 - 2 $$\n    $$ \\chi(M_{k+1}) = 2 - 2k - 2 = 2 - 2(k+1) $$\n\nThis confirms the result for all $g \\geq 1$.\n\n### Conclusion\nWe have shown through explicit cell decomposition and the connected sum formula that for a 2-dimensional manifold $M$ homeomorphic to a sphere with $g$ handles, the Euler characteristic is:\n$$ \\chi(M) = 2 - 2g $$",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ],
    "outer_attempt": 1
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "## Step 1: Understand the task and the domain\nThe task is to generate a challenging but solvable mathematics problem in the domain of \"Global analysis, analysis on manifolds\" along with a complete and verifiable solution. The problem should be well-posed, clear, non-trivial, and solvable with established mathematical methods.\n\n## Step 2: Identify key concepts in the domain\nGlobal analysis and analysis on manifolds involve the study of geometric and topological properties of manifolds using analytical techniques. Key concepts include differential forms, integration on manifolds, Stokes' theorem, and the study of differential equations on manifolds.\n\n## Step 3: Formulate a problem\nTo create a problem, we need to identify a specific concept or technique within global analysis or analysis on manifolds that can be used to formulate a challenging yet solvable question. One such concept is the application of Stokes' theorem, which relates the integral of a differential form over a manifold to the integral of its exterior derivative over the boundary of the manifold.\n\n## Step 4: Construct the problem\nLet's consider a specific manifold and a differential form. A common example is the unit sphere $S^2$ in $\\mathbb{R}^3$. We can formulate a problem involving the integration of a differential form over $S^2$ and applying Stokes' theorem.\n\n## Step 5: Specify the problem\nLet $\\omega = y \\, dx \\wedge dz$ be a 2-form on $\\mathbb{R}^3$. Consider the unit sphere $S^2 = \\{ (x, y, z) \\in \\mathbb{R}^3 : x^2 + y^2 + z^2 = 1 \\}$ oriented with the outward unit normal. Evaluate $\\int_{S^2} \\omega$.\n\n## Step 6: Provide the solution\nTo solve this, we can directly integrate $\\omega$ over $S^2$ or apply Stokes' theorem. Stokes' theorem states that $\\int_M d\\omega = \\int_{\\partial M} \\omega$, where $M$ is a manifold with boundary $\\partial M$. Here, we can consider $M = B^3 = \\{ (x, y, z) \\in \\mathbb{R}^3 : x^2 + y^2 + z^2 \\leq 1 \\}$, so $\\partial M = S^2$.\n\n## Step 7: Calculate $d\\omega$\n$d\\omega = d(y \\, dx \\wedge dz) = dy \\wedge dx \\wedge dz = -dx \\wedge dy \\wedge dz$ because $d(y) = dy$, $d(dx) = 0$, and $d(dz) = 0$. So, $d\\omega = -dx \\wedge dy \\wedge dz$.\n\n## Step 8: Apply Stokes' theorem\n$\\int_{S^2} \\omega = \\int_{B^3} d\\omega = \\int_{B^3} -dx \\wedge dy \\wedge dz = -\\int_{B^3} dV$, where $dV = dx \\wedge dy \\wedge dz$ is the volume form on $\\mathbb{R}^3$. Thus, $\\int_{S^2} \\omega = -\\text{Vol}(B^3)$.\n\n## Step 9: Calculate the volume of $B^3$\nThe volume of the unit ball $B^3$ is given by $\\frac{4}{3}\\pi(1)^3 = \\frac{4}{3}\\pi$.\n\n## Step 10: Conclude the solution\n$\\int_{S^2} \\omega = -\\frac{4}{3}\\pi$.\n\nThe final answer is: \n\nLet $\\omega = y \\, dx \\wedge dz$ be a 2-form on $\\mathbb{R}^3$. Consider the unit sphere $S^2 = \\{ (x, y, z) \\in \\mathbb{R}^3 : x^2 + y^2 + z^2 = 1 \\}$ oriented with the outward unit normal. Evaluate $\\int_{S^2} \\omega$.",
    "run_id": "41",
    "topic_slug": "global_analysis_on_manifolds",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "To solve the problem of evaluating the integral of the differential form $\\omega$ over the unit sphere $S^2$, we will utilize Stokes' Theorem (specifically, the Generalized Stokes' Theorem for manifolds with boundary).\n\n### 1. Problem Statement\n\nLet $\\omega = y \\, dx \\wedge dz$ be a 2-form on $\\mathbb{R}^3$. Let $S^2$ be the unit sphere defined by:\n$$S^2 = \\{ (x, y, z) \\in \\mathbb{R}^3 : x^2 + y^2 + z^2 = 1 \\}$$\noriented with the outward unit normal vector field. We wish to evaluate:\n$$I = \\int_{S^2} \\omega$$\n\n### 2. Method Selection: Stokes' Theorem\n\nThe Generalized Stokes' Theorem states that for a compact, oriented smooth manifold $M$ with boundary $\\partial M$, and a smooth differential form $\\alpha$ of degree $k-1$ (where $k$ is the dimension of $M$):\n$$\\int_{\\partial M} \\alpha = \\int_{M} d\\alpha$$\n\nIn this problem:\n1.  **The Manifold $M$**: Let $M$ be the closed unit ball $B^3$:\n    $$B^3 = \\{ (x, y, z) \\in \\mathbb{R}^3 : x^2 + y^2 + z^2 \\leq 1 \\}$$\n2.  **The Boundary $\\partial M$**: The boundary of the unit ball is the unit sphere, $\\partial B^3 = S^2$.\n3.  **Orientation**: The problem specifies that $S^2$ is oriented with the outward unit normal. This is the standard induced orientation consistent with the standard orientation of $\\mathbb{R}^3$ (and thus $B^3$) required for Stokes' Theorem.\n4.  **The Form**: The form $\\omega = y \\, dx \\wedge dz$ is a smooth 2-form defined on all of $\\mathbb{R}^3$.\n\nTherefore, we can convert the surface integral into a volume integral over the unit ball:\n$$\\int_{S^2} \\omega = \\int_{B^3} d\\omega$$\n\n### 3. Calculating the Exterior Derivative\n\nWe compute the exterior derivative $d\\omega$. The exterior derivative is linear and satisfies the product rule for wedge products.\n\nGiven:\n$$\\omega = y \\, dx \\wedge dz$$\n\nDifferentiating $\\omega$:\n$$d\\omega = d(y) \\wedge dx \\wedge dz$$\n\nSince $d(y) = dy$, we have:\n$$d\\omega = dy \\wedge dx \\wedge dz$$\n\nWe must reorder the terms to match the standard volume form $dx \\wedge dy \\wedge dz$. Recall that the wedge product of 1-forms is anticommutative ($du \\wedge dv = -dv \\wedge du$). We swap $dy$ and $dx$:\n$$dy \\wedge dx = -(dx \\wedge dy)$$\n\nSubstituting this back into our expression for $d\\omega$:\n$$d\\omega = -(dx \\wedge dy) \\wedge dz = -dx \\wedge dy \\wedge dz$$\n\n### 4. Evaluating the Volume Integral\n\nNow we substitute $d\\omega$ into the integral over the unit ball $B^3$:\n\n$$\\int_{S^2} \\omega = \\int_{B^3} (-dx \\wedge dy \\wedge dz) = -\\int_{B^3} dx \\wedge dy \\wedge dz$$\n\nThe integral $\\int_{B^3} dx \\wedge dy \\wedge dz$ represents the standard Euclidean volume of the unit ball $B^3$.\n\nThe volume $V$ of a ball of radius $r$ in $\\mathbb{R}^3$ is given by the formula:\n$$V = \\frac{4}{3}\\pi r^3$$\n\nFor the unit ball, $r = 1$:\n$$\\text{Vol}(B^3) = \\frac{4}{3}\\pi (1)^3 = \\frac{4}{3}\\pi$$\n\nSubstituting this value back into our equation:\n$$\\int_{S^2} \\omega = -\\left( \\frac{4}{3}\\pi \\right) = -\\frac{4}{3}\\pi$$\n\n### 5. Conclusion\n\nThe value of the integral is:\n$$-\\frac{4}{3}\\pi$$",
        "raw_answer": "To solve the problem of evaluating the integral of the differential form $\\omega$ over the unit sphere $S^2$, we will utilize Stokes' Theorem (specifically, the Generalized Stokes' Theorem for manifolds with boundary).\n\n### 1. Problem Statement\n\nLet $\\omega = y \\, dx \\wedge dz$ be a 2-form on $\\mathbb{R}^3$. Let $S^2$ be the unit sphere defined by:\n$$S^2 = \\{ (x, y, z) \\in \\mathbb{R}^3 : x^2 + y^2 + z^2 = 1 \\}$$\noriented with the outward unit normal vector field. We wish to evaluate:\n$$I = \\int_{S^2} \\omega$$\n\n### 2. Method Selection: Stokes' Theorem\n\nThe Generalized Stokes' Theorem states that for a compact, oriented smooth manifold $M$ with boundary $\\partial M$, and a smooth differential form $\\alpha$ of degree $k-1$ (where $k$ is the dimension of $M$):\n$$\\int_{\\partial M} \\alpha = \\int_{M} d\\alpha$$\n\nIn this problem:\n1.  **The Manifold $M$**: Let $M$ be the closed unit ball $B^3$:\n    $$B^3 = \\{ (x, y, z) \\in \\mathbb{R}^3 : x^2 + y^2 + z^2 \\leq 1 \\}$$\n2.  **The Boundary $\\partial M$**: The boundary of the unit ball is the unit sphere, $\\partial B^3 = S^2$.\n3.  **Orientation**: The problem specifies that $S^2$ is oriented with the outward unit normal. This is the standard induced orientation consistent with the standard orientation of $\\mathbb{R}^3$ (and thus $B^3$) required for Stokes' Theorem.\n4.  **The Form**: The form $\\omega = y \\, dx \\wedge dz$ is a smooth 2-form defined on all of $\\mathbb{R}^3$.\n\nTherefore, we can convert the surface integral into a volume integral over the unit ball:\n$$\\int_{S^2} \\omega = \\int_{B^3} d\\omega$$\n\n### 3. Calculating the Exterior Derivative\n\nWe compute the exterior derivative $d\\omega$. The exterior derivative is linear and satisfies the product rule for wedge products.\n\nGiven:\n$$\\omega = y \\, dx \\wedge dz$$\n\nDifferentiating $\\omega$:\n$$d\\omega = d(y) \\wedge dx \\wedge dz$$\n\nSince $d(y) = dy$, we have:\n$$d\\omega = dy \\wedge dx \\wedge dz$$\n\nWe must reorder the terms to match the standard volume form $dx \\wedge dy \\wedge dz$. Recall that the wedge product of 1-forms is anticommutative ($du \\wedge dv = -dv \\wedge du$). We swap $dy$ and $dx$:\n$$dy \\wedge dx = -(dx \\wedge dy)$$\n\nSubstituting this back into our expression for $d\\omega$:\n$$d\\omega = -(dx \\wedge dy) \\wedge dz = -dx \\wedge dy \\wedge dz$$\n\n### 4. Evaluating the Volume Integral\n\nNow we substitute $d\\omega$ into the integral over the unit ball $B^3$:\n\n$$\\int_{S^2} \\omega = \\int_{B^3} (-dx \\wedge dy \\wedge dz) = -\\int_{B^3} dx \\wedge dy \\wedge dz$$\n\nThe integral $\\int_{B^3} dx \\wedge dy \\wedge dz$ represents the standard Euclidean volume of the unit ball $B^3$.\n\nThe volume $V$ of a ball of radius $r$ in $\\mathbb{R}^3$ is given by the formula:\n$$V = \\frac{4}{3}\\pi r^3$$\n\nFor the unit ball, $r = 1$:\n$$\\text{Vol}(B^3) = \\frac{4}{3}\\pi (1)^3 = \\frac{4}{3}\\pi$$\n\nSubstituting this value back into our equation:\n$$\\int_{S^2} \\omega = -\\left( \\frac{4}{3}\\pi \\right) = -\\frac{4}{3}\\pi$$\n\n### 5. Conclusion\n\nThe value of the integral is:\n$$-\\frac{4}{3}\\pi$$",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ],
    "outer_attempt": 1
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "Let $X$ and $Y$ be independent random variables with $X \\sim \\text{Exp}(\\lambda)$ and $Y \\sim \\text{Exp}(\\mu)$, where $\\lambda > 0$ and $\\mu > 0$ are the rate parameters. Define $Z = \\min(X, Y)$. Derive the probability density function (pdf) of $Z$ and compute $\\mathbb{E}[Z]$.",
    "run_id": "42",
    "topic_slug": "probability_theory_and_stochastic_processes",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "To find the probability density function (pdf) and expected value of $Z = \\min(X, Y)$, where $X$ and $Y$ are independent exponential random variables, we proceed as follows.\n\n### 1. Preliminaries and Definitions\n\nLet $X \\sim \\text{Exp}(\\lambda)$ and $Y \\sim \\text{Exp}(\\mu)$ with $\\lambda > 0$ and $\\mu > 0$. The probability density functions (pdf) for $X$ and $Y$ are given by:\n$$\nf_X(x) = \\begin{cases} \\lambda e^{-\\lambda x} & x \\ge 0 \\\\ 0 & x < 0 \\end{cases}\n$$\n$$\nf_Y(y) = \\begin{cases} \\mu e^{-\\mu y} & y \\ge 0 \\\\ 0 & y < 0 \\end{cases}\n$$\nThe cumulative distribution functions (CDF) are:\n$$\nF_X(x) = P(X \\le x) = \\begin{cases} 1 - e^{-\\lambda x} & x \\ge 0 \\\\ 0 & x < 0 \\end{cases}\n$$\n$$\nF_Y(y) = P(Y \\le y) = \\begin{cases} 1 - e^{-\\mu y} & y \\ge 0 \\\\ 0 & y < 0 \\end{cases}\n$$\nIt is often convenient to work with the **survival function** (or complementary CDF), defined as $S(t) = P(T > t) = 1 - F(t)$. For our variables:\n$$\nS_X(x) = P(X > x) = e^{-\\lambda x} \\quad \\text{for } x \\ge 0\n$$\n$$\nS_Y(y) = P(Y > y) = e^{-\\mu y} \\quad \\text{for } y \\ge 0\n$$\n\n### 2. Deriving the Distribution of $Z$\n\nLet $Z = \\min(X, Y)$. To find the pdf of $Z$, we first determine its survival function $S_Z(z) = P(Z > z)$.\n\nBy the definition of the minimum, $\\min(X, Y) > z$ if and only if **both** $X > z$ and $Y > z$. Thus:\n$$\nP(Z > z) = P(X > z \\cap Y > z)\n$$\nSince $X$ and $Y$ are independent random variables, the probability of their intersection is the product of their individual probabilities:\n$$\nP(Z > z) = P(X > z) \\cdot P(Y > z)\n$$\n\nNow we consider the domain of $z$.\n\n**Case 1: $z < 0$**\nSince $X$ and $Y$ are non-negative random variables, $Z$ is always non-negative. Therefore, for any $z < 0$, $P(Z > z) = 1$.\n\n**Case 2: $z \\ge 0$**\nUsing the survival functions defined in Section 1:\n$$\n\\begin{aligned}\nS_Z(z) &= e^{-\\lambda z} \\cdot e^{-\\mu z} \\\\\n&= e^{-(\\lambda + \\mu)z}\n\\end{aligned}\n$$\n\nCombining these cases, the CDF of $Z$, denoted $F_Z(z) = 1 - S_Z(z)$, is:\n$$\nF_Z(z) = \\begin{cases} 1 - e^{-(\\lambda + \\mu)z} & z \\ge 0 \\\\ 0 & z < 0 \\end{cases}\n$$\n\n### 3. Probability Density Function (pdf) of $Z$\n\nThe pdf $f_Z(z)$ is the derivative of the CDF with respect to $z$ wherever the derivative exists.\n$$\nf_Z(z) = \\frac{d}{dz} F_Z(z)\n$$\nFor $z > 0$:\n$$\nf_Z(z) = \\frac{d}{dz} \\left( 1 - e^{-(\\lambda + \\mu)z} \\right) = -(-(\\lambda + \\mu)) e^{-(\\lambda + \\mu)z} = (\\lambda + \\mu) e^{-(\\lambda + \\mu)z}\n$$\nFor $z < 0$, $f_Z(z) = 0$.\n\nThus, the pdf of $Z$ is:\n$$\nf_Z(z) = \\begin{cases} (\\lambda + \\mu) e^{-(\\lambda + \\mu)z} & z \\ge 0 \\\\ 0 & z < 0 \\end{cases}\n$$\n**Observation:** By inspection of the pdf, we observe that $Z$ follows an Exponential distribution with rate parameter $(\\lambda + \\mu)$. That is, $Z \\sim \\text{Exp}(\\lambda + \\mu)$.\n\n### 4. Computing the Expected Value $\\mathbb{E}[Z]$\n\nThe expected value of a continuous random variable with pdf $f_Z(z)$ is defined as:\n$$\n\\mathbb{E}[Z] = \\int_{-\\infty}^{\\infty} z f_Z(z) \\, dz\n$$\nSubstituting our derived pdf:\n$$\n\\mathbb{E}[Z] = \\int_{0}^{\\infty} z (\\lambda + \\mu) e^{-(\\lambda + \\mu)z} \\, dz\n$$\nWe can evaluate this integral using integration by parts or by recognizing the standard property of the exponential distribution.\n\n**Method A: Using Standard Properties**\nSince we identified that $Z \\sim \\text{Exp}(\\lambda + \\mu)$, and the mean of an $\\text{Exp}(r)$ distribution is $1/r$, it follows directly that:\n$$\n\\mathbb{E}[Z] = \\frac{1}{\\lambda + \\mu}\n$$\n\n**Method B: Direct Integration**\nLet $k = \\lambda + \\mu$. We compute $\\int_{0}^{\\infty} k z e^{-k z} \\, dz$.\nIntegration by parts:\nLet $u = z$ and $dv = k e^{-kz} dz$.\nThen $du = dz$ and $v = -e^{-kz}$.\n$$\n\\begin{aligned}\n\\int_{0}^{\\infty} k z e^{-k z} \\, dz &= \\left[ -z e^{-kz} \\right]_0^\\infty - \\int_{0}^{\\infty} -e^{-kz} \\, dz \\\\\n&= \\left( \\lim_{z \\to \\infty} \\frac{-z}{e^{kz}} - 0 \\right) + \\int_{0}^{\\infty} e^{-kz} \\, dz\n\\end{aligned}\n$$\nUsing L'H\u00f4pital's rule, the limit term vanishes (since $k > 0$).\n$$\n\\begin{aligned}\n&= 0 + \\left[ -\\frac{1}{k} e^{-kz} \\right]_0^\\infty \\\\\n&= 0 - \\left( -\\frac{1}{k} \\right) \\\\\n&= \\frac{1}{k}\n\\end{aligned}\n$$\nSubstituting $k = \\lambda + \\mu$, we get $\\mathbb{E}[Z] = \\frac{1}{\\lambda + \\mu}$.\n\n### Conclusion\n\nThe probability density function of $Z$ is:\n$$\nf_Z(z) = \\begin{cases} (\\lambda + \\mu) e^{-(\\lambda + \\mu)z} & z \\ge 0 \\\\ 0 & z < 0 \\end{cases}\n$$\nThe expected value of $Z$ is:\n$$\n\\mathbb{E}[Z] = \\frac{1}{\\lambda + \\mu}\n$$",
        "raw_answer": "To find the probability density function (pdf) and expected value of $Z = \\min(X, Y)$, where $X$ and $Y$ are independent exponential random variables, we proceed as follows.\n\n### 1. Preliminaries and Definitions\n\nLet $X \\sim \\text{Exp}(\\lambda)$ and $Y \\sim \\text{Exp}(\\mu)$ with $\\lambda > 0$ and $\\mu > 0$. The probability density functions (pdf) for $X$ and $Y$ are given by:\n$$\nf_X(x) = \\begin{cases} \\lambda e^{-\\lambda x} & x \\ge 0 \\\\ 0 & x < 0 \\end{cases}\n$$\n$$\nf_Y(y) = \\begin{cases} \\mu e^{-\\mu y} & y \\ge 0 \\\\ 0 & y < 0 \\end{cases}\n$$\nThe cumulative distribution functions (CDF) are:\n$$\nF_X(x) = P(X \\le x) = \\begin{cases} 1 - e^{-\\lambda x} & x \\ge 0 \\\\ 0 & x < 0 \\end{cases}\n$$\n$$\nF_Y(y) = P(Y \\le y) = \\begin{cases} 1 - e^{-\\mu y} & y \\ge 0 \\\\ 0 & y < 0 \\end{cases}\n$$\nIt is often convenient to work with the **survival function** (or complementary CDF), defined as $S(t) = P(T > t) = 1 - F(t)$. For our variables:\n$$\nS_X(x) = P(X > x) = e^{-\\lambda x} \\quad \\text{for } x \\ge 0\n$$\n$$\nS_Y(y) = P(Y > y) = e^{-\\mu y} \\quad \\text{for } y \\ge 0\n$$\n\n### 2. Deriving the Distribution of $Z$\n\nLet $Z = \\min(X, Y)$. To find the pdf of $Z$, we first determine its survival function $S_Z(z) = P(Z > z)$.\n\nBy the definition of the minimum, $\\min(X, Y) > z$ if and only if **both** $X > z$ and $Y > z$. Thus:\n$$\nP(Z > z) = P(X > z \\cap Y > z)\n$$\nSince $X$ and $Y$ are independent random variables, the probability of their intersection is the product of their individual probabilities:\n$$\nP(Z > z) = P(X > z) \\cdot P(Y > z)\n$$\n\nNow we consider the domain of $z$.\n\n**Case 1: $z < 0$**\nSince $X$ and $Y$ are non-negative random variables, $Z$ is always non-negative. Therefore, for any $z < 0$, $P(Z > z) = 1$.\n\n**Case 2: $z \\ge 0$**\nUsing the survival functions defined in Section 1:\n$$\n\\begin{aligned}\nS_Z(z) &= e^{-\\lambda z} \\cdot e^{-\\mu z} \\\\\n&= e^{-(\\lambda + \\mu)z}\n\\end{aligned}\n$$\n\nCombining these cases, the CDF of $Z$, denoted $F_Z(z) = 1 - S_Z(z)$, is:\n$$\nF_Z(z) = \\begin{cases} 1 - e^{-(\\lambda + \\mu)z} & z \\ge 0 \\\\ 0 & z < 0 \\end{cases}\n$$\n\n### 3. Probability Density Function (pdf) of $Z$\n\nThe pdf $f_Z(z)$ is the derivative of the CDF with respect to $z$ wherever the derivative exists.\n$$\nf_Z(z) = \\frac{d}{dz} F_Z(z)\n$$\nFor $z > 0$:\n$$\nf_Z(z) = \\frac{d}{dz} \\left( 1 - e^{-(\\lambda + \\mu)z} \\right) = -(-(\\lambda + \\mu)) e^{-(\\lambda + \\mu)z} = (\\lambda + \\mu) e^{-(\\lambda + \\mu)z}\n$$\nFor $z < 0$, $f_Z(z) = 0$.\n\nThus, the pdf of $Z$ is:\n$$\nf_Z(z) = \\begin{cases} (\\lambda + \\mu) e^{-(\\lambda + \\mu)z} & z \\ge 0 \\\\ 0 & z < 0 \\end{cases}\n$$\n**Observation:** By inspection of the pdf, we observe that $Z$ follows an Exponential distribution with rate parameter $(\\lambda + \\mu)$. That is, $Z \\sim \\text{Exp}(\\lambda + \\mu)$.\n\n### 4. Computing the Expected Value $\\mathbb{E}[Z]$\n\nThe expected value of a continuous random variable with pdf $f_Z(z)$ is defined as:\n$$\n\\mathbb{E}[Z] = \\int_{-\\infty}^{\\infty} z f_Z(z) \\, dz\n$$\nSubstituting our derived pdf:\n$$\n\\mathbb{E}[Z] = \\int_{0}^{\\infty} z (\\lambda + \\mu) e^{-(\\lambda + \\mu)z} \\, dz\n$$\nWe can evaluate this integral using integration by parts or by recognizing the standard property of the exponential distribution.\n\n**Method A: Using Standard Properties**\nSince we identified that $Z \\sim \\text{Exp}(\\lambda + \\mu)$, and the mean of an $\\text{Exp}(r)$ distribution is $1/r$, it follows directly that:\n$$\n\\mathbb{E}[Z] = \\frac{1}{\\lambda + \\mu}\n$$\n\n**Method B: Direct Integration**\nLet $k = \\lambda + \\mu$. We compute $\\int_{0}^{\\infty} k z e^{-k z} \\, dz$.\nIntegration by parts:\nLet $u = z$ and $dv = k e^{-kz} dz$.\nThen $du = dz$ and $v = -e^{-kz}$.\n$$\n\\begin{aligned}\n\\int_{0}^{\\infty} k z e^{-k z} \\, dz &= \\left[ -z e^{-kz} \\right]_0^\\infty - \\int_{0}^{\\infty} -e^{-kz} \\, dz \\\\\n&= \\left( \\lim_{z \\to \\infty} \\frac{-z}{e^{kz}} - 0 \\right) + \\int_{0}^{\\infty} e^{-kz} \\, dz\n\\end{aligned}\n$$\nUsing L'H\u00f4pital's rule, the limit term vanishes (since $k > 0$).\n$$\n\\begin{aligned}\n&= 0 + \\left[ -\\frac{1}{k} e^{-kz} \\right]_0^\\infty \\\\\n&= 0 - \\left( -\\frac{1}{k} \\right) \\\\\n&= \\frac{1}{k}\n\\end{aligned}\n$$\nSubstituting $k = \\lambda + \\mu$, we get $\\mathbb{E}[Z] = \\frac{1}{\\lambda + \\mu}$.\n\n### Conclusion\n\nThe probability density function of $Z$ is:\n$$\nf_Z(z) = \\begin{cases} (\\lambda + \\mu) e^{-(\\lambda + \\mu)z} & z \\ge 0 \\\\ 0 & z < 0 \\end{cases}\n$$\nThe expected value of $Z$ is:\n$$\n\\mathbb{E}[Z] = \\frac{1}{\\lambda + \\mu}\n$$",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ],
    "outer_attempt": 1
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "Let $X$ and $Y$ be independent random variables with $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ and $Y \\sim \\mathcal{N}(0, 1)$, where $\\mu \\in \\mathbb{R}$ and $\\sigma > 0$ are unknown parameters. Suppose we have a sample of $n$ independent observations from $X$ and $m$ independent observations from $Y$, denoted as $x_1, x_2, \\ldots, x_n$ and $y_1, y_2, \\ldots, y_m$, respectively. Derive the likelihood ratio test statistic for testing $H_0: \\mu = 0, \\sigma = 1$ against $H_1: \\mu \\neq 0 \\text{ or } \\sigma \\neq 1$.",
    "run_id": "43",
    "topic_slug": "statistics",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "To derive the likelihood ratio test (LRT) statistic for the given hypotheses, we follow the standard procedure of constructing the likelihood function, maximizing it under the null and alternative hypotheses, and forming the ratio.\n\n### 1. The Likelihood Function\n\nLet the observed data be $\\mathbf{x} = (x_1, \\dots, x_n)$ and $\\mathbf{y} = (y_1, \\dots, y_m)$.\nThe random variables are defined as:\n*   $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ with unknown parameters $\\theta = (\\mu, \\sigma^2) \\in \\mathbb{R} \\times (0, \\infty)$.\n*   $Y \\sim \\mathcal{N}(0, 1)$ with no unknown parameters.\n*   $X$ and $Y$ are independent.\n\nThe joint likelihood function $L(\\mu, \\sigma^2; \\mathbf{x}, \\mathbf{y})$ is the product of the marginal densities due to independence:\n$$ L(\\mu, \\sigma^2) = \\left( \\prod_{i=1}^n f_X(x_i; \\mu, \\sigma^2) \\right) \\cdot \\left( \\prod_{j=1}^m f_Y(y_j) \\right) $$\n\nSubstituting the probability density functions for the normal distribution:\n$$ L(\\mu, \\sigma^2) = \\left( \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left\\{-\\frac{(x_i-\\mu)^2}{2\\sigma^2}\\right\\} \\right) \\cdot \\left( \\prod_{j=1}^m \\frac{1}{\\sqrt{2\\pi}} \\exp\\left\\{-\\frac{y_j^2}{2}\\right\\} \\right) $$\n\nLet $C(\\mathbf{y}) = \\prod_{j=1}^m \\frac{1}{\\sqrt{2\\pi}} \\exp\\left\\{-\\frac{y_j^2}{2}\\right\\}$. Since the distribution of $Y$ is completely specified (parameter-free), the term $C(\\mathbf{y})$ does not depend on $\\mu$ or $\\sigma^2$. It acts as a constant scaling factor in the likelihood with respect to the parameters.\n\nThe likelihood function simplifies to:\n$$ L(\\mu, \\sigma^2) = C(\\mathbf{y}) \\cdot (2\\pi\\sigma^2)^{-n/2} \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (x_i - \\mu)^2 \\right) $$\n\n### 2. Hypotheses and Parameter Spaces\n\nWe are testing:\n*   $H_0: \\mu = 0, \\sigma = 1$\n*   $H_1: \\mu \\neq 0 \\text{ or } \\sigma \\neq 1$\n\nThe likelihood ratio $\\Lambda$ is defined as:\n$$ \\Lambda = \\frac{\\sup_{(\\mu, \\sigma^2) \\in \\Theta_0} L(\\mu, \\sigma^2)}{\\sup_{(\\mu, \\sigma^2) \\in \\Theta} L(\\mu, \\sigma^2)} $$\nwhere $\\Theta_0 = \\{(0, 1)\\}$ and $\\Theta = \\mathbb{R} \\times (0, \\infty)$.\n\n### 3. Maximization under $H_0$ (Numerator)\n\nUnder $H_0$, the parameters are fixed at $\\mu = 0$ and $\\sigma^2 = 1$.\n$$ L_0 = L(0, 1) = C(\\mathbf{y}) \\cdot (2\\pi)^{-n/2} \\exp\\left( -\\frac{1}{2} \\sum_{i=1}^n x_i^2 \\right) $$\n\n### 4. Maximization under $H_1$ (Denominator)\n\nUnder the unrestricted parameter space, we must find the Maximum Likelihood Estimators (MLEs) for $\\mu$ and $\\sigma^2$ based on the $X$ sample. Standard calculus on the log-likelihood of a normal distribution yields:\n$$ \\hat{\\mu} = \\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i $$\n$$ \\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2 $$\n\nWe assume $n \\ge 2$ and not all $x_i$ are equal, ensuring $\\hat{\\sigma}^2 > 0$. Substituting these estimates back into the likelihood function:\n\nThe summation in the exponent becomes:\n$$ \\sum_{i=1}^n (x_i - \\hat{\\mu})^2 = n\\hat{\\sigma}^2 $$\n\nThus, the maximum likelihood value $L_1$ is:\n$$ L_1 = L(\\hat{\\mu}, \\hat{\\sigma}^2) = C(\\mathbf{y}) \\cdot (2\\pi \\hat{\\sigma}^2)^{-n/2} \\exp\\left( -\\frac{1}{2\\hat{\\sigma}^2} (n\\hat{\\sigma}^2) \\right) $$\n$$ L_1 = C(\\mathbf{y}) \\cdot (2\\pi)^{-n/2} (\\hat{\\sigma}^2)^{-n/2} e^{-n/2} $$\n\n### 5. Constructing the Likelihood Ratio Statistic\n\nNow we form the ratio $\\Lambda = \\frac{L_0}{L_1}$:\n\n$$ \\Lambda = \\frac{C(\\mathbf{y}) (2\\pi)^{-n/2} \\exp\\left( -\\frac{1}{2} \\sum x_i^2 \\right)}{C(\\mathbf{y}) (2\\pi)^{-n/2} (\\hat{\\sigma}^2)^{-n/2} e^{-n/2}} $$\n\nObserve that $C(\\mathbf{y})$ and $(2\\pi)^{-n/2}$ cancel out (confirming the $Y$ sample does not influence the test statistic structure):\n\n$$ \\Lambda = \\frac{\\exp\\left( -\\frac{1}{2} \\sum_{i=1}^n x_i^2 \\right)}{(\\hat{\\sigma}^2)^{-n/2} e^{-n/2}} $$\n$$ \\Lambda = (\\hat{\\sigma}^2)^{n/2} e^{n/2} \\exp\\left( -\\frac{1}{2} \\sum_{i=1}^n x_i^2 \\right) $$\n\nWe can simplify the exponent term using the identity $\\sum_{i=1}^n x_i^2 = \\sum_{i=1}^n (x_i - \\bar{x})^2 + n\\bar{x}^2 = n\\hat{\\sigma}^2 + n\\bar{x}^2$:\n\n$$ \\exp\\left( -\\frac{1}{2} \\sum_{i=1}^n x_i^2 \\right) = \\exp\\left( -\\frac{n}{2} \\hat{\\sigma}^2 - \\frac{n}{2} \\bar{x}^2 \\right) $$\n\nSubstituting this back into $\\Lambda$:\n\n$$ \\Lambda = (\\hat{\\sigma}^2)^{n/2} e^{n/2} \\exp\\left( -\\frac{n}{2} \\hat{\\sigma}^2 - \\frac{n}{2} \\bar{x}^2 \\right) $$\n$$ \\Lambda = (\\hat{\\sigma}^2)^{n/2} \\exp\\left( \\frac{n}{2} (1 - \\hat{\\sigma}^2 - \\bar{x}^2) \\right) $$\n\n### 6. The Log-Likelihood Ratio Statistic\n\nIt is standard to use $-2 \\ln \\Lambda$ as the test statistic, which asymptotically follows a Chi-squared distribution (with degrees of freedom equal to the difference in dimensionality of the parameter spaces, here $2 - 0 = 2$).\n\n$$ -2 \\ln \\Lambda = -2 \\left[ \\frac{n}{2} \\ln(\\hat{\\sigma}^2) + \\frac{n}{2}(1 - \\hat{\\sigma}^2 - \\bar{x}^2) \\right] $$\n$$ -2 \\ln \\Lambda = -n \\ln(\\hat{\\sigma}^2) - n(1 - \\hat{\\sigma}^2 - \\bar{x}^2) $$\n\nRearranging terms to group by parameter influence:\n$$ -2 \\ln \\Lambda = n \\left[ (\\hat{\\sigma}^2 - 1 - \\ln \\hat{\\sigma}^2) + \\bar{x}^2 \\right] $$\n\nWhere $\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^2$ and $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$.\n\n### Conclusion\n\nThe likelihood ratio test statistic is $\\Lambda$ (or equivalently $-2 \\ln \\Lambda$). Based on the derivation above, the statistic depends only on the sample from $X$.\n\nThe **Likelihood Ratio** is:\n$$ \\Lambda = \\left( \\hat{\\sigma}^2 \\right)^{n/2} \\exp\\left( \\frac{n}{2} (1 - \\hat{\\sigma}^2 - \\bar{x}^2) \\right) $$\n\nThe **Log-Likelihood Ratio Statistic** is:\n$$ -2 \\ln \\Lambda = n \\left( \\hat{\\sigma}^2 - \\ln(\\hat{\\sigma}^2) - 1 + \\bar{x}^2 \\right) $$",
        "raw_answer": "To derive the likelihood ratio test (LRT) statistic for the given hypotheses, we follow the standard procedure of constructing the likelihood function, maximizing it under the null and alternative hypotheses, and forming the ratio.\n\n### 1. The Likelihood Function\n\nLet the observed data be $\\mathbf{x} = (x_1, \\dots, x_n)$ and $\\mathbf{y} = (y_1, \\dots, y_m)$.\nThe random variables are defined as:\n*   $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ with unknown parameters $\\theta = (\\mu, \\sigma^2) \\in \\mathbb{R} \\times (0, \\infty)$.\n*   $Y \\sim \\mathcal{N}(0, 1)$ with no unknown parameters.\n*   $X$ and $Y$ are independent.\n\nThe joint likelihood function $L(\\mu, \\sigma^2; \\mathbf{x}, \\mathbf{y})$ is the product of the marginal densities due to independence:\n$$ L(\\mu, \\sigma^2) = \\left( \\prod_{i=1}^n f_X(x_i; \\mu, \\sigma^2) \\right) \\cdot \\left( \\prod_{j=1}^m f_Y(y_j) \\right) $$\n\nSubstituting the probability density functions for the normal distribution:\n$$ L(\\mu, \\sigma^2) = \\left( \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left\\{-\\frac{(x_i-\\mu)^2}{2\\sigma^2}\\right\\} \\right) \\cdot \\left( \\prod_{j=1}^m \\frac{1}{\\sqrt{2\\pi}} \\exp\\left\\{-\\frac{y_j^2}{2}\\right\\} \\right) $$\n\nLet $C(\\mathbf{y}) = \\prod_{j=1}^m \\frac{1}{\\sqrt{2\\pi}} \\exp\\left\\{-\\frac{y_j^2}{2}\\right\\}$. Since the distribution of $Y$ is completely specified (parameter-free), the term $C(\\mathbf{y})$ does not depend on $\\mu$ or $\\sigma^2$. It acts as a constant scaling factor in the likelihood with respect to the parameters.\n\nThe likelihood function simplifies to:\n$$ L(\\mu, \\sigma^2) = C(\\mathbf{y}) \\cdot (2\\pi\\sigma^2)^{-n/2} \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (x_i - \\mu)^2 \\right) $$\n\n### 2. Hypotheses and Parameter Spaces\n\nWe are testing:\n*   $H_0: \\mu = 0, \\sigma = 1$\n*   $H_1: \\mu \\neq 0 \\text{ or } \\sigma \\neq 1$\n\nThe likelihood ratio $\\Lambda$ is defined as:\n$$ \\Lambda = \\frac{\\sup_{(\\mu, \\sigma^2) \\in \\Theta_0} L(\\mu, \\sigma^2)}{\\sup_{(\\mu, \\sigma^2) \\in \\Theta} L(\\mu, \\sigma^2)} $$\nwhere $\\Theta_0 = \\{(0, 1)\\}$ and $\\Theta = \\mathbb{R} \\times (0, \\infty)$.\n\n### 3. Maximization under $H_0$ (Numerator)\n\nUnder $H_0$, the parameters are fixed at $\\mu = 0$ and $\\sigma^2 = 1$.\n$$ L_0 = L(0, 1) = C(\\mathbf{y}) \\cdot (2\\pi)^{-n/2} \\exp\\left( -\\frac{1}{2} \\sum_{i=1}^n x_i^2 \\right) $$\n\n### 4. Maximization under $H_1$ (Denominator)\n\nUnder the unrestricted parameter space, we must find the Maximum Likelihood Estimators (MLEs) for $\\mu$ and $\\sigma^2$ based on the $X$ sample. Standard calculus on the log-likelihood of a normal distribution yields:\n$$ \\hat{\\mu} = \\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i $$\n$$ \\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2 $$\n\nWe assume $n \\ge 2$ and not all $x_i$ are equal, ensuring $\\hat{\\sigma}^2 > 0$. Substituting these estimates back into the likelihood function:\n\nThe summation in the exponent becomes:\n$$ \\sum_{i=1}^n (x_i - \\hat{\\mu})^2 = n\\hat{\\sigma}^2 $$\n\nThus, the maximum likelihood value $L_1$ is:\n$$ L_1 = L(\\hat{\\mu}, \\hat{\\sigma}^2) = C(\\mathbf{y}) \\cdot (2\\pi \\hat{\\sigma}^2)^{-n/2} \\exp\\left( -\\frac{1}{2\\hat{\\sigma}^2} (n\\hat{\\sigma}^2) \\right) $$\n$$ L_1 = C(\\mathbf{y}) \\cdot (2\\pi)^{-n/2} (\\hat{\\sigma}^2)^{-n/2} e^{-n/2} $$\n\n### 5. Constructing the Likelihood Ratio Statistic\n\nNow we form the ratio $\\Lambda = \\frac{L_0}{L_1}$:\n\n$$ \\Lambda = \\frac{C(\\mathbf{y}) (2\\pi)^{-n/2} \\exp\\left( -\\frac{1}{2} \\sum x_i^2 \\right)}{C(\\mathbf{y}) (2\\pi)^{-n/2} (\\hat{\\sigma}^2)^{-n/2} e^{-n/2}} $$\n\nObserve that $C(\\mathbf{y})$ and $(2\\pi)^{-n/2}$ cancel out (confirming the $Y$ sample does not influence the test statistic structure):\n\n$$ \\Lambda = \\frac{\\exp\\left( -\\frac{1}{2} \\sum_{i=1}^n x_i^2 \\right)}{(\\hat{\\sigma}^2)^{-n/2} e^{-n/2}} $$\n$$ \\Lambda = (\\hat{\\sigma}^2)^{n/2} e^{n/2} \\exp\\left( -\\frac{1}{2} \\sum_{i=1}^n x_i^2 \\right) $$\n\nWe can simplify the exponent term using the identity $\\sum_{i=1}^n x_i^2 = \\sum_{i=1}^n (x_i - \\bar{x})^2 + n\\bar{x}^2 = n\\hat{\\sigma}^2 + n\\bar{x}^2$:\n\n$$ \\exp\\left( -\\frac{1}{2} \\sum_{i=1}^n x_i^2 \\right) = \\exp\\left( -\\frac{n}{2} \\hat{\\sigma}^2 - \\frac{n}{2} \\bar{x}^2 \\right) $$\n\nSubstituting this back into $\\Lambda$:\n\n$$ \\Lambda = (\\hat{\\sigma}^2)^{n/2} e^{n/2} \\exp\\left( -\\frac{n}{2} \\hat{\\sigma}^2 - \\frac{n}{2} \\bar{x}^2 \\right) $$\n$$ \\Lambda = (\\hat{\\sigma}^2)^{n/2} \\exp\\left( \\frac{n}{2} (1 - \\hat{\\sigma}^2 - \\bar{x}^2) \\right) $$\n\n### 6. The Log-Likelihood Ratio Statistic\n\nIt is standard to use $-2 \\ln \\Lambda$ as the test statistic, which asymptotically follows a Chi-squared distribution (with degrees of freedom equal to the difference in dimensionality of the parameter spaces, here $2 - 0 = 2$).\n\n$$ -2 \\ln \\Lambda = -2 \\left[ \\frac{n}{2} \\ln(\\hat{\\sigma}^2) + \\frac{n}{2}(1 - \\hat{\\sigma}^2 - \\bar{x}^2) \\right] $$\n$$ -2 \\ln \\Lambda = -n \\ln(\\hat{\\sigma}^2) - n(1 - \\hat{\\sigma}^2 - \\bar{x}^2) $$\n\nRearranging terms to group by parameter influence:\n$$ -2 \\ln \\Lambda = n \\left[ (\\hat{\\sigma}^2 - 1 - \\ln \\hat{\\sigma}^2) + \\bar{x}^2 \\right] $$\n\nWhere $\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^2$ and $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$.\n\n### Conclusion\n\nThe likelihood ratio test statistic is $\\Lambda$ (or equivalently $-2 \\ln \\Lambda$). Based on the derivation above, the statistic depends only on the sample from $X$.\n\nThe **Likelihood Ratio** is:\n$$ \\Lambda = \\left( \\hat{\\sigma}^2 \\right)^{n/2} \\exp\\left( \\frac{n}{2} (1 - \\hat{\\sigma}^2 - \\bar{x}^2) \\right) $$\n\nThe **Log-Likelihood Ratio Statistic** is:\n$$ -2 \\ln \\Lambda = n \\left( \\hat{\\sigma}^2 - \\ln(\\hat{\\sigma}^2) - 1 + \\bar{x}^2 \\right) $$",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ],
    "outer_attempt": 1
  },
  {
    "question_model": "meta-llama-llama-4-maverick",
    "answer_model": "google-gemini-3-pro-preview",
    "question": "Consider the function $f(x) = e^x - 2x - 1$. We want to approximate the root of $f(x)$ in the interval $[0,1]$ using the Newton-Raphson method. Starting with $x_0 = 1$, perform two iterations of the Newton-Raphson method to find $x_2$. Provide the exact value of $x_2$.",
    "run_id": "44",
    "topic_slug": "numerical_analysis",
    "status": "succeeded",
    "attempts": [
      {
        "round": 1,
        "answer": "To solve the problem, we will perform two iterations of the Newton-Raphson method for the function $f(x) = e^x - 2x - 1$ starting with $x_0 = 1$.\n\n### 1. The Newton-Raphson Method\n\nThe Newton-Raphson iteration formula is given by:\n$$x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}$$\n\nFirst, we determine the derivative of $f(x)$ with respect to $x$:\n$$f(x) = e^x - 2x - 1$$\n$$f'(x) = \\frac{d}{dx}(e^x - 2x - 1) = e^x - 2$$\n\n### 2. Iteration 1: Finding $x_1$\n\nWe are given the initial guess $x_0 = 1$. We calculate $f(x_0)$ and $f'(x_0)$:\n$$f(1) = e^1 - 2(1) - 1 = e - 3$$\n$$f'(1) = e^1 - 2 = e - 2$$\n\nNow, we compute $x_1$:\n$$x_1 = 1 - \\frac{e - 3}{e - 2}$$\n\nTo simplify this expression, we find a common denominator:\n$$x_1 = \\frac{e - 2}{e - 2} - \\frac{e - 3}{e - 2}$$\n$$x_1 = \\frac{(e - 2) - (e - 3)}{e - 2}$$\n$$x_1 = \\frac{e - 2 - e + 3}{e - 2}$$\n$$x_1 = \\frac{1}{e - 2}$$\n\n### 3. Iteration 2: Finding $x_2$\n\nNow we use $x_1 = \\frac{1}{e - 2}$ to find $x_2$. The formula is:\n$$x_2 = x_1 - \\frac{f(x_1)}{f'(x_1)}$$\n\nSubstituting the function and derivative expressions:\n$$x_2 = x_1 - \\frac{e^{x_1} - 2x_1 - 1}{e^{x_1} - 2}$$\n\nTo simplify, we combine the terms into a single fraction:\n$$x_2 = \\frac{x_1(e^{x_1} - 2) - (e^{x_1} - 2x_1 - 1)}{e^{x_1} - 2}$$\n$$x_2 = \\frac{x_1 e^{x_1} - 2x_1 - e^{x_1} + 2x_1 + 1}{e^{x_1} - 2}$$\n\nNotice that the $-2x_1$ and $+2x_1$ terms cancel out:\n$$x_2 = \\frac{x_1 e^{x_1} - e^{x_1} + 1}{e^{x_1} - 2}$$\n$$x_2 = \\frac{e^{x_1}(x_1 - 1) + 1}{e^{x_1} - 2}$$\n\nNow, we substitute the exact value of $x_1 = \\frac{1}{e - 2}$.\nFirst, let's simplify the term $(x_1 - 1)$:\n$$x_1 - 1 = \\frac{1}{e - 2} - 1 = \\frac{1 - (e - 2)}{e - 2} = \\frac{1 - e + 2}{e - 2} = \\frac{3 - e}{e - 2}$$\n\nNow substitute this back into the expression for $x_2$:\n$$x_2 = \\frac{e^{\\frac{1}{e - 2}} \\left( \\frac{3 - e}{e - 2} \\right) + 1}{e^{\\frac{1}{e - 2}} - 2}$$\n\nTo express this as a single fraction, we multiply the numerator and the denominator by $(e - 2)$:\n$$x_2 = \\frac{(3 - e)e^{\\frac{1}{e - 2}} + (e - 2)}{(e - 2)(e^{\\frac{1}{e - 2}} - 2)}$$\n\n### Final Answer\n\nThe exact value of $x_2$ is:\n$$x_2 = \\frac{(3 - e)e^{\\frac{1}{e - 2}} + e - 2}{(e - 2)(e^{\\frac{1}{e - 2}} - 2)}$$",
        "raw_answer": "To solve the problem, we will perform two iterations of the Newton-Raphson method for the function $f(x) = e^x - 2x - 1$ starting with $x_0 = 1$.\n\n### 1. The Newton-Raphson Method\n\nThe Newton-Raphson iteration formula is given by:\n$$x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}$$\n\nFirst, we determine the derivative of $f(x)$ with respect to $x$:\n$$f(x) = e^x - 2x - 1$$\n$$f'(x) = \\frac{d}{dx}(e^x - 2x - 1) = e^x - 2$$\n\n### 2. Iteration 1: Finding $x_1$\n\nWe are given the initial guess $x_0 = 1$. We calculate $f(x_0)$ and $f'(x_0)$:\n$$f(1) = e^1 - 2(1) - 1 = e - 3$$\n$$f'(1) = e^1 - 2 = e - 2$$\n\nNow, we compute $x_1$:\n$$x_1 = 1 - \\frac{e - 3}{e - 2}$$\n\nTo simplify this expression, we find a common denominator:\n$$x_1 = \\frac{e - 2}{e - 2} - \\frac{e - 3}{e - 2}$$\n$$x_1 = \\frac{(e - 2) - (e - 3)}{e - 2}$$\n$$x_1 = \\frac{e - 2 - e + 3}{e - 2}$$\n$$x_1 = \\frac{1}{e - 2}$$\n\n### 3. Iteration 2: Finding $x_2$\n\nNow we use $x_1 = \\frac{1}{e - 2}$ to find $x_2$. The formula is:\n$$x_2 = x_1 - \\frac{f(x_1)}{f'(x_1)}$$\n\nSubstituting the function and derivative expressions:\n$$x_2 = x_1 - \\frac{e^{x_1} - 2x_1 - 1}{e^{x_1} - 2}$$\n\nTo simplify, we combine the terms into a single fraction:\n$$x_2 = \\frac{x_1(e^{x_1} - 2) - (e^{x_1} - 2x_1 - 1)}{e^{x_1} - 2}$$\n$$x_2 = \\frac{x_1 e^{x_1} - 2x_1 - e^{x_1} + 2x_1 + 1}{e^{x_1} - 2}$$\n\nNotice that the $-2x_1$ and $+2x_1$ terms cancel out:\n$$x_2 = \\frac{x_1 e^{x_1} - e^{x_1} + 1}{e^{x_1} - 2}$$\n$$x_2 = \\frac{e^{x_1}(x_1 - 1) + 1}{e^{x_1} - 2}$$\n\nNow, we substitute the exact value of $x_1 = \\frac{1}{e - 2}$.\nFirst, let's simplify the term $(x_1 - 1)$:\n$$x_1 - 1 = \\frac{1}{e - 2} - 1 = \\frac{1 - (e - 2)}{e - 2} = \\frac{1 - e + 2}{e - 2} = \\frac{3 - e}{e - 2}$$\n\nNow substitute this back into the expression for $x_2$:\n$$x_2 = \\frac{e^{\\frac{1}{e - 2}} \\left( \\frac{3 - e}{e - 2} \\right) + 1}{e^{\\frac{1}{e - 2}} - 2}$$\n\nTo express this as a single fraction, we multiply the numerator and the denominator by $(e - 2)$:\n$$x_2 = \\frac{(3 - e)e^{\\frac{1}{e - 2}} + (e - 2)}{(e - 2)(e^{\\frac{1}{e - 2}} - 2)}$$\n\n### Final Answer\n\nThe exact value of $x_2$ is:\n$$x_2 = \\frac{(3 - e)e^{\\frac{1}{e - 2}} + e - 2}{(e - 2)(e^{\\frac{1}{e - 2}} - 2)}$$",
        "evaluation": {
          "verdict": "pass",
          "ill_posed": false,
          "issues": [],
          "improvements": ""
        }
      }
    ],
    "outer_attempt": 1
  }
]
