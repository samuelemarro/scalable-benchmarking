# Data Schema Documentation

This document describes the JSON data structures used throughout the Turing LLM evaluation pipeline.

## Table of Contents
1. [Benchmark Schema](#benchmark-schema)
2. [Answer Schema](#answer-schema)
3. [Critique Schema](#critique-schema)
4. [Debate Schema](#debate-schema)
5. [Automated Evaluation Schema](#automated-evaluation-schema)
6. [Human Evaluation Schema](#human-evaluation-schema)
7. [Judging Task Schema](#judging-task-schema)
8. [Metadata Schema](#metadata-schema)

Notes:
- List files may include `{}` placeholders for missing entries. These are stored as `None` in memory.
- Benchmark entries can be "pending" in memory but `pending` is not serialized.

---

## Benchmark Schema

**File Location:** `benchmarks/{model-slug}.json`

**Description:** Questions generated by a specific model, along with self-improvement history.

### Structure

```json
[
  {
    "run_id": "string",
    "topic_slug": "string",
    "topic_name": "string",
    "status": "succeeded" | "failed" | "ill-posed",
    "generation_rounds": [
      {
        "status": "succeeded" | "failed" | "ill-posed",
        "refinement_rounds": [
          {
            "round": 1,
            "question": "string",
            "answer": "string",
            "raw_answer": "string",
            "evaluation": {
              "verdict": "pass" | "fail",
              "ill_posed": true | false,
              "issues": ["string"],
              "improvements": "string"
            }
          }
        ]
      }
    ]
  },
  {}
]
```

### Field Descriptions

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `run_id` | string | Yes | Unique identifier for the generation run |
| `topic_slug` | string | Yes | Topic slug from topic configuration |
| `topic_name` | string | No | Human-readable topic name |
| `status` | string | Yes | Generation status |
| `generation_rounds` | array | No | Self-improvement iteration history |
| `generation_rounds[].status` | string | Yes | Round status |
| `generation_rounds[].refinement_rounds` | array | Yes | Refinement attempts |
| `refinement_rounds[].round` | int | Yes | Round number (1-indexed) |
| `refinement_rounds[].question` | string | Yes | Question text |
| `refinement_rounds[].answer` | string | Yes | Answer text |
| `refinement_rounds[].raw_answer` | string | No | Raw answer before cleaning |
| `refinement_rounds[].evaluation` | object | Yes | Self-check evaluation JSON |

---

## Answer Schema

**File Location:** `answers/{question-model-slug}/{answer-model-slug}.json`

**Description:** Answers generated by a specific model for questions created by another model.

### Structure

```json
[
  {
    "question_model": "string",
    "answer_model": "string",
    "question": "string",
    "run_id": "string",
    "topic_slug": "string",
    "ill_posed_claim": {
      "verdict": "pass" | "fail",
      "ill_posed": true | false,
      "issues": ["string"],
      "improvements": "string"
    },
    "status": "succeeded" | "failed" | "ill-posed",
    "attempts": [
      {
        "round": 1,
        "answer": "string",
        "raw_answer": "string",
        "evaluation": {
          "verdict": "pass" | "fail",
          "ill_posed": true | false,
          "issues": ["string"],
          "improvements": "string"
        }
      }
    ]
  },
  {}
]
```

### Field Descriptions

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `question_model` | string | Yes | Model that generated the question |
| `answer_model` | string | Yes | Model generating this answer |
| `question` | string | Yes | The question being answered |
| `run_id` | string | No | Run identifier |
| `topic_slug` | string | No | Topic slug |
| `ill_posed_claim` | object | No | Self-check evaluation if answer marked ill-posed |
| `status` | string | Yes | Answer generation status |
| `attempts` | array | No | Self-improvement attempts |
| `attempts[].round` | int | Yes | Round number |
| `attempts[].answer` | string | Yes | Answer text |
| `attempts[].raw_answer` | string | No | Raw answer before cleaning |
| `attempts[].evaluation` | object | No | Self-check evaluation JSON |

---

## Critique Schema

**File Location:** `critiques/{mode}/{question-model-slug}/{critic-slug}__{answer-model-slug}.json`

**Description:** Critiques of answers, either as contradictions or evaluations.

### Structure

```json
[
  {
    "question": "string",
    "run_id": "string",
    "topic_slug": "string",
    "question_author": "string",
    "critic": "string",
    "answer_author": "string",
    "status": "succeeded" | "failed",
    "attempts": [
      {
        "round": 1,
        "cleaned_critique": "string",
        "raw_critique": "string",
        "verdict": "correct" | "incorrect" | "insufficient" | "obscure" | "unknown",
        "notes": "string",
        "suggestions": "string",
        "evaluation": {
          "verdict": "pass" | "fail",
          "ill_posed": true | false,
          "issues": ["string"],
          "improvements": "string"
        }
      }
    ]
  },
  {}
]
```

### Field Descriptions

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `question` | string | Yes | The original question |
| `run_id` | string | No | Run identifier |
| `topic_slug` | string | No | Topic slug |
| `question_author` | string | Yes | Model that created the question |
| `critic` | string | Yes | Model providing critique |
| `answer_author` | string | Yes | Model that answered |
| `status` | string | Yes | Critique generation status |
| `attempts` | array | No | Self-improvement attempts |
| `attempts[].round` | int | Yes | Round number |
| `attempts[].cleaned_critique` | string | No | Cleaned critique text |
| `attempts[].raw_critique` | string | Yes | Raw critique text |
| `attempts[].verdict` | string | Yes | Parsed verdict |
| `attempts[].notes` | string | Yes | Parsed critique notes |
| `attempts[].suggestions` | string | No | Suggested fixes |
| `attempts[].evaluation` | object | No | Self-check evaluation JSON |

---

## Debate Schema

**File Location:** `debates/{mode}/{question-model-slug}/{answer-model-slug}.json`

**Description:** Multi-round debates between models about answer quality or ill-posedness.

### Structure

```json
[
  {
    "question": "string",
    "run_id": "string",
    "topic_slug": "string",
    "alice_model": "string",
    "bob_model": "string",
    "claimant": "string",
    "answer_author": "string",
    "critic": "string",
    "history": [
      {
        "round": 1,
        "speaker": "defender" | "claimant" | "Alice" | "Bob",
        "message": "string",
        "concede": true | false
      }
    ]
  },
  {}
]
```

### Field Descriptions

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `question` | string | Yes | The question text |
| `run_id` | string | No | Run identifier |
| `topic_slug` | string | No | Topic slug |
| `alice_model` | string | Yes | Model playing Alice role |
| `bob_model` | string | Yes | Model playing Bob role |
| `claimant` | string | No | Model making the claim |
| `answer_author` | string | No | Answer author (critique debates) |
| `critic` | string | No | Critic model (critique debates) |
| `history` | array | Yes | Debate transcript |
| `history[].round` | int | Yes | Round number (0 or higher) |
| `history[].speaker` | string | Yes | Speaker role or alias |
| `history[].message` | string | Yes | Argument text |
| `history[].concede` | bool | No | Whether speaker conceded |

---

## Automated Evaluation Schema

**File Location:** `automated_evaluations/{judge-slug}.json`

**Description:** Automated judgments of debates.

### Structure

```json
{
  "decisions": [
    {
      "id": "string",
      "type": "illposed" | "critique" | "critique_debate",
      "mode": "string",
      "question_model": "string",
      "question": "string",
      "answer_model": "string",
      "answer": "string",
      "critic_model": "string",
      "claim": "string",
      "claimant": "string",
      "defender": "string",
      "debate_history": [],
      "verdict": "string",
      "confidence": 1,
      "reasoning": "string",
      "judge_model": "string",
      "status": "succeeded" | "failed",
      "raw_response": "string",
      "run_id": "string",
      "topic_slug": "string"
    }
  ]
}
```

### Field Descriptions

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `id` | string | Yes | Unique task identifier |
| `type` | string | Yes | Evaluation type |
| `mode` | string | No | Critique mode |
| `question_model` | string | No | Question author |
| `question` | string | No | Question text |
| `answer_model` | string | No | Answer author |
| `answer` | string | No | Answer text |
| `critic_model` | string | No | Critic model |
| `claim` | string | No | Claim being judged |
| `claimant` | string | No | Model making claim |
| `defender` | string | No | Model defending answer |
| `debate_history` | array | No | Debate transcript |
| `verdict` | string | Yes | Judge's decision |
| `confidence` | int | No | Confidence level (1-5), may be missing if parsing fails |
| `reasoning` | string | No | Explanation of verdict, may be missing if parsing fails |
| `judge_model` | string | No | Judge model |
| `status` | string | No | Judgment status |
| `raw_response` | string | No | Raw judge response |
| `run_id` | string | No | Run identifier |
| `topic_slug` | string | No | Topic slug |

---

## Human Evaluation Schema

**File Location:** `evaluations/{username}.json`

**Description:** Human labeling decisions for ill-posed claims and critiques.

### Structure

```json
{
  "decisions": [
    {
      "id": "string",
      "type": "illposed" | "critique",
      "mode": "string",
      "question_model": "string",
      "answer_model": "string",
      "critic_model": "string",
      "verdict": "string",
      "confidence": 1,
      "comment": "string"
    }
  ]
}
```

---

## Judging Task Schema

**Description:** Internal task payloads for automated or human judging.

### Structure

```json
{
  "id": "string",
  "type": "illposed" | "critique",
  "mode": "string",
  "question_model": "string",
  "answer_model": "string",
  "critic_model": "string",
  "question": "string",
  "answer": "string",
  "critique": "string",
  "debate_history": [],
  "alice_model": "string",
  "bob_model": "string",
  "run_id": "string",
  "topic_slug": "string"
}
```

---

## Metadata Schema

**Description:** Optional metadata for versioning and tracking.

### Structure

```json
{
  "generated_at": "YYYYMMDD_HHMMSS",
  "model_name": "string",
  "model_version": "string",
  "response_id": "string",
  "created_timestamp": 0
}
```
