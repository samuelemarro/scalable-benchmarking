
\documentclass{article} % For LaTeX2e
\usepackage{iclr2026_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

\usepackage{epigraph}


% NOTE: WE USE AMERICAN ENGLISH FOR THIS ONE

\title{The Question is the Answer: Weak-to-Strong Benchmarking}

% Alternative title: Don't Criticize What You Can't Understand

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213, USA \\
\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
%\setlength{\epigraphwidth}{0.3\textwidth}
\begin{document}

\maketitle

\epigraph{But they are useless. They can only give you answers.}{\textit{Pablo Picasso on computers}}

\section{Introduction}

Traditional benchmarks designed to measure LLM performance against human baselines are rapidly saturating as models achieve and surpass human-level performance across diverse domains. For example, [...]

This saturation creates a series of evaluation challenges. As models exceed human performance on established metrics, these benchmarks lose their discriminative power, making it difficult to meaningfully compare state-of-the-art systems. The traditional approach of having human experts design increasingly difficult evaluation tasks becomes prohibitively expensive and often impossible.

The reliance on AI systems to judge other AI systems introduces evaluation circularity. Using an LLM to evaluate another LLM's outputs, or fine-tuning judge models on data generated by the systems they evaluate, risks creating closed loops that miss fundamental limitations or biases.
This circularity becomes particularly problematic when evaluating capabilities that approach or exceed the judging system's own abilities.

As large language models continue their rapid capability growth, we need evaluation methods that can scale effectively, ideally before these models completely overtake human performance in key domains. The current trajectory suggests that without new evaluation paradigms, we may soon find ourselves with powerful AI systems that we cannot adequately assess or compare.

%As large language models continue rapid capability growth, we need evaluation methods that scale effectively—before these models completely overtake human performance in key domains.

\paragraph{A Fundamental Gap} In this paper, we tackle the challenge of scalable benchmarking by starting with a key observation. While designing challenging questions and producing correct answers becomes increasingly difficult as AI capabilities advance, two critical human abilities remain accessible: \textbf{verifying correctness} of proposed solutions when appropriately framed, and \textbf{determining whether questions are meaningful} and relevant to human needs.

This asymmetry reflects deep patterns across domains. In mathematics, checking a proof is generally easier than discovering it. In software engineering, designing unit tests is often more tractable than writing correct code. These patterns suggest that verification capabilities can extend meaningfully beyond generation capabilities.

%The key insight is that verification, when properly structured, remains within human cognitive reach even as AI-generated content complexity grows. Humans retain the ability to assess whether AI-generated questions are meaningful and whether proposed solutions are correct when provided with appropriate context—judgments that remain fundamentally important regardless of AI capability levels.

This verification-generation gap represents an underutilized resource. Current benchmarking approaches largely ignore this asymmetry, requiring human evaluators to match AI capabilities in generation tasks. By reorienting evaluation around verification rather than generation, we can potentially maintain meaningful human oversight even as AI systems surpass human performance.

\paragraph{Human-as-a-Judge}
We thus propose a novel benchmarking framework that leverages the verification-generation asymmetry to create scalable evaluation protocols for strong AI systems. Our approach centers on a symmetric, agent-to-agent interaction protocol where AI systems challenge each other while humans serve as judges.

In each evaluation round, one AI system (the ``Tester") generates a question with its proposed answer, while another system (the ``Testee") attempts to answer independently. Human judges evaluate whether the question is meaningful and whether the Testee's answer is correct. The roles are then reversed, creating symmetric evaluation. If an AI Alice can consistently answer Bob's questions, but Bob cannot, we say that Alice is stronger than Bob.

This design addresses key limitations of existing approaches. %AI systems generate evaluation content, eliminating human question design bottlenecks while maintaining human oversight through verification. The symmetric structure ensures comparable evaluation conditions, and the agent-to-agent format naturally generates diverse, challenging problems.
Humans are not asked to generate problems or solve complex tasks: they need only verify correctness and assess question meaningfulness. This constraint ensures scalability: as AI capabilities advance, the verification burden grows much more slowly. The framework enables meaningful model-versus-model comparison without requiring human superiority in the underlying tasks.
This capability-agnostic comparison mechanism is largely setting-agnostic, scalable, and requires fewer resources even when the AI systems have comparable or superior performance to humans.

\paragraph{Contributions}
This work makes several key contributions to AI evaluation and benchmarking. First, we present a general benchmarking framework that scales gracefully as AI capabilities grow.
Second, we provide experimental validation across three critical research questions: validating that verification is measurably simpler than generation or answering (RQ1); demonstrating that our protocol yields meaningful partial orderings over model capabilities (RQ2); and showing that weak models can effectively benchmark strong models (RQ3), providing evidence for future human evaluation of superhuman systems.
Third, we analyze our framework's limitations, failure modes, and potential extensions, including scenarios where the protocol may break down and methods for handling intransitive model comparisons.

Our work addresses a critical gap in AI evaluation methodology. By showing that human-centered verification can remain viable for evaluating superhuman AI systems, we provide a path forward for maintaining meaningful AI evaluation practices. The framework suggests a sustainable role for human expertise in AI development that positions humans as judges and verifiers rather than competitors, potentially essential for maintaining human agency in AI development.

\section{Background and Related Work}

In this section, we examine evaluation methods for frontier and superhuman AI systems, with particular focus on NLP and mathematical reasoning, and explore connections to automated judging and scalable oversight frameworks.

\paragraph{The Limits of Human-Based Benchmarks.}
The evolution of AI benchmarking shows a clear pattern where, as soon as a benchmark reaches saturation, it is followed by attempts to maintain discriminative power. General-purpose NLP evaluation is an example of this cycle: GLUE (Wang et al., 2018) quickly hit ceiling effects where small improvements became difficult to interpret meaningfully. SuperGLUE (Wang et al., 2019) raised the bar temporarily, but frontier models again approached saturation. BIG-bench (Srivastava et al., 2022) broadened the scope of evaluation and increased its complexity, but remained fundamentally static, making it vulnerable to contamination and prompt engineering.
On the other hand, dynamic benchmarking approaches like Dynabench (Kiela et al., 2021) attempt to address saturation by having humans generate targeted hard questions that are likely to be answered incorrectly. This approach uncovers weaknesses that traditional leaderboards might miss, but it struggles with scalability as models approach human-level performance. Similar trends can be seen in other domains: in mathematics, benchmarks like GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) set reasoning standards that contemporary models can largely reach, while code evaluation has evolved from simple tasks to more complex programming challenges, as seen in HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021).
Notably, in areas where AI has already achieved superhuman performance, human baselines have been abandoned entirely. For instance, chess and Go AI evaluation shifted to self-play and rating systems (like Elo), showing that once systems exceed human capabilities, comparative and automated evaluations become essential.

\paragraph{Verification-Centric Evaluation Paradigms}
One important takeaway across various fields is that while generation can be quite complex, verification remains tractable. In mathematical reasoning, for instance, verification-based methods train additional models to check the reasoning steps or final answers (Cobbe et al., 2021). This is often combined with sampling and aggregation strategies, as illustrated in Minerva (Lewkowycz et al., 2022). In the world of code evaluation, the standard has shifted towards execution-based verification: metrics like pass@k under hidden unit tests offer a way to assess correctness mechanically. Meanwhile, systems like AlphaCode (Li et al., 2022) take this a step further in competitive programming by evaluating performance against past contest problems.

These verification-focused methods scale by minimizing the human involvement to just curating specifications while ensuring correctness through mechanical means.  However, there are well-known limitations: incomplete test suites can be manipulated, formal proof checkers can limit the expressiveness of solutions, and the quality of specifications often becomes a major bottleneck. Even with these challenges, these approaches effectively show that we can evaluate complex generation tasks through manageable verification processes, which is exactly the imbalance that our framework exploits.

%These verification-centric methods achieve scalability by reducing the human role to specification curation while establishing correctness mechanically. The limitations are well-understood: incomplete test suites can be gamed, formal proof checkers constrain solution expressivity, and specification quality becomes the primary bottleneck. Despite these trade-offs, these paradigms successfully demonstrate that complex generation tasks can be evaluated through tractable verification pipelines—precisely the asymmetry our framework exploits.

\paragraph{LLM-as-a-Judge.}
As advanced models surpass our ability to annotate data, automated judging has emerged as a promising yet tricky solution. Research from MT-Bench and Chatbot Arena (Zheng et al., 2023) shows that state-of-the-art LLMs can generate scalable pairwise judgments that align with expert preferences, allowing for quick model comparisons without the need for extensive human evaluation. Approaches like G-Eval (Liu et al., 2023) aim to bring some structure to this method through organized rubrics and critic models. LLM judges often show systematic biases, such as self-recognition and self-preference (Panickssery et al., 2024), position effects, and susceptibility to style-based manipulation. In technical fields like coding and mathematics, relying solely on text evaluations can overlook subtle semantic errors that execution or proof checking would catch. The growing consensus is that while LLM judges are useful for scaling and initial assessments, high-stakes evaluations still need external verification and human oversight to ensure meaningfulness and the validity of problems.

\paragraph{Scalable Oversight and Weak-to-Strong Safety.}
The AI alignment community has developed frameworks that enable weaker systems to evaluate stronger ones, which is directly relevant to our goal of human evaluation of strong AI. AI Safety via Debate (Irving et al., 2018) reframes evaluation as an adversarial dialogue where humans choose the best arguments rather than designing solutions. Another method, known as iterated amplification (Christiano et al., 2018), breaks down complex judgments into smaller, manageable questions that weaker agents can answer, and then combines those answers into a comprehensive evaluation.
Constitutional AI (Bai et al., 2022) and similar AI-feedback techniques use models to create critiques based on principles set by humans, which ensures transparency while reducing the need for constant supervision. Techniques like self-critique and self-refinement have proven effective in boosting factual accuracy and spotting errors in complex outputs.  All these strategies share a key design principle: they limit human tasks to decisions that can be audited, like selection, verification, or rubric-based assessments, while leaving the generation and adversarial probing to the models.


\paragraph{Positioning.}
Our approach combines insights from various evaluation traditions while tackling their main shortcomings. Instead of relying on static benchmarks, we avoid saturation by allowing models to create their own evaluation content, steering clear of fixed item banks. We also reduce circular reasoning by grounding correctness in external verification and limiting the scope of the judges, rather than using LLMs as judges. Finally, unlike scalable oversight methods that are typically designed for training, we adapt verification asymmetries specifically for benchmarking. This results in pairwise comparisons that naturally create partial orders while still preserving information about specialization and incomparability. In areas like mathematics and programming, where meaningfulness is fundamentally human and correctness can often be mechanically verified, this division of labor keeps evaluations interpretable and scalable, even as AIs are benchmarked on challenges that would be difficult for humans to solve or create.

\section{Framework}

In this section, we introduce human-as-a-judge, and outline [...]. Throughout, we define two predicates: meaningfulness, a human-defined gate on questions, and correctness, a domain-specific acceptance test on answers.

\subsection{Protocol}

Let $Q$ be a space of questions and $A$ a space of answers. A tester $T$ proposes $(q, a^\star)$ with $q \in Q$ and a reference certificate or answer $a^\star \in A$. A testee $S$ produces $\hat a \in A$. A judge $J$ performs two evaluations:
\begin{itemize}
    \item \textbf{Meaningfulness} $M(q) \in \{0, 1\}$: is the question well-posed, non-degenerate, and aligned with the domain specification?
    \item \textbf{Correctness} $V(q, a) \in \{0, 1\}$: do the answers $a^\star$ and $\hat a$ provided by the tester and testee satisfy the acceptance criteria?
\end{itemize}
Only trials with $M(q) = 1$ and $V(a^\star) = 1$ are considered valid. Valid trials with $V(\hat a) = 1$ are ruled in favor for $S$. Symmetry is then enforced by swapping $S$ and $T$.

\paragraph{Meaningfulness} Meaningfulness is tautologically human-defined. This is intended: it encodes task semantics and social desiderata that external oracles do not capture. Use a short rubric (clarity/unambiguity; non-triviality; policy compliance; no leakage), record a one-sentence rationale and confidence, and require inter-rater spot checks to monitor reliability.

\paragraph{Correctness} Correctness should be anchored wherever possible to external checkers (executors, solvers, proof checkers, engines). When automation is insufficient, the judge applies a rubric and reports confidence.

\paragraph{Invalid Trials} Items for which $M(q) = 0$ or $V(a^\star) = 0$ are not used to test $S$. %To keep incentives aligned, an invalid trial is automatically ruled in favor of the testee.
Testers with an invalid rate above a threshold $\kappa = 0.2$ are audited.

\paragraph{1-vs-N Testing} Questions are opponent-agnostic: this allows the tester to submit a reusable set $\{(q_i, a^\star_i)\}$ of question-answer pairs which are judged once. These sets can then be used to benchmark multiple models without rerunning the tester.

\subsection{Applicability}

The framework targets verification-dominant settings, i.e. domains where correctness can be checked more reliably and cheaply than generating or solving new items. We require the following operational properties:
\begin{itemize}
    \item There exists a deterministic or high-confidence procedure $V$ (e.g., execution on hidden tests; proof/type checking; engine evaluation; constraint/solver satisfaction; or even human review) that is substantially cheaper than constructing $a^\star$ from scratch-
    \item The domain provides an explicit acceptance contract (e.g., input–output mapping, theorem statement + proof obligations, game objective) such that $V$ reflects the construct of interest, not superficial proxies.
    \item Small errors induce detectable violations under $V$ (e.g., a failing test, an invalid inference, an illegal position), reducing burden on human raters.
    \item  Humans can reliably decide $M(q)$ with a brief rubric (clarity, non-ambiguity, non-triviality, rule compliance). This gate filters adversarial or meaningless items.
\end{itemize}

Typical fits include formal or semi-formal math, code synthesis with tests/specs, constrained planning/puzzles with solvers, knowledge-based Q\&A, and domains with simulators. Purely aesthetic or open-ended creative tasks fall outside scope unless they can be reduced to verifiable subgoals.

%\subsection{Ordering}

%Let $p_{A\to B}$ be the directional success rate of $B$ against $A$'s questions (i.e., the percentage of trials that are either invalid or answered correctly by $B$). We define strict dominance as follows:
%
%\paragraph{Strict Dominance.} $A \succ B$ if $p_{A \to B} < \tau$ and $p_{B \to A} > 1 - \tau$, with small $\tau$ statistical support (i.e. the confidence interval excludes $\tau$). This defines a partial order with intended incomparabilities.

%We then relax 

%To convert trials into ratings, we use a bipartite Bradley-Terry model with Maximum Likelihood Estimate. Bradley-Terry models are a generalization of Elo ratings that assume that performance is static; in their MLE variant, they are order-independent (i.e., 3 victories and 3 defeats against an opponent are equivalent to 3 defeats and 3 victories). Comparison-based rating platforms for LLMs, such as LMSys LMArena, use an MLE Bradley-Terry model. We extend this model to its bipartite form: we assume that there are two skills (generating questions with an answer, answering an unseen question), and compute Elo scores independently. Under this framework, a model might be a strong tester but a weak testee and vice versa. This allows us to take into account gaps between question generation and answering in a principled fashion.

%Formally, ...

%This approach decouples tester skill from testee skill...
%This allows us to take into account a victory for a testee against a strong or weak tester.

%While this approach yields two Elo scores (one for tester and one for testee), the testee Elo is used as the overall Elo for the model. This reflects the fact that a model's capability to answer questions is more relevant for [practical use] than that to ask questions.

\subsection{Converting into Ratings}
\label{sec:rating}

%While the protocol can be used as-is, it is ideal to have a mapping mechanism to convert success rates into Elo-like ratings. However, it is possible that

\paragraph{Overview.}
The symmetric protocol in \S\ref{sec:protocol} can, in principle, be used as--is by reporting empirical win rates; however, Elo--style ratings are more familiar and provide a concise, comparable summary of performance. Three complications preclude a naive Elo: (i) an LLM’s \emph{questioning} and \emph{answering} abilities need not coincide, (ii) the distribution of question difficulty is heavily dependent on the sampling strategy, and (iii) we have no reliable, a priori labels of per-question difficulty. While humans could rate difficulty, that approach does not scale once model capability surpasses human expertise. We therefore introduce a principled, order--invariant, batch estimator that accounts for distinct roles and unknown item difficulty and remains stable under extreme outcomes, while mapping to an Elo scale for interpretability.


%We now provide a principled approach to estimate Elo-like scores for each answerer, while taking into account asymmetries between asking questions and providing answers.

\paragraph{Design goals and model choice.}
We require a rating method that (i) is order-invariant (estimates do not depend on the sequence of trials), (ii) assumes static skill over the evaluation window (as in LMSYS Chatbot Arena's Elo benchmarking), (iii) distinguishes answering skill from questioning difficulty, (iv) remains numerically well-behaved under extreme win/loss patterns, and (v) can absorb unknown heterogeneity in item difficulty. A \emph{bipartite} Bradley--Terry (BT) model satisfies (i)--(iii); we estimate it by \emph{maximum likelihood} (MLE). To meet (iv) in rare separation cases we use a bias-reduced refit (Firth) as a safety valve; to meet (v) we extend BT with item-level random effects and estimate the variance component by (restricted) MLE. This yields a single, batch estimator that is comparable in spirit to Elo (used by LMSYS) but is fit in one shot and thus independent of presentation order.

\paragraph{Setup.}
Let $i\in\mathcal A$ index answerers, $j\in\mathcal Q$ index questioners, and $q\in\mathcal I$ index items. Each item $q$ is authored by $j=\mathrm{auth}(q)$. For a scored trial, the binary outcome $Y_{iq}\in\{0,1\}$ indicates whether answerer $i$ produced a correct solution to item $q$ after the meaningfulness and self-solve gates. The success probability follows a logistic Bradley--Terry form
\begin{equation}
\Pr(Y_{iq}=1 \mid \theta^A,\beta) \;=\; \sigma\!\big(\eta_{iq}\big), 
\qquad 
\eta_{iq} \;=\; \theta^A_i \;-\; \beta_q,
\qquad
\sigma(x)=\frac{1}{1+e^{-x}} ,
\end{equation}
where $\theta^A_i$ is the (log-odds) answering ability of model $i$, and $\beta_q$ is the difficulty of item $q$.

\paragraph{Item-aware extension (random effects).}
Questioners may contribute multiple, heterogeneous questions; since estimating the a priori relative difficulty of each question might be infeasible, we model the difficulty of each question as a parameter. We therefore write
\begin{equation}
\beta_q \;=\; \psi_{\mathrm{auth}(q)} \;+\; u_q,
\qquad
u_q \sim \mathcal N(0,\sigma_\beta^2),
\end{equation}
where $\psi_j$ is the author-level difficulty and $u_q$ is the item-level deviation. This induces \emph{partial pooling}: items with few observations shrink toward their author's mean difficulty; richly observed items are estimated more freely. The variance component $\sigma_\beta^2$ is estimated from the data.

\paragraph{Likelihoods and estimation.}
Conditional on $\{u_q\}$ the BT log-likelihood is
\begin{equation}
\ell_{\text{cond}}(\theta^A,\psi,u) \;=\; \sum_{(i,q)\in \mathcal D} \Big[ Y_{iq}\,(\theta^A_i-\psi_{\mathrm{auth}(q)}-u_q) \;-\; \log\!\big(1+e^{\theta^A_i-\psi_{\mathrm{auth}(q)}-u_q}\big) \Big],
\end{equation}
with data index set $\mathcal D=\{(i,q):\text{trial observed}\}$. The marginal (random-effects) likelihood integrates out $u$:
\begin{equation}
L(\theta^A,\psi,\sigma_\beta^2)
\;=\;
\int 
\exp\!\big\{\ell_{\text{cond}}(\theta^A,\psi,u)\big\}
\;\prod_{q}\phi(u_q;0,\sigma_\beta^2)\; \mathrm du,
\end{equation}
where $\phi(\cdot;0,\sigma_\beta^2)$ is the $\mathcal N(0,\sigma_\beta^2)$ density. We maximize the (restricted) marginal log-likelihood 
$\ell(\theta^A,\psi,\sigma_\beta^2)=\log L(\theta^A,\psi,\sigma_\beta^2)$ 
with respect to $(\theta^A,\psi,\sigma_\beta^2)$ under a location constraint (e.g., $\sum_{j}\psi_j=0$) for identifiability. %In practice we use Laplace or adaptive Gauss--Hermite quadrature for the integral over $u$, Newton/IRLS for fixed effects, and profile or REML updates for $\sigma_\beta^2$.
Standard errors and Wald confidence intervals follow from the observed information at the MLE.

\paragraph{Separation handling (safety refit).}
In datasets with unbeaten answerers or unsolved questioners, fixed-effect BT MLEs can diverge. Although the random-effects layer usually regularizes such patterns at the item level, we additionally detect (quasi) separation and, if present, refit the fixed-effect part with Firth's bias-reduced logistic likelihood (Jeffreys adjustment), yielding finite estimates and calibrated intervals. This refit is only a fallback; our primary estimator is MLE of the marginal random-effects BT described above.

\paragraph{Location and mapping to Elo.}
Only differences in $(\theta^A,\psi)$ are identified. After fitting, we center ($\sum_j\psi_j=0$) and report log-odds estimates. For interpretability we also present Elo-mapped scores on the base-10, 400-scale commonly used in practice:
\begin{equation}
R^{A}_i \;=\; R_0 \;+\; k\big(\theta^A_i - \mu\big),
\qquad
R^{Q}_j \;=\; R_0 \;+\; k\big(\psi_j - \mu\big),
\qquad
k \;=\; \frac{400}{\ln 10}\approx 173.72,
\end{equation}
with $R_0$ a chosen baseline (e.g., $1000$) and $\mu$ the centering constant used above. This linear map preserves pairwise differences and places both roles on a familiar scale; it also makes our batch MLE directly comparable to Elo-style online aggregates as used in LMSYS Arena. The answerer Elo represents the major reported result for this benchmark, although questioner Elo and per-item deviations can yield additional insights.

\subsection{Scalability}
\label{sec:scalability}

Our evaluation protocol scales beyond human problem–solving ability by concentrating scarce human effort on two auditable tasks, (i) a semantic gate on question \emph{meaningfulness} and (ii) verification triage, while delegating generation and solving to the systems under test and anchoring correctness to external oracles. This allocation keeps evaluation feasible even as model capability grows.

\paragraph{Amortized human effort via opponent-agnostic panels.}
Let each questioner $j$ contribute a panel $\mathcal I_j$ of items that pass a one-time meaningfulness screen $M(q)=1$ and a self-solve gate. The human time to screen $|\mathcal I|=\sum_j |\mathcal I_j|$ items is paid \emph{once}, then amortized across arbitrarily many answerers. For a new answerer $i$, the marginal human effort is limited to adjudicating the subset of items for which automated acceptance $V$ is inconclusive. Writing $t_M$ for minutes per meaningfulness check, $t_H$ for minutes per human verification, and $\rho\in[0,1]$ for the fraction of answers that require human judgment (the rest are decided by $V$), the expected human time is
\begin{equation}
\mathbb E[T_{\text{human}}] \;\approx\; |\mathcal I|\, t_M \;+\; \sum_{i\in\mathcal A} \rho_i\,|\mathcal I|\, t_H,
\end{equation}
which is linear in the number of answerers with a small coefficient $\rho_i$ when $V$ has high coverage (e.g., tests, solvers, proof checkers). Crucially, neither humans nor weaker models are asked to \emph{author} or \emph{solve} frontier items.


%\paragraph{Verification-centric robustness as capabilities grow.}
%As systems improve, difficulty can increase on the \emph{generation} side (harder items, more sophisticated solutions) without increasing the human burden proportionally. The correctness check $V$ scales mechanically (execution on hidden tests, solver/engine calls, proof/type checking). Humans adjudicate only semantic validity ($M$) and edge cases where $V$ is uncertain. The self-solve gate guarantees that items are tractable for at least one solver, preventing degenerate ``unsolved'' challenges that would otherwise tax judges.

%\paragraph{Statistical efficiency with sequential allocation.}
%Pairwise ordering does not require exhaustive evaluation. Let $\Delta$ denote a gap in log-odds ability between two answerers on a fixed panel. Standard sequential testing bounds imply $O(\Delta^{-2}\log(1/\delta))$ informative trials per pair to certify an ordering at error level $\delta$, and our stopping rules (Section~\ref{sec:allocation}) approximate this behavior in practice. Active allocation concentrates new trials where posterior uncertainty is largest and near the 40--60\% success region, further reducing the number of human-adjudicated cases.

%\paragraph{Heterogeneity without re-labeling.}
%Item-aware random effects absorb unknown variation in difficulty across panels and over time. When some questioners contribute many items (or add extra items later) and others do not, partial pooling stabilizes estimates without new human screening: sparse items borrow strength from their author, rich items speak for themselves. This makes it safe to accept uneven contributions while keeping answerer rankings comparable.

%\paragraph{Compatibility with weak judges and tools.}
%Where $V$ is not fully automated, weak LLM judges (or tool-augmented agents) can triage obviously correct/incorrect cases, reserving human attention for ambiguous ones. Since judges never need to generate or solve, their role remains within reach even when the systems under test are substantially stronger.

\paragraph{Failure modes and envelope of validity.}
Scalability hinges on the availability of an acceptance oracle $V$ with high coverage and on the continued feasibility of the meaningfulness gate. In domains where \emph{verification itself} becomes intractable for humans and tools, the approach will degrade; our protocol makes this boundary explicit through (i) rising $\rho$ and judge-time statistics, (ii) calibration drift, and (iii) increased disagreement on $M$. Until that boundary is reached, concentrating human effort on $M$ and verification triage provides a durable path to benchmarking systems whose generation and solving capabilities exceed human levels.

%\paragraph{Interpretability and continuity with practice.}
%Mapping MLE estimates to Elo-scale scores preserves pairwise differences and aligns with community expectations (e.g., LMSYS Arena), enabling drop-in interpretability while retaining the order-invariance and joint estimation benefits of batch BT. Together, these properties let a small human committee—augmented by verifiers—continue to produce credible, fine-grained comparisons of increasingly powerful systems at manageable marginal cost.


%\paragraph{Reporting} In practice, the relevant metric for this analysis is the 

%\paragraph{Diagnostics and robustness.}
%We report binned calibration of predicted probabilities $\hat p_{iq}=\sigma(\hat\theta^A_i-\hat\beta_q)$ versus empirical accuracy, confidence intervals for $\theta^A$ and $\psi$, and rank correlations between (i) our MLE, (ii) a pure fixed-effect BT MLE, and (iii) an online Elo run, to demonstrate order invariance and stability. Connectivity of the answerer--questioner graph is checked; disconnected components are fit and reported separately. Ties or partial credit can be accommodated by a Davidson or ordinal link if present, but our main results use the binary outcomes above.


\bibliography{iclr2026_conference}
\bibliographystyle{iclr2026_conference}

\appendix
\section{Appendix}
You may include other additional sections here.


\end{document}