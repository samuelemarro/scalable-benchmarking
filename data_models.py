"""
Pydantic data models for the Turing LLM evaluation pipeline.

These models provide schema validation for all JSON data structures used
throughout the pipeline, catching data corruption early and ensuring consistency.

See docs/schema.md for detailed schema documentation.
"""

from typing import List, Optional, Dict, Any, Literal
from pydantic import BaseModel, Field, field_validator
from constants import (
    VALID_CRITIQUE_VERDICTS,
    VALID_CRITIQUE_DEBATE_VERDICTS,
    VALID_ILLPOSED_DEBATE_VERDICTS,
    VALID_STATUSES,
)


# ============================================================================
# Benchmark Question Generation Models
# ============================================================================


class RefinementAttempt(BaseModel):
    """A single refinement attempt in self-improvement loop."""

    round: int = Field(ge=1, description="Round number (1-indexed)")
    question: str = Field(description="Question text for this attempt")
    answer: str = Field(description="Answer text for this attempt")
    evaluation: Dict[str, Any] = Field(
        description="Self-check evaluation (verdict, ill_posed, issues, improvements)"
    )

    @field_validator("evaluation")
    @classmethod
    def validate_evaluation(cls, v: Dict[str, Any]) -> Dict[str, Any]:
        """Validate evaluation structure."""
        if "verdict" in v and v["verdict"] not in {"pass", "fail"}:
            raise ValueError(f"Invalid evaluation verdict: {v['verdict']}")
        return v


class GenerationRound(BaseModel):
    """A generation round containing refinement attempts."""

    refinement_rounds: List[RefinementAttempt] = Field(
        description="Self-improvement attempts"
    )
    status: Literal["succeeded", "failed"] = Field(description="Round status")


class BenchmarkEntry(BaseModel):
    """A benchmark question generated by a model."""

    run_id: str = Field(description="Unique identifier for the generation run")
    topic_slug: str = Field(description="Topic slug from topic configuration")
    topic_name: Optional[str] = Field(
        None, description="Human-readable topic name"
    )
    status: Literal["succeeded", "failed"] = Field(
        description="Generation status"
    )
    generation_rounds: Optional[List[GenerationRound]] = Field(
        None, description="Self-improvement iteration history"
    )

    @field_validator("status")
    @classmethod
    def validate_status(cls, v: str) -> str:
        """Validate status against known values."""
        if v not in {"succeeded", "failed"}:
            raise ValueError(f"Invalid status: {v}")
        return v


# ============================================================================
# Answer Generation Models
# ============================================================================


class AnswerAttempt(BaseModel):
    """A single answer attempt in self-improvement loop."""

    round: int = Field(ge=1, description="Round number")
    answer: str = Field(description="Answer text")
    raw_answer: Optional[str] = Field(
        None, description="Truly raw answer before any cleaning"
    )
    evaluation: Optional[Dict[str, Any]] = Field(
        None, description="Self-check evaluation"
    )

    @field_validator("evaluation")
    @classmethod
    def validate_evaluation(cls, v: Optional[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """Validate evaluation structure if present."""
        if v is not None and "verdict" in v:
            if v["verdict"] not in {"pass", "fail"}:
                raise ValueError(f"Invalid evaluation verdict: {v['verdict']}")
        return v


class AnswerEntry(BaseModel):
    """An answer generated by a model for another model's question."""

    question_model: str = Field(description="Model that generated the question")
    answer_model: str = Field(description="Model generating this answer")
    question: str = Field(description="The question being answered")
    run_id: Optional[str] = Field(None, description="Run identifier")
    topic_slug: Optional[str] = Field(None, description="Topic slug")
    status: Literal["succeeded", "failed", "ill-posed"] = Field(
        description="Answer generation status"
    )
    attempts: Optional[List[AnswerAttempt]] = Field(
        None, description="Self-improvement attempts"
    )

    @field_validator("status")
    @classmethod
    def validate_status(cls, v: str) -> str:
        """Validate status against known values."""
        if v not in VALID_STATUSES:
            raise ValueError(f"Invalid status: {v} (valid: {VALID_STATUSES})")
        return v


# ============================================================================
# Critique Models
# ============================================================================


class CritiqueAttempt(BaseModel):
    """A single critique attempt in self-improvement loop."""

    round: int = Field(ge=1, description="Round number")
    raw_critique: str = Field(description="Raw critique text from model")
    verdict: str = Field(description="Parsed verdict")
    notes: str = Field(description="Parsed critique notes")
    evaluation: Optional[Dict[str, Any]] = Field(
        None, description="Self-check evaluation"
    )

    @field_validator("verdict")
    @classmethod
    def validate_verdict(cls, v: str) -> str:
        """Validate verdict against known values."""
        if v not in VALID_CRITIQUE_VERDICTS:
            raise ValueError(f"Invalid critique verdict: {v} (valid: {VALID_CRITIQUE_VERDICTS})")
        return v


class CritiqueEntry(BaseModel):
    """A critique of an answer."""

    question: str = Field(description="The original question")
    run_id: Optional[str] = Field(None, description="Run identifier")
    topic_slug: Optional[str] = Field(None, description="Topic slug")
    question_author: str = Field(description="Model that created the question")
    critic: str = Field(description="Model providing critique")
    answer_author: str = Field(description="Model that answered")
    status: Literal["succeeded", "failed"] = Field(
        description="Critique generation status"
    )
    attempts: Optional[List[CritiqueAttempt]] = Field(
        None, description="Self-improvement attempts"
    )

    @field_validator("status")
    @classmethod
    def validate_status(cls, v: str) -> str:
        """Validate status against known values."""
        if v not in {"succeeded", "failed"}:
            raise ValueError(f"Invalid status: {v}")
        return v


# ============================================================================
# Debate Models
# ============================================================================


class DebateMessage(BaseModel):
    """A single message in a debate transcript."""

    round: int = Field(ge=0, description="Round number (0-indexed)")
    speaker: Literal["defender", "claimant"] = Field(
        description="Speaker role"
    )
    message: str = Field(description="Argument text")
    concede: Optional[bool] = Field(
        False, description="Whether speaker conceded"
    )

    @field_validator("speaker")
    @classmethod
    def validate_speaker(cls, v: str) -> str:
        """Validate speaker is one of the allowed roles."""
        if v not in {"defender", "claimant"}:
            raise ValueError(f"Invalid speaker: {v} (must be 'defender' or 'claimant')")
        return v


class DebateEntry(BaseModel):
    """A multi-round debate between models."""

    question: str = Field(description="The question text")
    run_id: Optional[str] = Field(None, description="Run identifier")
    topic_slug: Optional[str] = Field(None, description="Topic slug")
    alice_model: str = Field(description="Model playing Alice role")
    bob_model: str = Field(description="Model playing Bob role")
    claimant: str = Field(description="Model making the claim")
    history: List[DebateMessage] = Field(description="Debate transcript")

    @field_validator("history")
    @classmethod
    def validate_history(cls, v: List[DebateMessage]) -> List[DebateMessage]:
        """Validate debate history is not empty and rounds are sequential."""
        if not v:
            # Empty debates are allowed with --allow-no-debate flag
            return v

        # Check that rounds are sequential
        rounds = [msg.round for msg in v]
        if rounds != sorted(rounds):
            raise ValueError("Debate rounds are not in sequential order")

        return v


# ============================================================================
# Automated Evaluation Models
# ============================================================================


class AutomatedEvaluation(BaseModel):
    """An automated judgment of a debate."""

    id: str = Field(description="Unique task identifier")
    type: Literal["illposed", "critique_debate"] = Field(
        description="Type of evaluation task"
    )
    question_model: str = Field(description="Question author")
    question: str = Field(description="Question text")
    answer_model: str = Field(description="Answer author")
    answer: str = Field(description="Answer text")
    claim: str = Field(description="Claim being judged")
    claimant: str = Field(description="Model making claim")
    defender: str = Field(description="Model defending answer")
    debate_history: Optional[List[DebateMessage]] = Field(
        None, description="Full debate transcript"
    )
    verdict: str = Field(description="Judge's decision")
    confidence: Optional[int] = Field(None, ge=1, le=5, description="Confidence level (1-5)")
    reasoning: str = Field(description="Explanation of verdict")
    judge_model: str = Field(description="Model making judgment")
    status: Optional[Literal["succeeded", "failed"]] = Field(
        None, description="Judgment status"
    )

    @field_validator("type")
    @classmethod
    def validate_type(cls, v: str) -> str:
        """Validate evaluation type."""
        if v not in {"illposed", "critique_debate"}:
            raise ValueError(f"Invalid evaluation type: {v}")
        return v

    @field_validator("verdict")
    @classmethod
    def validate_verdict(cls, v: str, info) -> str:
        """Validate verdict based on evaluation type."""
        # Get type from the validation info (if available)
        eval_type = info.data.get("type")

        if eval_type == "illposed":
            if v not in VALID_ILLPOSED_DEBATE_VERDICTS:
                raise ValueError(
                    f"Invalid illposed debate verdict: {v} (valid: {VALID_ILLPOSED_DEBATE_VERDICTS})"
                )
        elif eval_type == "critique_debate":
            if v not in VALID_CRITIQUE_DEBATE_VERDICTS:
                raise ValueError(
                    f"Invalid critique debate verdict: {v} (valid: {VALID_CRITIQUE_DEBATE_VERDICTS})"
                )

        return v


class EvaluationFile(BaseModel):
    """Container for automated evaluations."""

    decisions: List[AutomatedEvaluation] = Field(
        description="List of automated evaluation decisions"
    )


# ============================================================================
# Metadata Models
# ============================================================================


class Metadata(BaseModel):
    """Optional metadata for versioning and tracking."""

    generated_at: Optional[str] = Field(
        None, description="UTC timestamp of generation (YYYYMMDD_HHMMSS)"
    )
    model_name: Optional[str] = Field(None, description="Model identifier used")
    model_version: Optional[str] = Field(
        None, description="Specific model version from API"
    )
    response_id: Optional[str] = Field(None, description="API response ID")
    created_timestamp: Optional[int] = Field(
        None, description="Unix timestamp from API"
    )


# ============================================================================
# Helper Functions
# ============================================================================


def validate_benchmark_file(data: List[Dict[str, Any]]) -> List[BenchmarkEntry]:
    """Validate a benchmark JSON file."""
    return [BenchmarkEntry(**entry) for entry in data]


def validate_answer_file(data: List[Dict[str, Any]]) -> List[AnswerEntry]:
    """Validate an answer JSON file."""
    return [AnswerEntry(**entry) for entry in data]


def validate_critique_file(data: List[Dict[str, Any]]) -> List[CritiqueEntry]:
    """Validate a critique JSON file."""
    return [CritiqueEntry(**entry) for entry in data]


def validate_debate_file(data: List[Dict[str, Any]]) -> List[DebateEntry]:
    """Validate a debate JSON file."""
    return [DebateEntry(**entry) for entry in data]


def validate_evaluation_file(data: Dict[str, Any]) -> EvaluationFile:
    """Validate an automated evaluation JSON file."""
    if isinstance(data, dict) and "decisions" in data:
        return EvaluationFile(**data)
    # Legacy format: dict of task_id -> evaluation
    elif isinstance(data, dict):
        decisions = list(data.values())
        return EvaluationFile(decisions=decisions)
    else:
        raise ValueError("Invalid evaluation file format")
