"""
Pydantic data models for the Turing LLM evaluation pipeline.

These models provide schema validation for all JSON data structures used
throughout the pipeline, catching data corruption early and ensuring consistency.

See docs/schema.md for detailed schema documentation.
"""

import json
from pathlib import Path
from typing import List, Optional, Dict, Any, Literal, Type, TypeVar
from pydantic import BaseModel, Field, field_validator, model_serializer
from constants import (
    VALID_CRITIQUE_VERDICTS,
    VALID_CRITIQUE_DEBATE_VERDICTS,
    VALID_ILLPOSED_DEBATE_VERDICTS,
    VALID_STATUSES,
    STATUS_FAILED,
    STATUS_PENDING,
    STATUS_SUCCEEDED,
)


# ============================================================================
# Benchmark Question Generation Models
# ============================================================================


class RefinementAttempt(BaseModel):
    """A single refinement attempt in self-improvement loop."""

    round: int = Field(ge=1, description="Round number (1-indexed)")
    question: str = Field(description="Question text for this attempt")
    answer: str = Field(description="Answer text for this attempt")
    raw_answer: Optional[str] = Field(
        None, description="Raw answer text before any cleaning"
    )
    evaluation: Dict[str, Any] = Field(
        description="Self-check evaluation (verdict, ill_posed, issues, improvements)"
    )

    @field_validator("evaluation")
    @classmethod
    def validate_evaluation(cls, v: Dict[str, Any]) -> Dict[str, Any]:
        """Validate evaluation structure."""
        if "verdict" in v and v["verdict"] not in {"pass", "fail"}:
            raise ValueError(f"Invalid evaluation verdict: {v['verdict']}")
        return v


class GenerationRound(BaseModel):
    """A generation round containing refinement attempts."""

    refinement_rounds: List[RefinementAttempt] = Field(
        description="Self-improvement attempts"
    )
    status: Literal["succeeded", "failed", "ill-posed"] = Field(description="Round status")


class BenchmarkEntry(BaseModel):
    """A benchmark question generated by a model."""

    run_id: str = Field(description="Unique identifier for the generation run")
    topic_slug: str = Field(description="Topic slug from topic configuration")
    topic_name: Optional[str] = Field(
        None, description="Human-readable topic name"
    )
    status: Literal["succeeded", "failed", "ill-posed", "pending"] = Field(
        description="Generation status"
    )
    generation_rounds: Optional[List[GenerationRound]] = Field(
        None, description="Self-improvement iteration history"
    )

    @field_validator("status")
    @classmethod
    def validate_status(cls, v: str) -> str:
        """Validate status against known values."""
        if v not in VALID_STATUSES and v != STATUS_PENDING:
            raise ValueError(f"Invalid status: {v}")
        return v

    @model_serializer(mode="wrap")
    def serialize_benchmark_entry(self, handler):
        if self.status == STATUS_PENDING:
            raise ValueError("Cannot serialize BenchmarkEntry with status 'pending'")
        return handler(self)


# ============================================================================
# Answer Generation Models
# ============================================================================


class AnswerAttempt(BaseModel):
    """A single answer attempt in self-improvement loop."""

    round: int = Field(ge=1, description="Round number")
    answer: str = Field(description="Answer text")
    raw_answer: Optional[str] = Field(
        None, description="Truly raw answer before any cleaning"
    )
    evaluation: Optional[Dict[str, Any]] = Field(
        None, description="Self-check evaluation"
    )

    @field_validator("evaluation")
    @classmethod
    def validate_evaluation(cls, v: Optional[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """Validate evaluation structure if present."""
        if v is not None and "verdict" in v:
            if v["verdict"] not in {"pass", "fail"}:
                raise ValueError(f"Invalid evaluation verdict: {v['verdict']}")
        return v


class AnswerEntry(BaseModel):
    """An answer generated by a model for another model's question."""

    question_model: str = Field(description="Model that generated the question")
    answer_model: str = Field(description="Model generating this answer")
    question: str = Field(description="The question being answered")
    run_id: Optional[str] = Field(None, description="Run identifier")
    topic_slug: Optional[str] = Field(None, description="Topic slug")
    ill_posed_claim: Optional[Dict[str, Any]] = Field(
        None, description="Claim details if answer marked ill-posed"
    )
    status: Literal["succeeded", "failed", "ill-posed"] = Field(
        description="Answer generation status"
    )
    attempts: Optional[List[AnswerAttempt]] = Field(
        None, description="Self-improvement attempts"
    )

    @field_validator("status")
    @classmethod
    def validate_status(cls, v: str) -> str:
        """Validate status against known values."""
        if v not in VALID_STATUSES:
            raise ValueError(f"Invalid status: {v} (valid: {VALID_STATUSES})")
        return v


# ============================================================================
# Critique Models
# ============================================================================


class CritiqueAttempt(BaseModel):
    """A single critique attempt in self-improvement loop."""

    round: int = Field(ge=1, description="Round number")
    cleaned_critique: Optional[str] = Field(
        None, description="Cleaned critique text"
    )
    raw_critique: str = Field(description="Raw critique text from model")
    verdict: str = Field(description="Parsed verdict")
    notes: str = Field(description="Parsed critique notes")
    suggestions: Optional[str] = Field(
        None, description="Optional improvement suggestions from critique"
    )
    evaluation: Optional[Dict[str, Any]] = Field(
        None, description="Self-check evaluation"
    )

    @field_validator("verdict")
    @classmethod
    def validate_verdict(cls, v: str) -> str:
        """Validate verdict against known values."""
        if v not in VALID_CRITIQUE_VERDICTS:
            raise ValueError(f"Invalid critique verdict: {v} (valid: {VALID_CRITIQUE_VERDICTS})")
        return v


class CritiqueEntry(BaseModel):
    """A critique of an answer."""

    question: str = Field(description="The original question")
    run_id: Optional[str] = Field(None, description="Run identifier")
    topic_slug: Optional[str] = Field(None, description="Topic slug")
    question_author: str = Field(description="Model that created the question")
    critic: str = Field(description="Model providing critique")
    answer_author: str = Field(description="Model that answered")
    status: Literal["succeeded", "failed"] = Field(
        description="Critique generation status"
    )
    attempts: Optional[List[CritiqueAttempt]] = Field(
        None, description="Self-improvement attempts"
    )

    @field_validator("status")
    @classmethod
    def validate_status(cls, v: str) -> str:
        """Validate status against known values."""
        if v not in {STATUS_SUCCEEDED, STATUS_FAILED}:
            raise ValueError(f"Invalid status: {v}")
        return v


# ============================================================================
# Debate Models
# ============================================================================


class DebateMessage(BaseModel):
    """A single message in a debate transcript."""

    round: int = Field(ge=0, description="Round number")
    speaker: Literal["defender", "claimant", "Alice", "Bob"] = Field(
        description="Speaker role or alias"
    )
    message: str = Field(description="Argument text")
    concede: Optional[bool] = Field(
        False, description="Whether speaker conceded"
    )

    @field_validator("speaker")
    @classmethod
    def validate_speaker(cls, v: str) -> str:
        """Validate speaker is one of the allowed roles."""
        if v not in {"defender", "claimant", "Alice", "Bob"}:
            raise ValueError("Invalid speaker: must be 'defender', 'claimant', 'Alice', or 'Bob'")
        return v


class DebateEntry(BaseModel):
    """A multi-round debate between models."""

    question: str = Field(description="The question text")
    run_id: Optional[str] = Field(None, description="Run identifier")
    topic_slug: Optional[str] = Field(None, description="Topic slug")
    alice_model: str = Field(description="Model playing Alice role")
    bob_model: str = Field(description="Model playing Bob role")
    claimant: Optional[str] = Field(None, description="Model making the claim")
    answer_author: Optional[str] = Field(None, description="Answer author (critique debates)")
    critic: Optional[str] = Field(None, description="Critic model (critique debates)")
    history: List[DebateMessage] = Field(description="Debate transcript")

    @field_validator("history")
    @classmethod
    def validate_history(cls, v: List[DebateMessage]) -> List[DebateMessage]:
        """Validate debate history is not empty and rounds are sequential."""
        if not v:
            # Empty debates are allowed with --allow-no-debate flag
            return v

        # Check that rounds are sequential
        rounds = [msg.round for msg in v]
        if rounds != sorted(rounds):
            raise ValueError("Debate rounds are not in sequential order")

        return v


# ============================================================================
# Automated Evaluation Models
# ============================================================================


class AutomatedEvaluation(BaseModel):
    """An automated judgment of a debate."""

    id: str = Field(description="Unique task identifier")
    type: Literal["illposed", "critique", "critique_debate"] = Field(
        description="Type of evaluation task"
    )
    mode: Optional[str] = Field(None, description="Critique mode")
    question_model: Optional[str] = Field(None, description="Question author")
    question: Optional[str] = Field(None, description="Question text")
    answer_model: Optional[str] = Field(None, description="Answer author")
    answer: Optional[str] = Field(None, description="Answer text")
    critic_model: Optional[str] = Field(None, description="Critic model")
    claim: Optional[str] = Field(None, description="Claim being judged")
    claimant: Optional[str] = Field(None, description="Model making claim")
    defender: Optional[str] = Field(None, description="Model defending answer")
    debate_history: Optional[List[DebateMessage]] = Field(
        None, description="Full debate transcript"
    )
    verdict: str = Field(description="Judge's decision")
    confidence: Optional[int] = Field(None, ge=1, le=5, description="Confidence level (1-5)")
    reasoning: Optional[str] = Field(None, description="Explanation of verdict")
    judge_model: Optional[str] = Field(None, description="Model making judgment")
    status: Optional[Literal["succeeded", "failed"]] = Field(
        None, description="Judgment status"
    )
    raw_response: Optional[str] = Field(None, description="Raw judge response")
    run_id: Optional[str] = Field(None, description="Run identifier")
    topic_slug: Optional[str] = Field(None, description="Topic slug")

    @field_validator("type")
    @classmethod
    def validate_type(cls, v: str) -> str:
        """Validate evaluation type."""
        if v not in {"illposed", "critique", "critique_debate"}:
            raise ValueError(f"Invalid evaluation type: {v}")
        return v

    @field_validator("verdict")
    @classmethod
    def validate_verdict(cls, v: str, info) -> str:
        """Validate verdict based on evaluation type."""
        # Get type from the validation info (if available)
        eval_type = info.data.get("type")

        if eval_type == "illposed":
            if v not in VALID_ILLPOSED_DEBATE_VERDICTS:
                raise ValueError(
                    f"Invalid illposed debate verdict: {v} (valid: {VALID_ILLPOSED_DEBATE_VERDICTS})"
                )
        elif eval_type in {"critique", "critique_debate"}:
            if v not in VALID_CRITIQUE_DEBATE_VERDICTS:
                raise ValueError(
                    f"Invalid critique debate verdict: {v} (valid: {VALID_CRITIQUE_DEBATE_VERDICTS})"
                )

        return v


class EvaluationFile(BaseModel):
    """Container for automated evaluations."""

    decisions: List[AutomatedEvaluation] = Field(
        description="List of automated evaluation decisions"
    )


# ============================================================================
# Human Evaluation Models
# ============================================================================


class HumanEvaluation(BaseModel):
    """A human judgment for labeling tasks."""

    id: str = Field(description="Unique task identifier")
    type: Literal["illposed", "critique"] = Field(
        description="Type of evaluation task"
    )
    mode: Optional[str] = Field(None, description="Critique mode")
    question_model: Optional[str] = Field(None, description="Question author")
    answer_model: Optional[str] = Field(None, description="Answer author")
    critic_model: Optional[str] = Field(None, description="Critic model")
    verdict: str = Field(description="Human verdict")
    confidence: Optional[int] = Field(None, ge=1, le=5, description="Confidence level (1-5)")
    comment: Optional[str] = Field(None, description="Optional comment")

    @field_validator("verdict")
    @classmethod
    def validate_verdict(cls, v: str, info) -> str:
        eval_type = info.data.get("type")
        illposed_verdicts = {
            "ill-posed",
            "not ill-posed",
            "ill-posed but wrong reason",
            "correct",
            "incorrect",
            "unknown",
            "invalid",
            "other",
        }
        critique_verdicts = {
            "incorrect",
            "correct",
            "incorrect but wrong reason",
            "unknown",
            "invalid",
            "other",
        }
        if eval_type == "illposed":
            valid = illposed_verdicts | VALID_ILLPOSED_DEBATE_VERDICTS
            if v not in valid:
                raise ValueError(f"Invalid ill-posed verdict: {v}")
        if eval_type == "critique":
            valid = critique_verdicts | VALID_CRITIQUE_DEBATE_VERDICTS
            if v not in valid:
                raise ValueError(f"Invalid critique verdict: {v}")
        return v


class HumanEvaluationFile(BaseModel):
    """Container for human evaluations."""

    decisions: List[HumanEvaluation] = Field(
        description="List of human evaluation decisions"
    )


# ============================================================================
# Judging Task Models
# ============================================================================


class JudgingTask(BaseModel):
    """A task prepared for automated or human judging."""

    id: str = Field(description="Unique task identifier")
    type: Literal["illposed", "critique"] = Field(description="Task type")
    mode: Optional[str] = Field(None, description="Critique mode")
    question_model: Optional[str] = Field(None, description="Question author")
    answer_model: Optional[str] = Field(None, description="Answer author")
    critic_model: Optional[str] = Field(None, description="Critic model")
    question: Optional[str] = Field(None, description="Question text")
    answer: Optional[str] = Field(None, description="Answer text")
    critique: Optional[str] = Field(None, description="Critique text")
    debate_history: Optional[List[DebateMessage]] = Field(
        None, description="Debate transcript"
    )
    alice_model: Optional[str] = Field(None, description="Alice role model")
    bob_model: Optional[str] = Field(None, description="Bob role model")
    run_id: Optional[str] = Field(None, description="Run identifier")
    topic_slug: Optional[str] = Field(None, description="Topic slug")


# ============================================================================
# Metadata Models
# ============================================================================


class Metadata(BaseModel):
    """Optional metadata for versioning and tracking."""

    generated_at: Optional[str] = Field(
        None, description="UTC timestamp of generation (YYYYMMDD_HHMMSS)"
    )
    model_name: Optional[str] = Field(None, description="Model identifier used")
    model_version: Optional[str] = Field(
        None, description="Specific model version from API"
    )
    response_id: Optional[str] = Field(None, description="API response ID")
    created_timestamp: Optional[int] = Field(
        None, description="Unix timestamp from API"
    )


# ============================================================================
# Helper Functions
# ============================================================================


ModelT = TypeVar("ModelT", bound=BaseModel)


def _validate_entries(model: Type[ModelT], data: List[Dict[str, Any]]) -> List[ModelT]:
    validated: List[ModelT] = []
    for idx, entry in enumerate(data):
        if not entry:
            continue
        try:
            validated.append(model(**entry))
        except Exception as exc:
            raise ValueError(f"Invalid entry at index {idx}: {exc}") from exc
    return validated


def validate_benchmark_file(data: List[Dict[str, Any]]) -> List[BenchmarkEntry]:
    """Validate a benchmark JSON file."""
    return _validate_entries(BenchmarkEntry, data)


def validate_answer_file(data: List[Dict[str, Any]]) -> List[AnswerEntry]:
    """Validate an answer JSON file."""
    return _validate_entries(AnswerEntry, data)


def validate_critique_file(data: List[Dict[str, Any]]) -> List[CritiqueEntry]:
    """Validate a critique JSON file."""
    return _validate_entries(CritiqueEntry, data)


def validate_debate_file(data: List[Dict[str, Any]]) -> List[DebateEntry]:
    """Validate a debate JSON file."""
    return _validate_entries(DebateEntry, data)


def validate_evaluation_file(data: Dict[str, Any]) -> EvaluationFile:
    """Validate an automated evaluation JSON file."""
    if isinstance(data, dict) and "decisions" in data:
        decisions = data.get("decisions") or []
        validated = _validate_entries(AutomatedEvaluation, decisions)
        return EvaluationFile(decisions=validated)
    raise ValueError("Invalid evaluation file format")


def load_model_list(path: Path, model: Type[ModelT]) -> List[Optional[ModelT]]:
    if not path.exists():
        return []
    data = json.loads(path.read_text())
    if not isinstance(data, list):
        raise ValueError(f"Expected list in {path}, got {type(data).__name__}")
    parsed: List[Optional[ModelT]] = []
    for idx, entry in enumerate(data):
        if not entry:
            parsed.append(None)
            continue
        try:
            parsed.append(model(**entry))
        except Exception as exc:
            raise ValueError(f"Invalid entry at index {idx} in {path}: {exc}") from exc
    return parsed


def save_model_list(path: Path, items: List[Optional[BaseModel]]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    payload: List[Optional[Dict[str, Any]]] = []
    for item in items:
        if item is not None:
            payload.append(item.model_dump(exclude_none=True))
    path.write_text(json.dumps(payload, indent=2))


def load_benchmark_entries(path: Path) -> List[Optional[BenchmarkEntry]]:
    return load_model_list(path, BenchmarkEntry)


def load_answer_entries(path: Path) -> List[Optional[AnswerEntry]]:
    return load_model_list(path, AnswerEntry)


def load_critique_entries(path: Path) -> List[Optional[CritiqueEntry]]:
    return load_model_list(path, CritiqueEntry)


def load_debate_entries(path: Path) -> List[Optional[DebateEntry]]:
    return load_model_list(path, DebateEntry)


def save_benchmark_entries(path: Path, entries: List[Optional[BenchmarkEntry]]) -> None:
    save_model_list(path, entries)


def save_answer_entries(path: Path, entries: List[Optional[AnswerEntry]]) -> None:
    save_model_list(path, entries)


def save_critique_entries(path: Path, entries: List[Optional[CritiqueEntry]]) -> None:
    save_model_list(path, entries)


def save_debate_entries(path: Path, entries: List[Optional[DebateEntry]]) -> None:
    save_model_list(path, entries)


def load_evaluation_entries(path: Path) -> EvaluationFile:
    if not path.exists():
        return EvaluationFile(decisions=[])
    data = json.loads(path.read_text())
    if not (isinstance(data, dict) and "decisions" in data):
        raise ValueError(f"Invalid evaluation file format in {path}")
    raw_decisions = data.get("decisions") or []
    decisions: List[AutomatedEvaluation] = []
    for idx, entry in enumerate(raw_decisions):
        if not entry:
            continue
        try:
            decisions.append(AutomatedEvaluation(**entry))
        except Exception as exc:
            raise ValueError(f"Invalid decision at index {idx} in {path}: {exc}") from exc
    return EvaluationFile(decisions=decisions)


def save_evaluation_entries(path: Path, payload: EvaluationFile) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    decisions = [entry.model_dump(exclude_none=True) for entry in payload.decisions]
    path.write_text(json.dumps({"decisions": decisions}, indent=2))


def load_human_evaluation_entries(path: Path) -> HumanEvaluationFile:
    if not path.exists():
        return HumanEvaluationFile(decisions=[])
    data = json.loads(path.read_text())
    if not (isinstance(data, dict) and "decisions" in data):
        raise ValueError(f"Invalid human evaluation file format in {path}")
    raw_decisions = data.get("decisions") or []
    decisions: List[HumanEvaluation] = []
    for idx, entry in enumerate(raw_decisions):
        if not entry:
            continue
        try:
            decisions.append(HumanEvaluation(**entry))
        except Exception as exc:
            raise ValueError(f"Invalid human decision at index {idx} in {path}: {exc}") from exc
    return HumanEvaluationFile(decisions=decisions)


def save_human_evaluation_entries(path: Path, payload: HumanEvaluationFile) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    decisions = [entry.model_dump(exclude_none=True) for entry in payload.decisions]
    path.write_text(json.dumps({"decisions": decisions}, indent=2))
