[
    {
        "question": "### A Functional Equation for Polynomials\n\nFind all non-constant polynomials `P(x)` with complex coefficients that satisfy the functional equation\n$$P(x)P(x+1) = P(x^2)$$\nfor all $x \\in \\mathbb{C}$.\n\n**Goal:** Provide a complete characterization of all such polynomials.",
        "answer": "Let the given functional equation be denoted by $(*)$.\n$$P(x)P(x+1) = P(x^2) \\quad (*)$$\nWe are looking for all non-constant polynomial solutions $P(x) \\in \\mathbb{C}[x]$.\n\n### Step 1: Analyze the leading coefficient and the constant term.\n\nLet $d = \\deg(P) \\ge 1$ be the degree of the polynomial $P(x)$, and let $a_d$ be its leading coefficient.\nThe leading term of $P(x)$ is $a_d x^d$.\nThe leading term of the left-hand side (LHS) of $(*)$ is $(a_d x^d)(a_d (x+1)^d) = a_d^2 x^{2d} + \\dots$.\nThe leading term of the right-hand side (RHS) of $(*)$ is $a_d (x^2)^d + \\dots = a_d x^{2d} + \\dots$.\nComparing the leading coefficients, we must have $a_d^2 = a_d$. Since $P(x)$ is non-constant, $d \\ge 1$ and $a_d \\ne 0$. Thus, we must have $a_d=1$.\n\nNext, let's evaluate $(*)$ at $x=0$:\n$P(0)P(1) = P(0^2) = P(0)$.\nThis implies $P(0)(P(1)-1) = 0$.\nSo, we must have either $P(0)=0$ or $P(1)=1$. We analyze these two cases.\n\n### Step 2: Analyze the roots of P(x).\n\nLet $Z$ be the set of roots of $P(x)$. From $(*)$, if $P(z)=0$ for some $z \\in \\mathbb{C}$, then $P(z^2) = 0$. This means if $z \\in Z$, then $z^2 \\in Z$. By induction, $z, z^2, z^4, z^8, \\dots, z^{2^k}, \\dots$ are all roots of $P(x)$. Since a polynomial can only have a finite number of roots, this sequence must be finite. This occurs if and only if $z=0$ or $z$ is a root of unity.\n\nFurthermore, if we substitute $x-1$ for $x$ in $(*)$, we get $P(x-1)P(x) = P((x-1)^2)$.\nThis tells us that if $z \\in Z$, then $(z-1)^2$ must also be a root.\n\nCombining these, the set of roots $Z$ is closed under the transformations $f(z) = z^2$ and $g(z) = (z-1)^2$.\n\nLet's find the location of the roots. Let $z_0$ be a root of maximum possible modulus.\nIf $|z_0|>1$, then $|z_0^2| = |z_0|^2 > |z_0|$. This would generate a sequence of roots with strictly increasing moduli, contradicting that the set of roots is finite. Therefore, all roots must satisfy $|z| \\le 1$.\n\nNow consider $z_0$, a root of maximum modulus, so $|z_0| \\le 1$. The root $w_0 = (z_0-1)^2$ must also satisfy $|w_0| \\le |z_0|$.\nSo, $|z_0-1|^2 \\le |z_0|$.\nLet's analyze this for roots on the unit circle, i.e., $|z_0|=1$. If $z_0 = e^{i\\theta}$, this condition becomes:\n$|e^{i\\theta}-1|^2 \\le 1$\n$(\\cos\\theta - 1)^2 + \\sin^2\\theta \\le 1$\n$\\cos^2\\theta - 2\\cos\\theta + 1 + \\sin^2\\theta \\le 1$\n$2 - 2\\cos\\theta \\le 1 \\implies 1 \\le 2\\cos\\theta \\implies \\cos\\theta \\ge 1/2$.\nThis means $\\theta \\in [-\\pi/3, \\pi/3] \\pmod{2\\pi}$.\n\nSo, any root of $P(x)$ on the unit circle must lie on the arc defined by $\\theta \\in [-\\pi/3, \\pi/3]$.\nNow, if $z_0=e^{i\\theta}$ is a root on the unit circle, $w_0=(z_0-1)^2$ is also a root. Its modulus is $|w_0|=|z_0-1|^2 = 2-2\\cos\\theta$. For $w_0$ to also be on the unit circle, its modulus must be 1.\n$2-2\\cos\\theta = 1 \\implies \\cos\\theta = 1/2 \\implies \\theta = \\pm \\pi/3$.\nLet's test these cases:\n- If $z_0 = e^{i\\pi/3}$, then $w_0 = (e^{i\\pi/3}-1)^2 = ((\\frac{1}{2}-1) + i\\frac{\\sqrt{3}}{2})^2 = (-\\frac{1}{2} + i\\frac{\\sqrt{3}}{2})^2 = (e^{i2\\pi/3})^2 = e^{i4\\pi/3}$.\nThe argument of $w_0$ is $4\\pi/3$, which gives $\\cos(4\\pi/3) = -1/2$. This violates the condition $\\cos(\\arg(w_0)) \\ge 1/2$.\nSo, with the exception of $\\theta=0$, if a root $z$ is on the unit circle, the derived root $w=(z-1)^2$ is either strictly inside the unit circle or on the unit circle at a location that violates the condition.\nThe only root on the unit circle that does not lead to this contradiction is when $\\theta=0$, which is the root $z=1$. If $z=1$ is a root, then $(1-1)^2=0$ is a root.\n\n### Step 3: Case analysis from Step 1.\n\n**Case 1: $P(0)=0$.**\nThis means $0$ is a root. Then, as shown above, $1$ must be a root.\nLet $P(x) = x^k (x-1)^m Q(x)$ for integers $k,m \\ge 1$ and a polynomial $Q(x)$ with $Q(0) \\ne 0$ and $Q(1) \\ne 0$.\nSubstituting this into $(*)$:\n$$ [x^k (x-1)^m Q(x)] [(x+1)^k x^m Q(x+1)] = (x^2)^k (x^2-1)^m Q(x^2) $$\n$$ x^{k+m} (x-1)^m (x+1)^k Q(x) Q(x+1) = x^{2k} (x-1)^m (x+1)^m Q(x^2) $$\nDividing by common factors for $x \\notin \\{0, 1, -1\\}$:\n$$ x^{m-k} (x+1)^{k-m} Q(x) Q(x+1) = Q(x^2) $$\n$$ \\left(\\frac{x+1}{x}\\right)^{k-m} Q(x)Q(x+1) = Q(x^2) $$\nSince $Q(x)$ is a polynomial, $Q(x)Q(x+1)$ is a polynomial. For the LHS to be a polynomial for all $x$, we must have $k-m=0$, i.e., $k=m$. If $k>m$, setting $x \\to -1$ would yield $0$ on the LHS, while the RHS is $Q(1) \\ne 0$. If $m>k$, setting $x \\to 0$ would yield $0$ on the LHS, while the RHS is $Q(0) \\ne 0$.\nSo, we must have $k=m$. The equation for $Q(x)$ becomes:\n$$Q(x)Q(x+1) = Q(x^2)$$\n$Q(x)$ must satisfy the original functional equation. We also know $Q(0) \\ne 0$ and $Q(1) \\ne 0$. Since the leading coefficient of $P(x)=(x^2-x)^k Q(x)$ is 1, the leading coefficient of $Q(x)$ must also be 1.\nIf $Q(x)$ is non-constant, it must have roots. The analysis of roots showed that any root must satisfy $|z| \\le 1$. But we also have $Q(0) \\ne 0$ and $Q(1) \\ne 0$. The root analysis showed that the only possible root on the unit circle is $z=1$, which is not a root of $Q$. Thus, all roots of $Q(x)$ must satisfy $|z|<1$.\nLet $z_r$ be a root of $Q(x)$ of maximum modulus, so $0 < |z_r| < 1$. Then $z_r^2, z_r^4, z_r^8, \\dots$ must all be roots. This sequence of roots converges to 0. Since the set of roots is finite, this is only possible if $z_r=0$, but $Q(0) \\ne 0$. This is a contradiction.\nTherefore, $Q(x)$ can have no roots. A polynomial with no roots must be a constant. Since its leading coefficient is 1, $Q(x)$ must be the constant polynomial $Q(x)=1$.\nThis gives the solutions in this case: $P(x) = (x^2-x)^k$ for any integer $k \\ge 1$.\n\n**Case 2: $P(1)=1$ and $P(0) \\ne 0$.**\nLet $P(x)$ be a non-constant solution. As before, the leading coefficient is 1.\nThe root analysis tells us that any root $z$ must satisfy $|z|\\le 1$.\nSince $P(0) \\ne 0$, 0 is not a root.\nSince $P(1) = 1$, 1 is not a root.\nThis means any root must satisfy $0 < |z| < 1$ or $|z|=1, z \\ne 1$.\nThe argument from Step 2 showed that any root on the unit circle must be $z=1$. Since $P(1) \\ne 0$, there are no roots on the unit circle.\nSo all roots must satisfy $0 < |z| < 1$.\nIf $P(x)$ has a root $z_r$, then $z_r, z_r^2, z_r^4, \\dots$ must all be roots. This sequence converges to 0. By continuity of polynomials, we must have $P(0)=0$. But this contradicts our case assumption $P(0) \\ne 0$.\nTherefore, $P(x)$ has no roots. It must be a constant polynomial, $P(x)=c$.\nThe leading coefficient is 1, so $c=1$. But we assumed $P(x)$ to be non-constant. So there are no solutions in this case.\n\n### Conclusion\n\nThe only solutions are those found in Case 1. We can verify this family of solutions:\nLet $P(x) = (x^2-x)^k$.\nLHS: $P(x)P(x+1) = (x^2-x)^k ((x+1)^2-(x+1))^k = (x(x-1))^k (x^2+2x+1-x-1)^k = (x(x-1))^k (x^2+x)^k = (x(x-1)x(x+1))^k = (x^2(x^2-1))^k = (x^4-x^2)^k$.\nRHS: $P(x^2) = ((x^2)^2-x^2)^k = (x^4-x^2)^k$.\nThe LHS and RHS are equal.\n\nThe set of all non-constant polynomial solutions is:\n$$ P(x) = (x^2 - x)^k \\quad \\text{for any integer } k \\ge 1 $$"
    },
    {
        "question": "### An Integral for Ap\u00e9ry's Constant\n\nEvaluate the following definite integral. The bounds of integration are from 0 to 1.\n\n$$I = \\int_0^1 \\frac{\\ln(x) \\ln(1-x)}{x} \\,dx$$\n\n**Goal:** Provide the exact, closed-form value of the integral. The final answer may be expressed in terms of known mathematical constants.",
        "answer": "The value of the integral is $\\zeta(3)$, which is Ap\u00e9ry's constant. Here is a step-by-step derivation.\n\n### 1. Initial Analysis\nThe integral is improper at both endpoints, $x=0$ and $x=1$. We must first ensure it converges.\n- As $x \\to 0^+$, the integrand behaves like $\\frac{\\ln(x) \\cdot (-x)}{x} = -\\ln(x)$, which is integrable near $0$.\n- As $x \\to 1^-$, let $u = 1-x$. The integrand behaves like $\\frac{\\ln(1-u) \\ln(u)}{1-u} \\approx \\ln(u)$, since $\\ln(1-u) \\approx -(u)$ is well-behaved. The function $\\ln(u)$ is integrable near $u=0$.\nSince the integral converges, we can proceed with its evaluation.\n\n### 2. Series Expansion\nThe core of the solution lies in expanding one of the logarithmic terms into its Maclaurin series. The term $\\ln(1-x)$ is a good candidate. For $|x| < 1$, the expansion is:\n$$ \\ln(1-x) = -\\sum_{n=1}^{\\infty} \\frac{x^n}{n} $$\nSubstituting this series into the integral gives:\n$$ I = \\int_0^1 \\frac{\\ln(x)}{x} \\left( -\\sum_{n=1}^{\\infty} \\frac{x^n}{n} \\right) \\,dx = -\\int_0^1 \\ln(x) \\sum_{n=1}^{\\infty} \\frac{x^{n-1}}{n} \\,dx $$\n\n### 3. Swapping Integral and Summation\nWe want to swap the order of integration and summation. This is permissible if the conditions for a convergence theorem are met.\n$$ I = -\\sum_{n=1}^{\\infty} \\frac{1}{n} \\int_0^1 x^{n-1} \\ln(x) \\,dx $$\nThis step can be rigorously justified using the Monotone Convergence Theorem, as the integrand $f(x) = \\frac{\\ln(x)\\ln(1-x)}{x}$ is non-negative on the interval $(0, 1)$. Alternatively, one can use dominated convergence.\n\n### 4. Evaluating the Inner Integral\nLet's focus on evaluating the integral inside the summation for a fixed integer $n \\ge 1$:\n$$ J_n = \\int_0^1 x^{n-1} \\ln(x) \\,dx $$\nWe can solve this using integration by parts, with the parts chosen as:\n- $u = \\ln(x) \\implies du = \\frac{1}{x} \\,dx$\n- $dv = x^{n-1} \\,dx \\implies v = \\frac{x^n}{n}$\n\nApplying the integration by parts formula, $\\int u \\,dv = uv - \\int v \\,du$:\n$$ J_n = \\left[ \\frac{x^n}{n} \\ln(x) \\right]_0^1 - \\int_0^1 \\frac{x^n}{n} \\cdot \\frac{1}{x} \\,dx $$\nThe boundary term $[ \\frac{x^n}{n} \\ln(x) ]_0^1$ evaluates to zero:\n- At $x=1$, $\\frac{1^n}{n} \\ln(1) = 0$.\n- As $x \\to 0^+$, we have $\\lim_{x\\to 0^+} x^n \\ln(x) = 0$ (a standard limit, provable with L'H\u00f4pital's rule).\n\nThis leaves us with the remaining integral:\n$$ J_n = - \\int_0^1 \\frac{x^{n-1}}{n} \\,dx = -\\frac{1}{n} \\left[ \\frac{x^n}{n} \\right]_0^1 = -\\frac{1}{n} \\left( \\frac{1}{n} - 0 \\right) = -\\frac{1}{n^2} $$\n\n### 5. Final Summation\nNow, we substitute the result for $J_n$ back into the expression for $I$:\n$$ I = -\\sum_{n=1}^{\\infty} \\frac{1}{n} J_n = -\\sum_{n=1}^{\\infty} \\frac{1}{n} \\left( -\\frac{1}{n^2} \\right) = \\sum_{n=1}^{\\infty} \\frac{1}{n^3} $$\nThis sum is the definition of the Riemann zeta function evaluated at $s=3$.\n$$ \\sum_{n=1}^{\\infty} \\frac{1}{n^3} = \\zeta(3) $$\nThe constant $\\zeta(3)$ is known as Ap\u00e9ry's constant, with an approximate value of $1.2020569...$.\n\nThus, the exact value of the integral is:\n$$ I = \\zeta(3) $$"
    },
    {
        "question": "### A Congruence for an Elementary Symmetric Polynomial\n\nLet `s_k(x_1, ..., x_n)` be the elementary symmetric polynomial of degree `k` in `n` variables. This is defined as the sum of all distinct products of `k` variables from the set `{x_1, ..., x_n}`. For example, `s_2(a, b, c) = ab + ac + bc`.\n\nLet `p` be a prime number such that `p > 3`. Consider the set of the first `p-1` positive integers, `S = {1, 2, ..., p-1}`.\n\n**Goal:** Determine the value of `s_{p-2}(1, 2, ..., p-1)` modulo `p^2`.\n\nThat is, find the unique integer `r` such that `s_{p-2}(1, 2, ..., p-1) \\equiv r \\pmod{p^2}` and `0 \\le r < p^2`.",
        "answer": "The value is `r=0`. In other words, `s_{p-2}(1, 2, ..., p-1)` is divisible by `p^2`. This is a result known as Wolstenholme's theorem.\n\nHere is a full derivation of the result.\n\n### Derivation\n\nLet `A = s_{p-2}(1, 2, ..., p-1)`. The derivation proceeds in four main steps:\n1.  Establish an identity relating `A` to the sum of reciprocals of the integers `{1, 2, ..., p-1}`.\n2.  Algebraically manipulate this sum of reciprocals to reveal a factor of `p`.\n3.  Analyze the remaining terms using modular arithmetic in the field `\\mathbb{Z}_p`.\n4.  Combine the results to prove the final congruence `A \\equiv 0 \\pmod{p^2}`.\n\n#### Step 1: An identity for `s_{p-2}`\n\nThe term `s_{p-2}(1, 2, ..., p-1)` is the sum of all products of `p-2` distinct elements from the set `S = {1, 2, ..., p-1}`. Each term in this sum is formed by omitting exactly one element, say `k`, from the product of all elements in `S`.\n\nThe product of all elements in `S` is `(p-1)!`.\nA product of `p-2` elements can be written as `(p-1)! / k` for some `k \\in S`.\nSumming over all possible choices of `k` gives the value of `A`:\n$$A = \\sum_{k=1}^{p-1} \\frac{(p-1)!}{k} = (p-1)! \\sum_{k=1}^{p-1} \\frac{1}{k}$$\nThis is an exact identity in the rational numbers `\\mathbb{Q}`. Note that `A` is by definition an integer.\n\n#### Step 2: Transformation of the Sum of Reciprocals\n\nLet `H_{p-1} = \\sum_{k=1}^{p-1} \\frac{1}{k}`. We can rearrange the terms of this sum by pairing `k` with `p-k`:\n$$H_{p-1} = \\sum_{k=1}^{p-1} \\frac{1}{k} = \\frac{1}{2} \\sum_{k=1}^{p-1} \\left(\\frac{1}{k} + \\frac{1}{p-k}\\right)$$\nThe term inside the summation is:\n$$\\frac{1}{k} + \\frac{1}{p-k} = \\frac{(p-k) + k}{k(p-k)} = \\frac{p}{k(p-k)}$$\nSubstituting this back, we get:\n$$H_{p-1} = \\frac{1}{2} \\sum_{k=1}^{p-1} \\frac{p}{k(p-k)} = \\frac{p}{2} \\sum_{k=1}^{p-1} \\frac{1}{k(p-k)}$$\nNow, substitute this expression for `H_{p-1}` back into our identity for `A`:\n$$A = (p-1)! \\cdot H_{p-1} = (p-1)! \\cdot \\frac{p}{2} \\sum_{k=1}^{p-1} \\frac{1}{k(p-k)} = \\frac{p!}{2} \\sum_{k=1}^{p-1} \\frac{1}{k(p-k)}$$\nThis provides an exact integer expression for `A`, since the denominator of the sum, when written as a single fraction, will only contain prime factors smaller than `p` and thus will be cancelled by `p!`.\n\n#### Step 3: Modular Analysis\n\nOur goal is to determine `A` modulo `p^2`. From the expression `A = \\frac{p!}{2} \\sum_{k=1}^{p-1} \\frac{1}{k(p-k)}`, we can see `A` has a factor of `p`. To check for a factor of `p^2`, we must analyze the remaining part `\\frac{(p-1)!}{2} \\sum_{k=1}^{p-1} \\frac{1}{k(p-k)}` modulo `p`.\n\nLet `T = \\sum_{k=1}^{p-1} \\frac{1}{k(p-k)}`. This is a rational number whose denominator is coprime to `p`. We can therefore analyze its value modulo `p`. Working in `\\mathbb{Z}_p` (the integers modulo `p`):\n$$T \\equiv \\sum_{k=1}^{p-1} \\frac{1}{k(0-k)} \\equiv \\sum_{k=1}^{p-1} \\frac{1}{-k^2} \\equiv -\\sum_{k=1}^{p-1} (k^{-1})^2 \\pmod{p}$$\nThe mapping `k \\mapsto k^{-1}` (modular inverse) is a permutation of the set `(\\mathbb{Z}_p)^\\times = \\{1, 2, ..., p-1\\}`. Therefore, the sum of `(k^{-1})^2` over all `k` in this set is the same as the sum of `j^2` over all `j` in the set:\n$$\\sum_{k=1}^{p-1} (k^{-1})^2 \\equiv \\sum_{j=1}^{p-1} j^2 \\pmod{p}$$\nThe sum of the first `n` squares is given by the formula `n(n+1)(2n+1)/6`. For `n=p-1`, we have:\n$$\\sum_{j=1}^{p-1} j^2 = \\frac{(p-1)p(2p-1)}{6}$$\nSince `p > 3` is a prime number, `p` is not `2` or `3`, so `\\gcd(p, 6) = 1`. This means the expression `(p-1)p(2p-1)/6` is an integer and is divisible by `p`.\nThus,\n$$\\sum_{j=1}^{p-1} j^2 \\equiv 0 \\pmod{p}$$\nThis implies that `T \\equiv 0 \\pmod p`.\n\n#### Step 4: Conclusion\n\nThe congruence `T \\equiv 0 \\pmod p` means that if we write `T` as a fraction `a/b` where `\\gcd(b, p) = 1`, then the numerator `a` must be divisible by `p`. So we can write `T = \\frac{mp}{b}` for some integer `m`.\n\nNow substitute this back into our expression for `A`:\n$$A = \\frac{p!}{2} \\cdot T = \\frac{p!}{2} \\cdot \\frac{mp}{b} = p^2 \\cdot \\frac{m \\cdot (p-1)!}{2b}$$\nWe know `A` is an integer. Since `p > 3`, `p` is coprime to `2`. Also, by construction of `T`, `p` is coprime to `b`. Therefore, `\\gcd(p^2, 2b) = 1`. From the final expression for `A`, for `A` to be an integer, the term `\\frac{m \\cdot (p-1)!}{2b}` must be an integer.\n\nThis shows that `A` is an integer multiple of `p^2`. Therefore, `A` is divisible by `p^2`.\n\nSo, `s_{p-2}(1, 2, ..., p-1) \\equiv 0 \\pmod{p^2}`.\n\nThe final answer is `r=0`."
    },
    {
        "question": "### Sum of Squares of Alternating Permutations\n\nLet `n` be a positive integer and let `S_n` denote the set of all permutations of the set `{1, 2, ..., n}`.\n\nFor a given permutation `\u03c3 \u2208 S_n`, we define its **alternating sum** `A(\u03c3)` as:\n$$A(\u03c3) = \\sum_{k=1}^{n} (-1)^{k-1} \u03c3(k) = \u03c3(1) - \u03c3(2) + \u03c3(3) - \\dots + (-1)^{n-1}\u03c3(n)$$\n\n**Goal:**\nLet `n` be an **odd** positive integer. Find a simplified, closed-form expression for the following total sum, `T_n`, in terms of `n`:\n$$T_n = \\sum_{\u03c3 \\in S_n} [A(\u03c3)]^2$$",
        "answer": "The closed-form expression for the sum is:\n$$T_n = \\frac{n! (n+1)^2 (n+2)}{12}$$\n\n---\n\n### Derivation\n\n**1. Expand the Square**\n\nFirst, we expand the term `[A(\u03c3)]^2` inside the summation:\n$$[A(\u03c3)]^2 = \\left( \\sum_{i=1}^{n} (-1)^{i-1} \u03c3(i) \\right) \\left( \\sum_{j=1}^{n} (-1)^{j-1} \u03c3(j) \\right) = \\sum_{i=1}^{n} \\sum_{j=1}^{n} (-1)^{i+j-2} \u03c3(i)\u03c3(j)$$\nSince `(-1)^{-2} = 1`, this simplifies to:\n$$[A(\u03c3)]^2 = \\sum_{i,j=1}^{n} (-1)^{i+j} \u03c3(i)\u03c3(j)$$\nWe can separate the sum into two parts: terms where `i = j` and terms where `i \u2260 j`.\n$$[A(\u03c3)]^2 = \\sum_{i=1}^{n} (-1)^{2i} \u03c3(i)^2 + \\sum_{i \\neq j} (-1)^{i+j} \u03c3(i)\u03c3(j) = \\sum_{i=1}^{n} \u03c3(i)^2 + \\sum_{i \\neq j} (-1)^{i+j} \u03c3(i)\u03c3(j)$$\n\n**2. Sum Over All Permutations**\n\nNow, we sum `[A(\u03c3)]^2` over all `\u03c3 \u2208 S_n`. By swapping the order of summation, we get:\n$$T_n = \\sum_{\u03c3 \\in S_n} \\left( \\sum_{i=1}^{n} \u03c3(i)^2 + \\sum_{i \\neq j} (-1)^{i+j} \u03c3(i)\u03c3(j) \\right)$$\n$$T_n = \\sum_{i=1}^{n} \\left( \\sum_{\u03c3 \\in S_n} \u03c3(i)^2 \\right) + \\sum_{i \\neq j} (-1)^{i+j} \\left( \\sum_{\u03c3 \\in S_n} \u03c3(i)\u03c3(j) \\right)$$\n\n**3. Evaluate the Inner Sums by Symmetry**\n\nLet's evaluate the two types of inner sums over `S_n`.\n\n*   **Case 1: `C_1 = \u03a3_{\u03c3 \u2208 S_n} \u03c3(i)^2`**\n    For any fixed position `i`, the value `\u03c3(i)` can be any integer `k \u2208 {1, ..., n}`. The number of permutations for which `\u03c3(i) = k` is `(n-1)!`. Therefore, the sum `C_1` is independent of `i`.\n    $$C_1 = \\sum_{k=1}^{n} (n-1)! \\cdot k^2 = (n-1)! \\sum_{k=1}^{n} k^2$$\n    Using the well-known formula for the sum of the first `n` squares, `\u03a3 k^2 = n(n+1)(2n+1)/6`:\n    $$C_1 = (n-1)! \\frac{n(n+1)(2n+1)}{6} = n! \\frac{(n+1)(2n+1)}{6}$$\n\n*   **Case 2: `C_2 = \u03a3_{\u03c3 \u2208 S_n} \u03c3(i)\u03c3(j)` for `i \u2260 j`**\n    For any pair of distinct positions `(i, j)`, the values `(\u03c3(i), \u03c3(j))` can be any pair of distinct integers `(k, l)` from `{1, ..., n}`. The number of permutations for which `\u03c3(i) = k` and `\u03c3(j) = l` is `(n-2)!`. Thus, `C_2` is independent of the choice of `i` and `j`.\n    $$C_2 = \\sum_{k \\neq l} (n-2)! \\cdot kl = (n-2)! \\sum_{k \\neq l} kl$$\n    The sum `\u03a3_{k\u2260l} kl` can be found using the identity `(\u03a3k)^2 = \u03a3k^2 + \u03a3_{k\u2260l}kl`.\n    $$\\sum_{k \\neq l} kl = \\left(\\sum_{k=1}^{n} k\\right)^2 - \\left(\\sum_{k=1}^{n} k^2\\right)$$\n    Using the formulas `\u03a3 k = n(n+1)/2` and `\u03a3 k^2 = n(n+1)(2n+1)/6`:\n    $$\\sum_{k \\neq l} kl = \\left(\\frac{n(n+1)}{2}\\right)^2 - \\frac{n(n+1)(2n+1)}{6} = \\frac{n^2(n+1)^2}{4} - \\frac{n(n+1)(2n+1)}{6}$$\n    Factoring out `n(n+1)/12`, we get:\n    $$\\sum_{k \\neq l} kl = \\frac{n(n+1)}{12} [3n(n+1) - 2(2n+1)] = \\frac{n(n+1)}{12} [3n^2 - n - 2]$$\n    The quadratic `3n^2 - n - 2` factors into `(3n+2)(n-1)`.\n    $$\\sum_{k \\neq l} kl = \\frac{n(n+1)(n-1)(3n+2)}{12}$$\n    Now we find `C_2`:\n    $$C_2 = (n-2)! \\frac{n(n-1)(n+1)(3n+2)}{12} = n! \\frac{(n+1)(3n+2)}{12}$$\n\n**4. Assemble the Final Sum**\n\nWe now substitute `C_1` and `C_2` back into the expression for `T_n`:\n$$T_n = \\left(\\sum_{i=1}^{n} 1\\right) C_1 + \\left(\\sum_{i \\neq j} (-1)^{i+j}\\right) C_2 = n \\cdot C_1 + \\left(\\sum_{i \\neq j} (-1)^{i+j}\\right) C_2$$\n\nWe need to evaluate the coefficient `\u03a3_{i\u2260j} (-1)^{i+j}`. We can use a similar identity as before:\n$$\\sum_{i \\neq j} (-1)^{i+j} = \\left(\\sum_{k=1}^{n} (-1)^k\\right)^2 - \\sum_{k=1}^{n} ((-1)^k)^2$$\nSince `n` is **odd**:\n$$\\sum_{k=1}^{n} (-1)^k = (-1+1)+(-1+1)+\\dots+(-1) = -1$$\nAnd for any `n`:\n$$\\sum_{k=1}^{n} ((-1)^k)^2 = \\sum_{k=1}^{n} 1 = n$$\nSo the coefficient is `(-1)^2 - n = 1 - n`.\n\nFinally, we substitute all parts into the equation for `T_n`:\n$$T_n = n \\left( n! \\frac{(n+1)(2n+1)}{6} \\right) + (1-n) \\left( n! \\frac{(n+1)(3n+2)}{12} \\right)$$\nFactor out the common term `n!(n+1)/12`:\n$$T_n = \\frac{n!(n+1)}{12} [2n(2n+1) + (1-n)(3n+2)]$$\nNow, we simplify the expression in the brackets:\n$$[2n(2n+1) + (1-n)(3n+2)] = (4n^2 + 2n) + (-3n^2 - 2n + 3n + 2) = 4n^2 + 2n - 3n^2 + n + 2$$\n$$= n^2 + 3n + 2$$\nThis quadratic factors into `(n+1)(n+2)`. Substituting this back gives the final result:\n$$T_n = \\frac{n!(n+1)}{12} (n+1)(n+2)$$\n$$T_n = \\frac{n! (n+1)^2 (n+2)}{12}$$"
    },
    {
        "question": "### The Law of Cosines on a Median\n\nLet $\\triangle ABC$ be a triangle on the surface of a unit sphere. A triangle of this type, a **spherical triangle**, is formed by the intersection of three distinct great circles. The sides of the triangle are arcs of these great circles.\n\nLet the vertices of the triangle be the points $A, B$, and $C$. The side lengths opposite these vertices are denoted $a, b,$ and $c$, respectively. These side lengths are measured by the angle they subtend at the center of the sphere, so they are expressed in radians. We can assume side lengths are in $(0, \\pi)$.\n\nLet $M$ be the midpoint of the side $c$ (the great circle arc connecting $A$ and $B$). The great circle arc connecting vertex $C$ to the point $M$ is the **spherical median** from $C$. Let the length of this median be $m_c$.\n\n**Goal:** Prove the following identity relating the lengths of the sides to the length of the median $m_c$:\n$$ \\cos(a) + \\cos(b) = 2 \\cos\\left(\\frac{c}{2}\\right) \\cos(m_c) $$",
        "answer": "The problem asks for a proof of an identity involving the side lengths of a spherical triangle and one of its medians. The key to the solution is a judicious application of the **spherical law of cosines** to the two smaller triangles created by the median.\n\nThe spherical law of cosines for a triangle with sides $x, y, z$ and angle $Z$ opposite side $z$ is:\n$$ \\cos(z) = \\cos(x)\\cos(y) + \\sin(x)\\sin(y)\\cos(Z) $$\n\n### Proof Strategy\n\n1.  The median $CM$ (with length $m_c$) divides the triangle $\\triangle ABC$ into two smaller triangles: $\\triangle AMC$ and $\\triangle BMC$.\n2.  We will apply the spherical law of cosines to both of these smaller triangles.\n3.  The crucial insight is to apply the law of cosines with respect to the angles at point $M$, the midpoint of side $c$. Let $\\angle CMA = \\theta$. Since $A, M, B$ lie on a great circle, the angle $\\angle CMB$ must be $\\pi - \\theta$. The property $\\cos(\\pi - \\theta) = -\\cos(\\theta)$ will allow for a key cancellation.\n\n### Step-by-Step Derivation\n\n1.  **Consider triangle $\\triangle AMC$.**\n    The sides of this triangle are:\n    -   $AC = b$\n    -   $AM = c/2$ (since $M$ is the midpoint of $c$)\n    -   $CM = m_c$\n\n    Let the angle at vertex $M$ within this triangle, $\\angle CMA$, be $\\theta$. Applying the spherical law of cosines to find the side opposite this angle ($AC = b$):\n    $$ \\cos(b) = \\cos(m_c) \\cos\\left(\\frac{c}{2}\\right) + \\sin(m_c) \\sin\\left(\\frac{c}{2}\\right) \\cos(\\theta) \\quad (*)$$\n\n2.  **Consider triangle $\\triangle BMC$.**\n    The sides of this triangle are:\n    -   $BC = a$\n    -   $BM = c/2$\n    -   $CM = m_c$\n\n    The angle at vertex $M$ within this triangle, $\\angle CMB$, is supplementary to $\\theta$, so its measure is $\\pi - \\theta$. Applying the spherical law of cosines to find the side opposite this angle ($BC = a$):\n    $$ \\cos(a) = \\cos(m_c) \\cos\\left(\\frac{c}{2}\\right) + \\sin(m_c) \\sin\\left(\\frac{c}{2}\\right) \\cos(\\pi - \\theta) $$\n    Since $\\cos(\\pi - \\theta) = -\\cos(\\theta)$, this equation becomes:\n    $$ \\cos(a) = \\cos(m_c) \\cos\\left(\\frac{c}{2}\\right) - \\sin(m_c) \\sin\\left(\\frac{c}{2}\\right) \\cos(\\theta) \\quad (**) $$\n\n3.  **Combine the results.**\n    We now have two expressions, one for $\\cos(b)$ and one for $\\cos(a)$. The problem asks for an identity for the sum $\\cos(a) + \\cos(b)$. Let's add equation $(*)$ and equation $(**)$:\n\n    $$ \\cos(a) + \\cos(b) = \\left[ \\cos(m_c) \\cos\\left(\\frac{c}{2}\\right) - \\sin(m_c) \\sin\\left(\\frac{c}{2}\\right) \\cos(\\theta) \\right] + \\left[ \\cos(m_c) \\cos\\left(\\frac{c}{2}\\right) + \\sin(m_c) \\sin\\left(\\frac{c}{2}\\right) \\cos(\\theta) \\right] $$\n\n    The terms involving $\\sin(m_c)$, $\\sin(c/2)$, and $\\cos(\\theta)$ cancel each other out:\n    $$ \\cos(a) + \\cos(b) = \\cos(m_c) \\cos\\left(\\frac{c}{2}\\right) + \\cos(m_c) \\cos\\left(\\frac{c}{2}\\right) $$\n    $$ \\cos(a) + \\cos(b) = 2 \\cos\\left(\\frac{c}{2}\\right) \\cos(m_c) $$\n\nThis completes the proof. The identity is a direct consequence of applying the spherical law of cosines to the two triangles formed by the median and exploiting the supplementary-angle relationship at the median's foot. This result is the spherical analogue of Apollonius's theorem for planar triangles ($a^2 + b^2 = 2(m_c^2 + (c/2)^2)$)."
    },
    {
        "question": "### A Generalization of Euler's Partition Theorem\n\nLet $n$ be a positive integer. A **partition** of $n$ is a way of writing $n$ as a sum of positive integers, where the order of the addends (called **parts**) does not matter. For example, the partitions of 4 are:\n-   $4$\n-   $3 + 1$\n-   $2 + 2$\n-   $2 + 1 + 1$\n-   $1 + 1 + 1 + 1$\n\nLet $k$ be a fixed integer such that $k \\ge 2$. We define two distinct types of partitions of $n$:\n\n1.  Let $A_k(n)$ be the number of partitions of $n$ where none of the parts are divisible by $k$.\n2.  Let $B_k(n)$ be the number of partitions of $n$ where no part appears $k$ or more times.\n\nFor example, consider $n=4$ and $k=3$:\n-   To calculate $A_3(4)$, we count partitions of 4 whose parts are not divisible by 3. The allowed parts are $\\{1, 2, 4, 5, \\dots\\}$. The partitions are:\n    -   $4$\n    -   $2 + 2$\n    -   $2 + 1 + 1$\n    -   $1 + 1 + 1 + 1$\n    Thus, $A_3(4) = 4$.\n-   To calculate $B_3(4)$, we count partitions of 4 where no part appears 3 or more times. The partitions are:\n    -   $4$ (each part appears once)\n    -   $3 + 1$ (each part appears once)\n    -   $2 + 2$ (the part 2 appears twice)\n    -   $2 + 1 + 1$ (the part 2 appears once, the part 1 appears twice)\n    The partition $1+1+1+1$ is disallowed because the part 1 appears four times, which is $\\ge 3$.\n    Thus, $B_3(4) = 4$.\n\n**Goal:** Prove that for all positive integers $n$ and any integer $k \\ge 2$, the following identity holds:\n$$A_k(n) = B_k(n)$$",
        "answer": "The proof relies on the method of generating functions. We will construct the generating functions for the sequences $A_k(n)$ and $B_k(n)$ and show that these functions are identical. Let's define the generating functions $f_A(x)$ and $f_B(x)$ as follows, where we take $A_k(0) = B_k(0) = 1$ to represent the single empty partition of 0:\n$$f_A(x) = \\sum_{n=0}^{\\infty} A_k(n) x^n$$\n$$f_B(x) = \\sum_{n=0}^{\\infty} B_k(n) x^n$$\n\nOur goal is to show that $f_A(x) = f_B(x)$. If the generating functions are equal, then the coefficients of their power series expansions must be equal term-by-term, which implies $A_k(n) = B_k(n)$ for all $n$.\n\n### Step 1: Construct the generating function $f_A(x)$\n\n$A_k(n)$ counts the number of partitions of $n$ where no part is divisible by $k$. This means the parts must be chosen from the set $S = \\{m \\in \\mathbb{Z}^+ \\mid k \\nmid m\\}$.\n\nA generating function for partitions with parts from a set $S$ can be constructed as a product of series, where each term in the product corresponds to a possible part. For each part $m \\in S$, we can use it any number of times in a partition. This contributes a factor of $(1 + x^m + x^{2m} + x^{3m} + \\dots)$ to the generating function. This is a geometric series that sums to $\\frac{1}{1-x^m}$.\n\nTherefore, the generating function for $A_k(n)$ is the product of these terms for all $m \\in S$:\n$$f_A(x) = \\prod_{m \\in S} \\frac{1}{1-x^m} = \\prod_{m=1, \\, k \\nmid m}^{\\infty} \\frac{1}{1-x^m}$$\n\n### Step 2: Construct the generating function $f_B(x)$\n\n$B_k(n)$ counts the number of partitions of $n$ where no part appears $k$ or more times. The parts can be any positive integer $j \\in \\mathbb{Z}^+$.\n\nFor each possible part $j$, we can include it $0, 1, 2, \\dots, k-1$ times in the partition. The corresponding term in the generating function for the part $j$ is therefore $(1 + x^j + x^{2j} + \\dots + x^{(k-1)j})$.\n\nThe total generating function $f_B(x)$ is the product of these factors for all possible parts $j=1, 2, 3, \\dots$:\n$$f_B(x) = \\prod_{j=1}^{\\infty} (1 + x^j + x^{2j} + \\dots + x^{(k-1)j})$$\n\n### Step 3: Show that $f_A(x) = f_B(x)$\n\nLet's simplify the expression for $f_B(x)$. Each factor in the product is a finite geometric series:\n$$1 + x^j + x^{2j} + \\dots + x^{(k-1)j} = \\frac{1 - (x^j)^k}{1 - x^j} = \\frac{1 - x^{kj}}{1 - x^j}$$\n\nSubstituting this back into the expression for $f_B(x)$:\n$$f_B(x) = \\prod_{j=1}^{\\infty} \\frac{1 - x^{kj}}{1 - x^j}$$\n\nWe can separate the infinite product into a product over the numerator and a product over the denominator:\n$$f_B(x) = \\frac{\\prod_{j=1}^{\\infty} (1 - x^{kj})}{\\prod_{j=1}^{\\infty} (1 - x^j)}$$\n\nNow, let's analyze the product in the denominator. We can split the product $\\prod_{j=1}^{\\infty} (1 - x^j)$ into two parts: one where the index $j$ is a multiple of $k$, and one where it is not.\nLet $j=mk$ for terms where $j$ is a multiple of $k$, and let $j=m$ where $k \\nmid m$ for the other terms.\n$$\\prod_{j=1}^{\\infty} (1 - x^j) = \\left( \\prod_{m=1}^{\\infty} (1 - x^{km}) \\right) \\left( \\prod_{m=1, \\, k \\nmid m}^{\\infty} (1 - x^m) \\right)$$\n\nSubstituting this expanded form back into our expression for $f_B(x)$:\n$$f_B(x) = \\frac{\\prod_{j=1}^{\\infty} (1 - x^{kj})}{\\left( \\prod_{m=1}^{\\infty} (1 - x^{km}) \\right) \\left( \\prod_{m=1, \\, k \\nmid m}^{\\infty} (1 - x^m) \\right)}$$\n\nThe product in the numerator is $\\prod_{j=1}^{\\infty} (1 - x^{kj})$, which is identical to the first product in the denominator, $\\prod_{m=1}^{\\infty} (1 - x^{km})$. These two terms cancel out.\n\n$$f_B(x) = \\frac{1}{\\prod_{m=1, \\, k \\nmid m}^{\\infty} (1 - x^m)}$$\n\nThis simplifies to:\n$$f_B(x) = \\prod_{m=1, \\, k \\nmid m}^{\\infty} \\frac{1}{1 - x^m}$$\n\nThis final expression for $f_B(x)$ is identical to the expression we derived for $f_A(x)$.\n\n### Conclusion\n\nSince we have shown that $f_A(x) = f_B(x)$, their series coefficients must be equal. Therefore, for all $n \\ge 0$ and any integer $k \\ge 2$:\n$$A_k(n) = B_k(n)$$\nThis completes the proof. Note that Euler's partition theorem is the special case where $k=2$, which states that the number of partitions into distinct parts equals the number of partitions into odd parts."
    },
    {
        "question": "### Asymptotic Behavior of a Recurrent Sequence\n\nLet the sequence of real numbers $(x_n)_{n \\ge 0}$ be defined by the initial value $x_0 > 0$ and the recurrence relation:\n$$x_{n+1} = \\arctan(x_n) \\quad \\text{for } n \\ge 0$$\n\nIt can be shown that this sequence converges to zero. The question concerns the *rate* of this convergence.\n\n**Goal:** Determine the value of the limit $L$:\n$$ L = \\lim_{n \\to \\infty} n \\cdot x_n^2 $$\n\nProvide the exact value of $L$.",
        "answer": "The value of the limit is $L = \\frac{3}{2}$.\n\n### Derivation\n\nThe solution involves two main steps:\n1.  Proving that the sequence $(x_n)$ converges to 0.\n2.  Determining the asymptotic behavior of $x_n$ as $n \\to \\infty$ to evaluate the given limit.\n\n---\n\n#### Step 1: Prove Convergence to Zero\n\nFirst, we establish that $\\lim_{n \\to \\infty} x_n = 0$.\n\n1.  **Positivity:** Since $x_0 > 0$ and the function $f(x) = \\arctan(x)$ maps positive numbers to positive numbers, it follows by induction that $x_n > 0$ for all $n \\ge 0$. The sequence is bounded below by 0.\n\n2.  **Monotonicity:** For any $x > 0$, we have the well-known inequality $\\arctan(x) < x$. To see this, consider the function $g(x) = x - \\arctan(x)$. Its derivative is $g'(x) = 1 - \\frac{1}{1+x^2} = \\frac{x^2}{1+x^2}$, which is strictly positive for $x \\ne 0$. Since $g(0) = 0$, we have $g(x) > 0$ for all $x > 0$.\n    Applying this to our sequence, we get $x_{n+1} = \\arctan(x_n) < x_n$.\n    Thus, the sequence $(x_n)$ is strictly decreasing.\n\n3.  **Convergence:** Since $(x_n)$ is a strictly decreasing sequence that is bounded below by 0, by the Monotone Convergence Theorem, it must converge to a limit, say $l$. This limit must satisfy $l \\ge 0$.\n    By taking the limit of the recurrence relation, we get:\n    $$ \\lim_{n \\to \\infty} x_{n+1} = \\lim_{n \\to \\infty} \\arctan(x_n) $$\n    $$ l = \\arctan(l) $$\n    The only real number that satisfies the equation $l = \\arctan(l)$ is $l=0$. Therefore, $\\lim_{n \\to \\infty} x_n = 0$.\n\n---\n\n#### Step 2: Determine the Asymptotic Limit\n\nWe want to evaluate $L = \\lim_{n \\to \\infty} n \\cdot x_n^2$. This form suggests that $x_n^2$ is approximately proportional to $1/n$ for large $n$.\n\nA powerful tool for this type of problem is the **Stolz\u2013Ces\u00e0ro theorem**, which is a discrete version of L'H\u00f4pital's rule. The theorem states that if $(a_n)$ and $(b_n)$ are two sequences such that $(b_n)$ is strictly monotone and unbounded, then:\n$$ \\lim_{n \\to \\infty} \\frac{a_n}{b_n} = \\lim_{n \\to \\infty} \\frac{a_{n+1} - a_n}{b_{n+1} - b_n} $$\nprovided the limit on the right-hand side exists.\n\nLet's rearrange our desired limit into a form suitable for this theorem. We want to find $L = \\lim_{n \\to \\infty} \\frac{n}{1/x_n^2}$.\nLet $a_n = n$ and $b_n = 1/x_n^2$. Since $x_n \\to 0$, $b_n \\to \\infty$ monotonically. We can apply the Stolz\u2013Ces\u00e0ro theorem:\n$$ L = \\lim_{n \\to \\infty} \\frac{n}{1/x_n^2} = \\lim_{n \\to \\infty} \\frac{(n+1) - n}{(1/x_{n+1}^2) - (1/x_n^2)} = \\lim_{n \\to \\infty} \\frac{1}{(1/x_{n+1}^2) - (1/x_n^2)} $$\n\nThe problem now reduces to finding the limit of the denominator:\n$$ D = \\lim_{n \\to \\infty} \\left( \\frac{1}{x_{n+1}^2} - \\frac{1}{x_n^2} \\right) $$\n\nWe use the recurrence relation $x_{n+1} = \\arctan(x_n)$. Since $x_n \\to 0$, we can use the Taylor series expansion of $\\arctan(u)$ around $u=0$:\n$$ \\arctan(u) = u - \\frac{u^3}{3} + \\frac{u^5}{5} - \\dots $$\nLet $u = x_n$. Then:\n$$ x_{n+1} = x_n - \\frac{x_n^3}{3} + O(x_n^5) $$\nNow we compute $1/x_{n+1}^2$:\n$$ \\frac{1}{x_{n+1}^2} = \\frac{1}{\\left(x_n - \\frac{x_n^3}{3} + O(x_n^5)\\right)^2} = \\frac{1}{x_n^2 \\left(1 - \\frac{x_n^2}{3} + O(x_n^4)\\right)^2} $$\nUsing the binomial approximation $(1-z)^{-2} = 1 + 2z + 3z^2 + \\dots$, for small $z = \\frac{x_n^2}{3} - O(x_n^4)$:\n$$ \\frac{1}{x_{n+1}^2} = \\frac{1}{x_n^2} \\left( 1 + 2\\left(\\frac{x_n^2}{3}\\right) + O(x_n^4) \\right) = \\frac{1}{x_n^2} \\left( 1 + \\frac{2x_n^2}{3} + O(x_n^4) \\right) $$\n$$ \\frac{1}{x_{n+1}^2} = \\frac{1}{x_n^2} + \\frac{2}{3} + O(x_n^2) $$\nNow we can evaluate the limit of the difference:\n$$ D = \\lim_{n \\to \\infty} \\left( \\left(\\frac{1}{x_n^2} + \\frac{2}{3} + O(x_n^2)\\right) - \\frac{1}{x_n^2} \\right) = \\lim_{n \\to \\infty} \\left( \\frac{2}{3} + O(x_n^2) \\right) $$\nSince $x_n \\to 0$, the term $O(x_n^2)$ also goes to zero. Thus:\n$$ D = \\frac{2}{3} $$\nFinally, we substitute this back into our expression for $L$:\n$$ L = \\frac{1}{D} = \\frac{1}{2/3} = \\frac{3}{2} $$"
    },
    {
        "question": "### A Determinant of Pairwise GCDs\nLet $n$ be a positive integer. Let $A_n$ be the $n \\times n$ matrix whose entry in the $i$-th row and $j$-th column is given by the greatest common divisor of $i$ and $j$.\n$$ (A_n)_{ij} = \\gcd(i, j) \\quad \\text{for } 1 \\le i, j \\le n $$\nFor example, for $n=4$, the matrix is:\n$$ A_4 = \\begin{pmatrix} 1 & 1 & 1 & 1 \\\\ 1 & 2 & 1 & 2 \\\\ 1 & 1 & 3 & 1 \\\\ 1 & 2 & 1 & 4 \\end{pmatrix} $$\n**Goal:** Find a simplified, closed-form expression for the determinant of $A_n$.",
        "answer": "The determinant of the matrix $A_n$ is the product of the values of Euler's totient function from 1 to $n$.\n\n$$ \\det(A_n) = \\prod_{k=1}^n \\phi(k) $$\n\nHere, $\\phi(k)$ is Euler's totient function, which counts the number of positive integers up to $k$ that are relatively prime to $k$.\n\n### Derivation\nThe solution relies on finding a specific matrix factorization for $A_n$ of the form $LDL^T$, where $L$ is a unit lower triangular matrix and $D$ is a diagonal matrix. The determinant can then be easily computed from the determinants of these factors.\n\n**1. Using a Number-Theoretic Identity**\n\nA key identity involving Euler's totient function is Gauss's identity:\n$$ \\sum_{d|m} \\phi(d) = m $$\nThis states that the sum of $\\phi(d)$ over all positive divisors $d$ of an integer $m$ is equal to $m$ itself.\n\nWe can apply this identity to each entry of our matrix $A_n$. The entry $(A_n)_{ij}$ is $\\gcd(i, j)$. Setting $m = \\gcd(i, j)$, we have:\n$$ (A_n)_{ij} = \\gcd(i, j) = \\sum_{d|\\gcd(i,j)} \\phi(d) $$\nThe condition that $d$ divides $\\gcd(i, j)$ is equivalent to the condition that $d$ divides both $i$ and $j$. Thus, we can rewrite the entry as:\n$$ (A_n)_{ij} = \\sum_{d=1}^n \\mathbb{I}(d|i) \\cdot \\mathbb{I}(d|j) \\cdot \\phi(d) $$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function, which is 1 if the condition is true and 0 otherwise.\n\n**2. Constructing the Matrix Factorization**\n\nThis sum structure suggests a matrix product. Let's define two matrices:\n*   Let $L$ be an $n \\times n$ matrix with entries $L_{ik}$ defined as:\n    $$ L_{ik} = \\begin{cases} 1 & \\text{if } k|i \\\\ 0 & \\text{otherwise} \\end{cases} $$\n*   Let $D$ be an $n \\times n$ diagonal matrix with entries $D_{kk} = \\phi(k)$.\n\nNow, let's compute the $(i, j)$-th entry of the matrix product $LDL^T$.\n$$ (LDL^T)_{ij} = \\sum_{k=1}^n \\sum_{m=1}^n L_{ik} D_{km} (L^T)_{mj} $$\nSince $D$ is diagonal, $D_{km}=0$ unless $k=m$. The sum simplifies to:\n$$ (LDL^T)_{ij} = \\sum_{k=1}^n L_{ik} D_{kk} (L^T)_{kj} $$\nThe transpose element $(L^T)_{kj}$ is equal to $L_{jk}$. Substituting the definitions of $L$ and $D$:\n$$ (LDL^T)_{ij} = \\sum_{k=1}^n L_{ik} \\phi(k) L_{jk} $$\nThe term $L_{ik} \\phi(k) L_{jk}$ is non-zero only if $L_{ik}=1$ and $L_{jk}=1$, which from the definition of $L$ means $k|i$ and $k|j$. Therefore, the sum is over all $k$ that divide both $i$ and $j$:\n$$ (LDL^T)_{ij} = \\sum_{k|i \\text{ and } k|j} \\phi(k) = \\sum_{k|\\gcd(i,j)} \\phi(k) $$\nFrom Step 1, this is exactly the expression for $\\gcd(i, j)$. Thus, we have found the factorization:\n$$ A_n = LDL^T $$\n\n**3. Calculating the Determinant**\n\nUsing the property that $\\det(XYZ) = \\det(X)\\det(Y)\\det(Z)$, we have:\n$$ \\det(A_n) = \\det(L) \\det(D) \\det(L^T) $$\n\nLet's compute the determinant of each factor:\n*   **Determinant of D:** $D$ is a diagonal matrix, so its determinant is the product of its diagonal entries.\n    $$ \\det(D) = \\prod_{k=1}^n D_{kk} = \\prod_{k=1}^n \\phi(k) $$\n*   **Determinant of L:** The matrix $L$ is a lower triangular matrix because $L_{ik}=0$ if $k>i$ (since a number cannot be a divisor of a smaller number). The diagonal entries of $L$ are $L_{ii}$, which are equal to 1 for all $i$ since every integer divides itself. The determinant of a triangular matrix is the product of its diagonal entries.\n    $$ \\det(L) = \\prod_{i=1}^n L_{ii} = \\prod_{i=1}^n 1 = 1 $$\n*   **Determinant of L<sup>T</sup>:** The determinant of a matrix transpose is equal to the determinant of the original matrix.\n    $$ \\det(L^T) = \\det(L) = 1 $$\n\n**4. Final Result**\n\nSubstituting these determinants back into the equation for $\\det(A_n)$:\n$$ \\det(A_n) = 1 \\cdot \\left(\\prod_{k=1}^n \\phi(k)\\right) \\cdot 1 = \\prod_{k=1}^n \\phi(k) $$\nThis completes the derivation."
    },
    {
        "question": "### Counting Involutory Matrices over a Finite Field\n\nLet $\\mathbb{F}_q$ be a finite field with $q$ elements, where $q$ is a power of an odd prime. Let $M_2(\\mathbb{F}_q)$ denote the set of all $2 \\times 2$ matrices with entries in $\\mathbb{F}_q$.\n\nAn **involution** is a matrix $A \\in M_2(\\mathbb{F}_q)$ such that $A^2 = I$, where $I$ is the $2 \\times 2$ identity matrix.\n\n**Goal:** Find a simplified, closed-form formula, in terms of $q$, for the number of involutions in $M_2(\\mathbb{F}_q)$.",
        "answer": "The number of $2 \\times 2$ involutory matrices over $\\mathbb{F}_q$ (for odd $q$) is $q^2 + q + 2$.\n\nHere is the derivation of the solution:\n\nAn involutory matrix $A$ is a matrix that satisfies the polynomial equation $x^2 - 1 = 0$. This means that the minimal polynomial of $A$, denoted $m_A(x)$, must be a divisor of $x^2 - 1$.\n\nSince $q$ is a power of an odd prime, the characteristic of the field $\\mathbb{F}_q$ is not 2. Therefore, the polynomial $x^2 - 1$ has two distinct roots in $\\mathbb{F}_q$, which are $1$ and $-1$.\nThe divisors of $x^2 - 1 = (x-1)(x+1)$ are:\n1.  $x-1$\n2.  $x+1$\n3.  $(x-1)(x+1) = x^2 - 1$\n\nWe can classify all involutory matrices $A \\in M_2(\\mathbb{F}_q)$ based on their minimal polynomial.\n\n**Case 1: The minimal polynomial is $m_A(x) = x - 1$.**\nIf $m_A(x) = x-1$, then by definition, $A - I = 0$, which means $A = I$.\nThe identity matrix $I = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$ is indeed an involution since $I^2 = I$.\nThis gives exactly **one** such matrix.\n\n**Case 2: The minimal polynomial is $m_A(x) = x + 1$.**\nIf $m_A(x) = x+1$, then $A + I = 0$, which means $A = -I$.\nThe matrix $-I = \\begin{pmatrix} -1 & 0 \\\\ 0 & -1 \\end{pmatrix}$ is an involution since $(-I)^2 = I$.\nSince the characteristic of $\\mathbb{F}_q$ is odd, $1 \\neq -1$, so $I \\neq -I$.\nThis gives exactly **one** such matrix, distinct from the one in Case 1.\n\n**Case 3: The minimal polynomial is $m_A(x) = x^2 - 1$.**\nFor a $2 \\times 2$ matrix $A$, the degree of the minimal polynomial can be 1 or 2. If the minimal polynomial is of degree 2, it must be identical to the characteristic polynomial, $p_A(x) = \\det(xI - A)$.\n\nThe characteristic polynomial of a $2 \\times 2$ matrix is given by $p_A(x) = x^2 - \\text{tr}(A)x + \\det(A)$.\nIf $p_A(x) = m_A(x) = x^2 - 1`, we can equate the coefficients:\n-   $-\\text{tr}(A) = 0 \\implies \\text{tr}(A) = 0$\n-   $\\det(A) = -1$\n\nSo, this case corresponds to counting the number of matrices $A \\in M_2(\\mathbb{F}_q)$ that have a trace of 0 and a determinant of -1.\nLet $A = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$, where $a, b, c, d \\in \\mathbb{F}_q$.\n\nThe conditions are:\n1.  $\\text{tr}(A) = a+d = 0 \\implies d = -a$.\n2.  $\\det(A) = ad-bc = -1$.\n\nSubstituting $d = -a$ into the determinant condition gives:\n$a(-a) - bc = -1 \\implies -a^2 - bc = -1 \\implies a^2 + bc = 1$.\n\nWe need to count the number of triples $(a, b, c) \\in \\mathbb{F}_q^3$ satisfying $a^2 + bc = 1$. We can do this by considering all possible values for $a$.\n\nFor each choice of $a \\in \\mathbb{F}_q$, we need to count the number of pairs $(b, c) \\in \\mathbb{F}_q^2$ such that $bc = 1 - a^2$. Let $k = 1 - a^2$.\n\n-   **Subcase 3a: $1 - a^2 \\neq 0$.**\n    This occurs when $a^2 \\neq 1$, which means $a \\neq 1$ and $a \\neq -1$. Since $\\mathbb{F}_q$ has characteristic not equal to 2, there are $q-2$ such values of $a$.\n    For a fixed non-zero $k = 1 - a^2$, we need to find pairs $(b,c)$ such that $bc = k$.\n    For any non-zero choice of $b \\in \\mathbb{F}_q^*$ (of which there are $q-1$), the value of $c$ is uniquely determined as $c = k b^{-1}$. If $b=0$, there is no solution.\n    So, for each of the $q-2$ values of $a$, there are $q-1$ a pair of `(b,c)` solutions.\n    Number of matrices in this subcase: $(q-2)(q-1)$.\n\n-   **Subcase 3b: $1 - a^2 = 0$.**\n    This occurs when $a^2=1$, which means $a=1$ or $a=-1$. There are two such values of $a$.\n    For these values, we need to count pairs $(b,c)$ such that $bc = 0$.\n    This equation holds if $b=0$ or $c=0$ (or both).\n    If we choose $b=0$, $c$ can be any of the $q$ elements in $\\mathbb{F}_q$. This gives $q$ pairs of the form $(0, c)$.\n    If we choose $b \\neq 0$ (of which there are $q-1$ choices), then we must have $c=0$. This gives $q-1$ pairs of the form $(b, 0)$ with $b \\neq 0$.\n    The total number of pairs $(b,c)$ with $bc=0$ is $q + (q-1) = 2q-1$.\n    Since this applies to two values of $a$ (1 and -1), the number of matrices in this subcase is $2(2q-1)$.\n\nThe total number of matrices in Case 3 is the sum of the counts from the two subcases:\nNumber(Case 3) = $(q-2)(q-1) + 2(2q-1)$\n$= (q^2 - 3q + 2) + (4q - 2)$\n$= q^2 + q$.\n\n**Final Count:**\nThe total number of involutions is the sum of the counts from the three disjoint cases:\nTotal = (Case 1) + (Case 2) + (Case 3)\nTotal = $1 + 1 + (q^2+q)$\nTotal = $q^2 + q + 2$."
    },
    {
        "question": "### An Integral Connecting Pi, e, and the Sine Function\n\nEvaluate the following definite integral. The bounds of integration are from 0 to infinity.\n\n$$I = \\int_0^\\infty \\frac{\\sin x}{e^x - 1} \\,dx$$\n\n**Goal:** Provide the exact, closed-form value of the integral. The final answer may be expressed in terms of elementary functions and well-known mathematical constants (like $\\pi$ and $e$).",
        "answer": "The value of the integral is:\n$$ I = \\frac{\\pi\\coth(\\pi) - 1}{2} $$\nwhich can also be written as $\\frac{\\pi}{2} \\cdot \\frac{e^{2\\pi} + 1}{e^{2\\pi} - 1} - \\frac{1}{2}$.\n\n### Derivation of the Solution\n\nThe evaluation of this integral requires several steps, combining techniques from series expansions, integration, and Fourier analysis.\n\n**Step 1: Expand the integrand into a series**\n\nFor $x > 0$, we can express the term $\\frac{1}{e^x - 1}$ as a geometric series:\n$$ \\frac{1}{e^x - 1} = \\frac{e^{-x}}{1 - e^{-x}} = e^{-x} \\sum_{n=0}^{\\infty} (e^{-x})^n = \\sum_{n=0}^{\\infty} e^{-(n+1)x} $$\nBy re-indexing with $k = n+1$, this becomes:\n$$ \\frac{1}{e^x - 1} = \\sum_{k=1}^{\\infty} e^{-kx} $$\nSubstituting this back into the integral gives:\n$$ I = \\int_0^\\infty \\sin(x) \\left( \\sum_{k=1}^{\\infty} e^{-kx} \\right) dx $$\n\n**Step 2: Justify the interchange of integration and summation**\n\nWe can swap the order of integration and summation if the integral of the absolute values converges (by the Fubini-Tonelli theorem). Let's check this condition:\n$$ \\int_0^\\infty \\sum_{k=1}^{\\infty} | \\sin(x) e^{-kx} | \\,dx = \\int_0^\\infty |\\sin x| \\left( \\sum_{k=1}^{\\infty} e^{-kx} \\right) dx = \\int_0^\\infty \\frac{|\\sin x|}{e^x - 1} \\,dx $$\nThis integral is finite. Near $x=0$, the integrand behaves like $\\frac{x}{x} = 1$. For large $x$, the integrand is bounded by $\\frac{1}{e^x - 1}$, which is rapidly convergent. Since the condition is met, we can interchange the summation and integral:\n$$ I = \\sum_{k=1}^{\\infty} \\int_0^\\infty e^{-kx} \\sin(x) \\,dx $$\n\n**Step 3: Evaluate the inner integral**\n\nLet's evaluate the integral $J_k = \\int_0^\\infty e^{-kx} \\sin(x) \\,dx$. We can use complex numbers for a quick evaluation.\n$$ J_k = \\text{Im} \\left( \\int_0^\\infty e^{-kx} e^{ix} \\,dx \\right) = \\text{Im} \\left( \\int_0^\\infty e^{(-k+i)x} \\,dx \\right) $$\n$$ J_k = \\text{Im} \\left[ \\frac{e^{(-k+i)x}}{-k+i} \\right]_0^\\infty = \\text{Im} \\left( 0 - \\frac{1}{-k+i} \\right) = \\text{Im} \\left( \\frac{1}{k-i} \\right) $$\nTo find the imaginary part, we multiply the numerator and denominator by the conjugate:\n$$ J_k = \\text{Im} \\left( \\frac{k+i}{k^2+1} \\right) = \\frac{1}{k^2+1} $$\n\n**Step 4: Reduce the problem to a series summation**\n\nSubstituting this result back, the original integral is now the sum of a series:\n$$ I = \\sum_{k=1}^{\\infty} \\frac{1}{k^2+1} $$\nThis is a known, but non-trivial, series. Its sum can be found using various advanced methods, such as residue calculus or Fourier series. We will use the latter as it is more elementary.\n\n**Step 5: Summing the series using Fourier analysis**\n\nConsider the function $f(x) = \\cosh(ax)$ for a parameter $a > 0$. We will find its Fourier cosine series expansion on the interval $[-\\pi, \\pi]$.\nThe general form is $f(x) = \\frac{a_0}{2} + \\sum_{k=1}^\\infty a_k \\cos(kx)$.\nThe coefficients are calculated as follows:\n$$ a_0 = \\frac{1}{\\pi} \\int_{-\\pi}^\\pi \\cosh(ax) \\,dx = \\frac{1}{\\pi} \\left[ \\frac{\\sinh(ax)}{a} \\right]_{-\\pi}^\\pi = \\frac{2\\sinh(a\\pi)}{a\\pi} $$\n$$ a_k = \\frac{1}{\\pi} \\int_{-\\pi}^\\pi \\cosh(ax) \\cos(kx) \\,dx $$\nThis integral can be solved using integration by parts (twice) and yields:\n$$ a_k = \\frac{2a(-1)^k \\sinh(a\\pi)}{\\pi(a^2+k^2)} $$\nSo, the Fourier series for $\\cosh(ax)$ on $[-\\pi, \\pi]$ is:\n$$ \\cosh(ax) = \\frac{\\sinh(a\\pi)}{a\\pi} + \\sum_{k=1}^\\infty \\frac{2a(-1)^k \\sinh(a\\pi)}{\\pi(a^2+k^2)} \\cos(kx) $$\nTo find the sum of our series, we can evaluate this identity at a specific point. Let's choose $x = \\pi$. Since $\\cos(k\\pi) = (-1)^k$, we get:\n$$ \\cosh(a\\pi) = \\frac{\\sinh(a\\pi)}{a\\pi} + \\sum_{k=1}^\\infty \\frac{2a(-1)^k \\sinh(a\\pi)}{\\pi(a^2+k^2)} (-1)^k $$\n$$ \\cosh(a\\pi) = \\frac{\\sinh(a\\pi)}{a\\pi} + \\frac{2a\\sinh(a\\pi)}{\\pi} \\sum_{k=1}^\\infty \\frac{1}{a^2+k^2} $$\nWe can now solve for the sum. Assuming $a \\ne 0$, we divide by $\\sinh(a\\pi)$:\n$$ \\frac{\\cosh(a\\pi)}{\\sinh(a\\pi)} = \\frac{1}{a\\pi} + \\frac{2a}{\\pi} \\sum_{k=1}^\\infty \\frac{1}{a^2+k^2} $$\n$$ \\coth(a\\pi) = \\frac{1}{a\\pi} + \\frac{2a}{\\pi} \\sum_{k=1}^\\infty \\frac{1}{a^2+k^2} $$\nOur desired sum is for $a=1$. Let's substitute $a=1$:\n$$ \\coth(\\pi) = \\frac{1}{\\pi} + \\frac{2}{\\pi} \\sum_{k=1}^\\infty \\frac{1}{k^2+1} $$\nNow, we rearrange the equation to solve for the sum:\n$$ \\pi\\coth(\\pi) = 1 + 2 \\sum_{k=1}^\\infty \\frac{1}{k^2+1} $$\n$$ \\sum_{k=1}^\\infty \\frac{1}{k^2+1} = \\frac{\\pi\\coth(\\pi) - 1}{2} $$\n\n**Step 6: Final Answer**\n\nSince $I = \\sum_{k=1}^{\\infty} \\frac{1}{k^2+1}$, we have our final answer:\n$$ I = \\frac{\\pi\\coth(\\pi) - 1}{2} $$"
    },
    {
        "question": "### An Integral Involving the Arctangent Function\nEvaluate the following definite integral. The bounds of integration are from 0 to 1.\n$$ I = \\int_0^1 \\frac{\\arctan x}{x \\sqrt{1-x^2}} \\,dx $$\n**Goal:** Provide the exact, closed-form value of the integral. The final answer may be expressed in terms of elementary functions and well-known mathematical constants.",
        "answer": "The value of the integral is $\\frac{\\pi}{2}\\ln(1+\\sqrt{2})$.\n\nWe can solve this integral using the technique of differentiation under the integral sign, also known as Feynman's trick.\n\n#### Step 1: Parameterize the Integral\nLet's introduce a parameter $a$ into the arctangent function. Define a new function $I(a)$ as:\n$$ I(a) = \\int_0^1 \\frac{\\arctan(ax)}{x \\sqrt{1-x^2}} \\,dx $$\nOur goal is to find the value of $I(1)$. Note that $I(0) = \\int_0^1 0 \\,dx = 0$. This will serve as our boundary condition.\n\n#### Step 2: Differentiate Under the Integral Sign\nWe differentiate $I(a)$ with respect to the parameter $a$. Assuming we can interchange the derivative and the integral:\n$$ \\frac{dI}{da} = I'(a) = \\int_0^1 \\frac{\\partial}{\\partial a} \\left( \\frac{\\arctan(ax)}{x \\sqrt{1-x^2}} \\right) \\,dx $$\nThe derivative of $\\arctan(u)$ with respect to $u$ is $1/(1+u^2)$. By the chain rule, the derivative with respect to $a$ is:\n$$ \\frac{\\partial}{\\partial a} \\arctan(ax) = \\frac{x}{1+(ax)^2} = \\frac{x}{1+a^2x^2} $$\nSubstituting this into the expression for $I'(a)$:\n$$ I'(a) = \\int_0^1 \\frac{1}{x \\sqrt{1-x^2}} \\cdot \\frac{x}{1+a^2x^2} \\,dx = \\int_0^1 \\frac{dx}{(1+a^2x^2) \\sqrt{1-x^2}} $$\n\n#### Step 3: Evaluate the New Integral\nThe integral for $I'(a)$ is simpler. We can solve it with a trigonometric substitution. Let $x = \\sin\\theta$. Then $dx = \\cos\\theta \\,d\\theta$. The limits of integration change from $x=0 \\to 1$ to $\\theta=0 \\to \\pi/2$.\n$$ \\sqrt{1-x^2} = \\sqrt{1-\\sin^2\\theta} = \\cos\\theta $$\nSubstituting these into the integral for $I'(a)$:\n$$ I'(a) = \\int_0^{\\pi/2} \\frac{\\cos\\theta \\, d\\theta}{(1+a^2\\sin^2\\theta) \\cos\\theta} = \\int_0^{\\pi/2} \\frac{d\\theta}{1+a^2\\sin^2\\theta} $$\nThis is a standard integral. We can evaluate it by dividing the numerator and denominator by $\\cos^2\\theta$:\n$$ I'(a) = \\int_0^{\\pi/2} \\frac{\\sec^2\\theta \\, d\\theta}{\\sec^2\\theta + a^2\\tan^2\\theta} $$\nUsing the identity $\\sec^2\\theta = 1 + \\tan^2\\theta$:\n$$ I'(a) = \\int_0^{\\pi/2} \\frac{\\sec^2\\theta \\, d\\theta}{1+\\tan^2\\theta + a^2\\tan^2\\theta} = \\int_0^{\\pi/2} \\frac{\\sec^2\\theta \\, d\\theta}{1+(1+a^2)\\tan^2\\theta} $$\nNow, perform another substitution. Let $u = \\sqrt{1+a^2}\\tan\\theta$. Then $du = \\sqrt{1+a^2}\\sec^2\\theta \\,d\\theta$. The limits of integration change from $\\theta=0 \\to \\pi/2$ to $u=0 \\to \\infty$.\n$$ I'(a) = \\int_0^\\infty \\frac{1}{1+u^2} \\cdot \\frac{du}{\\sqrt{1+a^2}} = \\frac{1}{\\sqrt{1+a^2}} \\left[ \\arctan(u) \\right]_0^\\infty $$\n$$ I'(a) = \\frac{1}{\\sqrt{1+a^2}} \\left( \\frac{\\pi}{2} - 0 \\right) = \\frac{\\pi}{2\\sqrt{1+a^2}} $$\n\n#### Step 4: Integrate with Respect to the Parameter\nNow we have found a simple expression for the derivative of $I(a)$. To find $I(a)$, we integrate this expression with respect to $a$:\n$$ I(a) = \\int \\frac{\\pi}{2\\sqrt{1+a^2}} \\,da = \\frac{\\pi}{2} \\int \\frac{da}{\\sqrt{1+a^2}} $$\nThe integral of $1/\\sqrt{1+a^2}$ is a standard result: $\\text{arsinh}(a)$ or $\\ln(a+\\sqrt{1+a^2})$.\n$$ I(a) = \\frac{\\pi}{2} \\text{arsinh}(a) + C $$\nwhere $C$ is the constant of integration.\n\n#### Step 5: Determine the Constant of Integration\nWe use the boundary condition we established in Step 1: $I(0) = 0$.\n$$ I(0) = \\frac{\\pi}{2} \\text{arsinh}(0) + C = \\frac{\\pi}{2} \\cdot 0 + C = C $$\nSince $I(0) = 0$, we have $C=0$. Therefore, the expression for $I(a)$ is:\n$$ I(a) = \\frac{\\pi}{2} \\text{arsinh}(a) $$\n\n#### Step 6: Final Answer\nThe original integral is the value of $I(a)$ at $a=1$:\n$$ I = I(1) = \\frac{\\pi}{2} \\text{arsinh}(1) $$\nThe inverse hyperbolic sine can be expressed in terms of logarithms: $\\text{arsinh}(z) = \\ln(z+\\sqrt{z^2+1})$. For $z=1$:\n$$ \\text{arsinh}(1) = \\ln(1+\\sqrt{1^2+1}) = \\ln(1+\\sqrt{2}) $$\nSubstituting this back gives the final answer:\n$$ I = \\frac{\\pi}{2}\\ln(1+\\sqrt{2}) $$"
    },
    {
        "question": "### The Motzkin Polynomial: Non-negativity and Sums of Squares\n\nIn mathematics, a polynomial $P(x_1, \\dots, x_n)$ with real coefficients is called **positive semidefinite** (or non-negative) if $P(x_1, \\dots, x_n) \\ge 0$ for all real numbers $x_1, \\dots, x_n$.\n\nA simple way to prove a polynomial is positive semidefinite is to show that it can be written as a **sum of squares** of other polynomials. That is, $P = \\sum_{i=1}^k [Q_i(x_1, \\dots, x_n)]^2$ for some polynomials $Q_i$. It is clear that if a polynomial is a sum of squares, it must be positive semidefinite.\n\nThis problem concerns the **Motzkin polynomial**, defined as:\n$$M(x, y) = x^4 y^2 + x^2 y^4 - 3x^2 y^2 + 1$$\n\n**Goal:** Prove the following two properties of the Motzkin polynomial:\n1.  Prove that $M(x,y)$ is positive semidefinite.\n2.  Prove that $M(x,y)$ cannot be written as a sum of squares of polynomials with real coefficients.\n\nThis demonstrates that the converse of the statement \"a sum of squares is positive semidefinite\" is false.",
        "answer": "This problem is solved in two parts as requested.\n\n### Part 1: Proving $M(x,y)$ is Positive Semidefinite\n\nWe want to show that $M(x, y) = x^4 y^2 + x^2 y^4 - 3x^2 y^2 + 1 \\ge 0$ for all $x, y \\in \\mathbb{R}$.\n\nLet $u = x^2$ and $v = y^2$. Since $x$ and $y$ are real, we have $u \\ge 0$ and $v \\ge 0$. The expression for $M$ becomes a function of $u$ and $v$:\n$$f(u, v) = u^2 v + u v^2 - 3uv + 1$$\nOur goal is to show $f(u, v) \\ge 0$ for all $u, v \\ge 0$.\n\nWe can apply the Arithmetic Mean-Geometric Mean (AM-GM) inequality.\nIf we group the terms as $(u^2v + uv^2) - 3uv + 1$, it's not immediately obvious.\nLet's try a different approach by treating $f(u,v)$ as a polynomial in one variable, say $u$, with coefficients depending on $v$.\n$$f(u,v) = v u^2 + (v^2 - 3v)u + 1$$\nThis is a quadratic in $u$. Let's analyze it for non-negative $u$ and $v$.\n\n**Case 1: $v=0$.**\nIf $v=0$, the expression becomes $f(u, 0) = 1$. Since $1 \\ge 0$, the inequality holds.\n\n**Case 2: $v > 0$.**\nFor a fixed $v > 0$, the function $g(u) = v u^2 + (v^2 - 3v)u + 1$ is an upward-opening parabola. Its minimum value for $u \\in \\mathbb{R}$ occurs at the vertex, $u = -\\frac{v^2-3v}{2v} = \\frac{3-v}{2}$. We must consider whether this vertex occurs at $u \\ge 0$.\n\n**Subcase 2a: The vertex is at $u \\ge 0$.**\nThis occurs when $\\frac{3-v}{2} \\ge 0$, which means $3-v \\ge 0$, or $0 < v \\le 3$.\nThe minimum value of $f(u,v)$ for a fixed $v$ in this range is at $u = (3-v)/2$. Let's evaluate $f$ at this point:\n$$f\\left(\\frac{3-v}{2}, v\\right) = v\\left(\\frac{3-v}{2}\\right)^2 + (v^2 - 3v)\\left(\\frac{3-v}{2}\\right) + 1$$\n$$= v\\frac{(3-v)^2}{4} - v(3-v)\\frac{3-v}{2} + 1 = \\frac{v(3-v)^2}{4} - \\frac{2v(3-v)^2}{4} + 1$$\n$$= 1 - \\frac{v(3-v)^2}{4}$$\nWe need to show that this quantity is non-negative for $0 < v \\le 3$. This is equivalent to showing $4 \\ge v(3-v)^2$.\nLet $h(v) = v(3-v)^2 = v(9 - 6v + v^2) = v^3 - 6v^2 + 9v$.\nTo find the maximum of $h(v)$ on $(0, 3]$, we compute its derivative:\n$h'(v) = 3v^2 - 12v + 9 = 3(v^2 - 4v + 3) = 3(v-1)(v-3)$.\nThe critical points are $v=1$ and $v=3$.\n- For $v \\in (0, 1)$, $h'(v) > 0$, so $h(v)$ is increasing.\n- For $v \\in (1, 3)$, $h'(v) < 0$, so $h(v)$ is decreasing.\nThus, the maximum value of $h(v)$ on $(0, 3]$ occurs at $v=1$.\n$h(1) = 1(3-1)^2 = 4$.\nSince the maximum value of $v(3-v)^2$ on $(0, 3]$ is 4, we have $v(3-v)^2 \\le 4$. Therefore, $1 - \\frac{v(3-v)^2}{4} \\ge 0$.\n\n**Subcase 2b: The vertex is at $u < 0$.**\nThis occurs when $\\frac{3-v}{2} < 0$, which means $v > 3$.\nSince the parabola $g(u)$ opens upwards and its vertex is at a negative $u$ value, for the domain $u \\ge 0$, $g(u)$ is a strictly increasing function. Its minimum value on $u \\ge 0$ is therefore at $u=0$.\n$f(0,v) = v(0)^2 + (v^2 - 3v)(0) + 1 = 1$.\nSince $1 \\ge 0$, the inequality holds for this subcase as well.\n\nCombining all cases, we have shown that $M(x,y) \\ge 0$ for all real $x,y$.\n\n---\n\n### Part 2: Proving $M(x,y)$ is Not a Sum of Squares\n\nWe proceed by contradiction. Assume that $M(x,y)$ can be written as a sum of squares of real polynomials:\n$$M(x,y) = \\sum_{i=1}^k [Q_i(x,y)]^2$$\nThe degree of $M(x,y)$ is 6 (from the terms $x^4y^2$ and $x^2y^4$). This implies that the degree of each polynomial $Q_i(x,y)$ can be at most 3.\n\nLet's examine the monomials that can appear in the polynomials $Q_i$.\nLet $Q_i(x,y) = \\sum_{j,l} c_{ijl} x^j y^l$.\nThen $\\sum_i Q_i(x,y)^2 = \\sum_i (\\sum_{j,l} c_{ijl} x^j y^l)^2$.\n\nConsider the coefficients of certain monomials in the expansion of $\\sum_i Q_i^2$.\n- **Coefficient of $x^6$**: The only way to get an $x^6$ term is from squaring a term with $x^3$. The coefficient of $x^6$ in $\\sum_i Q_i^2$ is $\\sum_i (\\text{coeff of } x^3 \\text{ in } Q_i)^2$. The coefficient of $x^6$ in $M(x,y)$ is 0. Since we are summing squares of real numbers, this means the coefficient of $x^3$ in every $Q_i$ must be 0.\n- **Coefficient of $y^6$**: By the same logic, the coefficient of $y^3$ in every $Q_i$ must be 0.\n- **Coefficient of $x^4$**: This term can only arise from squaring an $x^2$ term. Its coefficient in $\\sum_i Q_i^2$ is $\\sum_i (\\text{coeff of } x^2 \\text{ in } Q_i)^2$. Since the coefficient of $x^4$ in $M(x,y)$ is 0, the coefficient of $x^2$ in every $Q_i$ must be 0.\n- **Coefficient of $y^4$**: Similarly, the coefficient of $y^2$ in every $Q_i$ must be 0.\n- **Coefficient of $x^2$**: This term can only arise from squaring an $x$ term. Its coefficient in $\\sum_i Q_i^2$ is $\\sum_i (\\text{coeff of } x \\text{ in } Q_i)^2$. Since the coefficient of $x^2$ in $M(x,y)$ is 0, the coefficient of $x$ in every $Q_i$ must be 0.\n- **Coefficient of $y^2$**: Similarly, the coefficient of $y$ in every $Q_i$ must be 0.\n\nBased on this, the only monomials that could possibly have non-zero coefficients in any $Q_i(x,y)$ are those that do not produce pure powers $x^{2j}$ or $y^{2l}$ when squared. The possible monomials are of the form $x^j y^l$ where $j,l>0$, plus the constant term.\nThe monomials remaining for consideration in the $Q_i$ (up to degree 3) are:\n$\\{1, xy, x^2y, xy^2\\}$. Any other monomial would either have degree greater than 3 or has already been ruled out. For instance, $x^3y$ is not possible, as its square gives $x^6y^2$, which is not in $M$.\n\nSo, each $Q_i$ must be of the form:\n$$Q_i(x,y) = a_i + b_i xy + c_i x^2 y + d_i xy^2$$\nwhere $a_i, b_i, c_i, d_i$ are real coefficients.\n\nLet's expand $\\sum_i Q_i(x,y)^2$:\n$$\\sum_i (a_i + b_i xy + c_i x^2 y + d_i xy^2)^2 = \\sum_i (a_i^2 + b_i^2 x^2y^2 + c_i^2 x^4y^2 + d_i^2 x^2y^4 + \\text{cross terms})$$\nThe cross terms are $2a_ib_i xy$, $2a_ic_i x^2y$, $2a_id_i xy^2$, $2b_ic_i x^3y^2$, $2b_id_i x^2y^3$, and $2c_id_i x^3y^3$.\n\nNow we compare the coefficients of the expansion with the coefficients of $M(x,y) = 1 - 3x^2y^2 + x^4y^2 + x^2y^4$.\n- **Constant term (1)**: $\\sum_i a_i^2 = 1$.\n- **$x^4y^2$**: $\\sum_i c_i^2 = 1$.\n- **$x^2y^4$**: $\\sum_i d_i^2 = 1$.\n- **$xy$**: The coefficient in the expansion is $\\sum_i 2a_ib_i$. $M(x,y)$ has no $xy$ term, so $\\sum_i a_ib_i = 0$.\n- **$x^2y^2$**: The coefficient comes only from squaring the $xy$ term in $Q_i$, since all other products of terms result in different monomials (e.g., $a_i \\cdot (\\text{no term}) = x^2y^2$, $c_i x^2y \\cdot (\\text{no term}) = x^2y^2$). The coefficient is $\\sum_i b_i^2$.\nSo, by comparing with $M(x,y)$, we must have:\n$$\\sum_{i=1}^k b_i^2 = -3$$\nHowever, the coefficients $b_i$ are real numbers. The square of any real number is non-negative. A sum of non-negative numbers cannot be negative. Thus, $\\sum_i b_i^2 \\ge 0$.\nThis is a contradiction.\n\nTherefore, our initial assumption must be false. $M(x,y)$ cannot be written as a sum of squares of polynomials with real coefficients."
    },
    {
        "question": "### The Sum of Angular Defects of a Convex Polyhedron\n\nLet $P$ be a convex polyhedron in three-dimensional Euclidean space $\\mathbb{R}^3$. A polyhedron is defined by its set of vertices, edges, and faces. Let $V, E,$ and $F$ be the number of vertices, edges, and faces of $P$, respectively.\n\nFor each vertex $v$ of the polyhedron, there are several faces that meet at $v$. The plane angles of these faces at the vertex $v$ are called the **face angles** at $v$. Let the sum of these face angles at a vertex $v$ be denoted by $\\theta_v$. For a convex polyhedron, this sum is always less than $2\\pi$.\n\nThe **angular defect** at the vertex $v$, denoted $D_v$, is defined as the difference between the angle of a flat plane ($2\\pi$ radians) and the sum of the face angles at that vertex.\n$$ D_v = 2\\pi - \\theta_v $$\n\n**Goal:** Prove that the sum of the angular defects over all vertices of the polyhedron, $\\sum D_v$, is determined entirely by the numbers of vertices, edges, and faces. Specifically, prove the following identity, known as Descartes' theorem:\n$$ \\sum_{v \\in \\text{Vertices}} D_v = 2\\pi (V - E + F) $$\n\n*Note: For a convex polyhedron, Euler's Polyhedron Formula states that $V - E + F = 2$. Combining this with the identity you are asked to prove shows that the total angular defect is always $4\\pi$. However, you should prove the identity relating the sum of defects to $V, E,$ and $F$ directly, without assuming Euler's formula.*",
        "answer": "The goal is to prove the identity $\\sum_{v} D_v = 2\\pi (V - E + F)$ for a convex polyhedron.\n\n1.  **Start with the definition of the total angular defect.**\n    The total angular defect is the sum of the individual defects at each vertex. Let this sum be $S_D$.\n    $$ S_D = \\sum_{v \\in \\text{Vertices}} D_v $$\n    Using the definition of the angular defect, $D_v = 2\\pi - \\theta_v$, we can rewrite the sum as:\n    $$ S_D = \\sum_{v \\in \\text{Vertices}} (2\\pi - \\theta_v) $$\n    We can split this summation into two parts:\n    $$ S_D = \\sum_{v \\in \\text{Vertices}} 2\\pi - \\sum_{v \\in \\text{Vertices}} \\theta_v $$\n    The first term is simply $2\\pi$ summed over all $V$ vertices, which equals $2\\pi V$.\n    $$ S_D = 2\\pi V - \\sum_{v \\in \\text{Vertices}} \\theta_v $$\n\n2.  **Analyze the sum of all face angles.**\n    The term $\\sum_{v} \\theta_v$ represents the sum of all face angles, summed over all vertices of the polyhedron. We can re-evaluate this sum by grouping the angles by face instead of by vertex. Let $\\alpha_f$ be the sum of the interior angles of a single face $f$. The sum of all face angles across all vertices is the same as the sum of all interior angles across all faces.\n    $$ \\sum_{v \\in \\text{Vertices}} \\theta_v = \\sum_{f \\in \\text{Faces}} \\alpha_f $$\n\n3.  **Use the formula for the sum of interior angles of a polygon.**\n    Let's say a particular face $f$ is a polygon with $n_f$ edges (and thus $n_f$ vertices). The formula for the sum of the interior angles of a polygon with $n_f$ sides is $(n_f - 2)\\pi$.\n    Therefore, $\\alpha_f = (n_f - 2)\\pi$.\n    Substituting this into our sum over faces:\n    $$ \\sum_{f \\in \\text{Faces}} \\alpha_f = \\sum_{f \\in \\text{Faces}} (n_f - 2)\\pi $$\n    We can split this sum:\n    $$ \\sum_{f \\in \\text{Faces}} n_f \\pi - \\sum_{f \\in \\text{Faces}} 2\\pi $$\n    The second term is $2\\pi$ summed over all $F$ faces, which is $2\\pi F$.\n    $$ \\sum_{f \\in \\text{Faces}} \\alpha_f = \\pi \\left(\\sum_{f \\in \\text{Faces}} n_f\\right) - 2\\pi F $$\n\n4.  **Relate the sum of sides of all faces to the number of edges.**\n    The term $\\sum_{f \\in \\text{Faces}} n_f$ is the sum of the number of edges of each face. Consider the edges of the polyhedron. Each edge is shared by exactly two faces. Therefore, if we sum the number of edges for every face, we will have counted each edge of the polyhedron exactly twice.\n    This means:\n    $$ \\sum_{f \\in \\text{Faces}} n_f = 2E $$\n    Substituting this result back into our expression for the sum of angles:\n    $$ \\sum_{f \\in \\text{Faces}} \\alpha_f = \\pi (2E) - 2\\pi F = 2\\pi E - 2\\pi F $$\n\n5.  **Combine the results to complete the proof.**\n    We now have an expression for the total sum of face angles:\n    $$ \\sum_{v \\in \\text{Vertices}} \\theta_v = 2\\pi E - 2\\pi F $$\n    Finally, we substitute this back into our original equation for the sum of angular defects, $S_D$:\n    $$ S_D = 2\\pi V - \\left( \\sum_{v \\in \\text{Vertices}} \\theta_v \\right) $$\n    $$ S_D = 2\\pi V - (2\\pi E - 2\\pi F) $$\n    $$ S_D = 2\\pi V - 2\\pi E + 2\\pi F $$\n    Factoring out $2\\pi$, we arrive at the desired identity:\n    $$ \\sum_{v \\in \\text{Vertices}} D_v = 2\\pi (V - E + F) $$\n    This concludes the proof."
    },
    {
        "question": "### The Non-Simplicity of a Group of Order 144\n\nIn abstract algebra, groups are fundamental structures. A group `G` is a set with an operation that satisfies certain axioms (closure, associativity, identity, and invertibility). A subgroup `H` of `G` is a subset of `G` that is also a group under the same operation.\n\nA subgroup `H` is called **normal** if for every element `g` in `G`, the set `gHg^{-1} = \\{ghg^{-1} \\mid h \\in H\\}` is equal to `H`.\n\nA group `G` is called **simple** if its only normal subgroups are the trivial subgroup (containing only the identity element) and the group `G` itself. Finite simple groups are considered the \"building blocks\" of all finite groups, in a way analogous to how prime numbers are the building blocks of integers. A central achievement of 20th-century mathematics was the complete classification of all finite simple groups.\n\nThis problem asks you to prove that a group of a particular order cannot be one of these fundamental blocks.\n\n**Goal:** Prove that no group of order 144 can be simple.\n\n*You may use standard theorems of finite group theory, such as Lagrange's theorem and the Sylow theorems, without proof.*",
        "answer": "The goal is to prove that any group `G` with order `|G| = 144` must contain a non-trivial, proper normal subgroup.\n\n**Step 1: Initial Analysis using Sylow's Theorems**\n\nFirst, we find the prime factorization of the order of `G`:\n`|G| = 144 = 144 = 16 \\times 9 = 2^4 \\times 3^2`.\n\nLet `n_p` denote the number of Sylow `p`-subgroups of `G`. According to Sylow's theorems:\n1. `n_p` must divide the index of the Sylow `p`-subgroup, which is `|G| / p^k`.\n2. `n_p \\equiv 1 \\pmod{p}`.\n3. All Sylow `p`-subgroups are conjugate to each other.\n\nIf `n_p = 1` for some prime `p`, the unique Sylow `p`-subgroup is normal in `G`. Since its order would be `p^k` (where `1 < p^k < 144`), this subgroup would be a non-trivial proper normal subgroup, and `G` would not be simple.\n\nLet's analyze `n_3`, the number of Sylow 3-subgroups. The order of each Sylow 3-subgroup is `3^2 = 9`.\n- `n_3` must divide `|G|/9 = 16`. The divisors of 16 are `{1, 2, 4, 8, 16}`.\n- `n_3 \\equiv 1 \\pmod{3}`.\n\nCombining these conditions, the possible values for `n_3` are `1`, `4`, or `16`. We examine each case, assuming `G` is simple and looking for a contradiction.\n\n- **Case 1: `n_3 = 1`**\nIf `n_3 = 1`, the Sylow 3-subgroup is unique and therefore normal. This subgroup has order 9, so it is a non-trivial proper normal subgroup. Thus, `G` is not simple.\n\n**Step 2: Ruling out `n_3 = 4`**\n\n- **Case 2: `n_3 = 4`**\nAssume `n_3 = 4`. Let `X` be the set of the four Sylow 3-subgroups of `G`. `G` acts on `X` by conjugation. This action induces a group homomorphism `\u03c6: G \\to S_X \\cong S_4`, where `S_4` is the symmetric group on 4 elements.\n\nThe kernel of this homomorphism, `ker(\u03c6)`, is a normal subgroup of `G`. Since we assume `G` is simple, `ker(\u03c6)` must be either the trivial subgroup `{e}` or the entire group `G`.\n\nThe action of `G` on its Sylow 3-subgroups by conjugation is transitive, so the image of `\u03c6` is a non-trivial subgroup of `S_4`. This means `ker(\u03c6)` cannot be `G`.\nTherefore, if `G` is simple, `ker(\u03c6) = \\{e\\}`. This implies that `\u03c6` is injective, and `G` must be isomorphic to a subgroup of `S_4`.\n\nHowever, `|G| = 144`, while `|S_4| = 4! = 24`. A group of order 144 cannot be a subgroup of a group of order 24. This is a contradiction. Thus, the case `n_3 = 4` is impossible for a simple group of order 144.\n\n**Step 3: Ruling out `n_3 = 16`**\n\n- **Case 3: `n_3 = 16`**\nNow we assume `n_3 = 16`. Let `P_1, P_2, \\dots, P_{16}` be the Sylow 3-subgroups, each of order 9. We consider the intersections of these subgroups.\n\n**Subcase 3a: Trivial intersections**\nSuppose `P_i \\cap P_j = \\{e\\}` for all `i \\ne j`. Each `P_i` contains `9 - 1 = 8` elements of order 3 or 9. Since the intersections are trivial, the total number of such elements is `16 \\times 8 = 128`.\n\nThe total number of elements in `G` is 144. The remaining elements are `144 - 128 = 16`. These 16 elements must include the identity element. The other 15 elements have orders that are powers of 2. `G` has at least one Sylow 2-subgroup of order `2^4 = 16`. These 16 elements perfectly account for one Sylow 2-subgroup. If there were more than one Sylow 2-subgroup, there would be more than 15 non-identity elements of order a power of 2. Therefore, there must be only one Sylow 2-subgroup (`n_2=1`). This unique Sylow 2-subgroup is normal, contradicting the assumption that `G` is simple.\n\n**Subcase 3b: Non-trivial intersection**\nFrom the argument above, we must have at least one pair of distinct Sylow 3-subgroups, say `P_1` and `P_2`, with a non-trivial intersection `D = P_1 \\cap P_2`.\nSince `D` is a subgroup of `P_1` (which has order 9), by Lagrange's theorem, `|D|` must be 3.\n\nGroups of order `p^2` (like `P_1` and `P_2` of order `3^2=9`) are abelian. In an abelian group, every subgroup is normal.\nSo, `D` is a normal subgroup of `P_1` and `D` is a normal subgroup of `P_2`.\n\nConsider the normalizer of `D` in `G`, denoted `N_G(D) = \\{g \\in G \\mid gDg^{-1} = D\\}`.\nSince `D` is normal in `P_1`, we have `P_1 \\subseteq N_G(D)`.\nSince `D` is normal in `P_2`, we have `P_2 \\subseteq N_G(D)`.\n\n`N_G(D)` is a subgroup of `G`, and it contains two distinct subgroups of order 9 (`P_1` and `P_2`). The order of `N_G(D)` must divide `|G| = 144`. Also, `|N_G(D)|` must be a multiple of the order of its subgroup `P_1`, so `|N_G(D)|` is a multiple of 9. As `N_G(D)` contains at least two distinct 3-subgroups `P_1` and `P_2`, its order must be strictly greater than 9.\nPossible values for `|N_G(D)|` are divisors of 144 that are multiples of 9, greater than 9: `{18, 36, 72, 144}`.\n\nNow, let `K = N_G(D)`. `P_1` and `P_2` are subgroups of `K` of order 9. Since `|K|` divides `144 = 2^4 \\cdot 3^2`, the highest power of 3 dividing `|K|` is at most `3^2=9`. Therefore, `P_1` and `P_2` are Sylow 3-subgroups of `K`.\n\nLet `n_3(K)` be the number of Sylow 3-subgroups of `K`. By Sylow's theorem applied to `K`, `n_3(K) \\equiv 1 \\pmod 3` and `n_3(K)` must divide `|K|/9`.\n\nLet's check the possible values for `|K|`:\n- If `|K| = 18`, `n_3(K)` must divide `18/9 = 2`. Possible values are 1, 2. But `n_3(K) \\equiv 1 \\pmod 3`, so this case is impossible (`1 \\ne 2`).\n- If `|K| = 36`, `n_3(K)` must divide `36/9 = 4`. Possible values are 1, 2, 4. Since `n_3(K) \\equiv 1 \\pmod 3`, `n_3(K)` must be 4. This is a possibility.\n- If `|K| = 72`, `n_3(K)` must divide `72/9 = 8`. `n_3(K) \\equiv 1 \\pmod 3` gives no solutions from the divisors of 8. Impossible.\n- If `|K| = 144`, then `K = N_G(D) = G`. This means `D` is a normal subgroup of `G`. Since `|D|=3`, this contradicts `G` being simple.\n\nThe only remaining possibility to check is `|K| = |N_G(D)| = 36`.\nLet `K = N_G(D)` be this subgroup of order 36. The index of `K` in `G` is `[G:K] = |G|/|K| = 144/36 = 4`.\nConsider the action of `G` on the set of left cosets of `K`. This action induces a homomorphism `\u03c8: G \\to S_4`.\nThe kernel `ker(\u03c8)` is a normal subgroup of `G`. Since `G` is assumed to be simple, `ker(\u03c8)` must be `{e}` or `G`.\nThe kernel of this action is the largest normal subgroup of `G` contained in `K`. So `ker(\u03c8) \\subseteq K`.\nSince `|K|=36`, `K \\ne G`, so `ker(\u03c8) \\ne G`.\nThis leaves `ker(\u03c8) = \\{e\\}`.\nThis implies `G` is isomorphic to a subgroup of `S_4`. But `|G|=144` and `|S_4|=24`, which is a contradiction.\n\n**Step 4: Conclusion**\n\nWe have shown that for every possible value of `n_3` (the number of Sylow 3-subgroups), a group `G` of order 144 must have a non-trivial proper normal subgroup.\n- If `n_3 = 1`, the Sylow 3-subgroup is normal.\n- If `n_3 = 4`, `G` must have a normal subgroup `ker(\u03c6)`.\n- If `n_3 = 16`, `G` must have a normal Sylow 2-subgroup or a normal subgroup arising from the analysis of subgroup normalizers.\n\nIn every case, the assumption that `G` is simple leads to a contradiction. Therefore, no group of order 144 can be simple."
    },
    {
        "question": "### An Integral Related to the Poisson Kernel\n\nLet $a$ be a real number such that $0 < |a| < 1$.\n\n**Goal:** Evaluate the following definite integral and provide its exact, closed-form value in terms of $a$.\n$$ I(a) = \\int_0^\\pi \\frac{x \\sin x}{1 - 2a \\cos x + a^2} \\,dx $$",
        "answer": "The solution to this problem involves expanding the denominator of the integrand into a Fourier cosine series, integrating term by term, and summing the resulting infinite series.\n\n**Step 1: Expand the integrand using the Poisson kernel**\n\nThe denominator of the integrand, $D(a, x) = 1 - 2a \\cos x + a^2$, is a well-known expression in the theory of Fourier series. For $|a| < 1$, the reciprocal can be expressed as:\n$$ \\frac{1}{1 - 2a \\cos x + a^2} = \\sum_{n=0}^\\infty (2 \\cos x)^n a^n \\text{ (not a power series in a)} $$\nA more useful form is the generating function for Chebyshev polynomials or the Poisson kernel for the unit disk. The standard identity for $|a|<1$ is:\n$$ \\frac{1 - a^2}{1 - 2a \\cos x + a^2} = 1 + 2 \\sum_{n=1}^\\infty a^n \\cos(nx) $$\nThis gives us the expansion:\n$$ \\frac{1}{1 - 2a \\cos x + a^2} = \\frac{1}{1 - a^2} \\left( 1 + 2 \\sum_{n=1}^\\infty a^n \\cos(nx) \\right) $$\nSince $|a|<1$, the series converges absolutely and uniformly for all $x \\in [0, \\pi]$. This allows us to substitute the series into the integral and integrate term by term.\n\n$$ I(a) = \\int_0^\\pi x \\sin x \\cdot \\frac{1}{1 - a^2} \\left( 1 + 2 \\sum_{n=1}^\\infty a^n \\cos(nx) \\right) \\,dx $$\n$$ I(a) = \\frac{1}{1 - a^2} \\left( \\int_0^\\pi x \\sin x \\,dx + 2 \\sum_{n=1}^\\infty a^n \\int_0^\\pi x \\sin x \\cos(nx) \\,dx \\right) $$\n\n**Step 2: Evaluate the integrals**\n\nLet's define the family of integrals we need to compute:\n$$ L_n = \\int_0^\\pi x \\sin x \\cos(nx) \\,dx \\quad \\text{for } n \\ge 0 $$\nFirst, for $n=0$:\n$$ L_0 = \\int_0^\\pi x \\sin x \\,dx $$\nUsing integration by parts ($\\int u \\,dv = uv - \\int v \\,du$) with $u=x, dv=\\sin x \\,dx$:\n$$ L_0 = [-x \\cos x]_0^\\pi - \\int_0^\\pi (-\\cos x) \\,dx = (-\\pi \\cos \\pi) - 0 + [\\sin x]_0^\\pi = \\pi $$\nFor $n \\ge 1$, we use the product-to-sum identity $\\sin A \\cos B = \\frac{1}{2}(\\sin(A+B) + \\sin(A-B))$:\n$$ L_n = \\frac{1}{2} \\int_0^\\pi x (\\sin((n+1)x) + \\sin((1-n)x)) \\,dx = \\frac{1}{2} \\int_0^\\pi x (\\sin((n+1)x) - \\sin((n-1)x)) \\,dx $$\nWe need to evaluate the helper integral $J_k = \\int_0^\\pi x \\sin(kx) \\,dx$ for an integer $k \\ge 1$. Using integration by parts with $u=x, dv=\\sin(kx) \\,dx$:\n$$ J_k = \\left[x \\frac{-\\cos(kx)}{k}\\right]_0^\\pi - \\int_0^\\pi \\frac{-\\cos(kx)}{k} \\,dx = -\\frac{\\pi \\cos(k\\pi)}{k} + \\left[\\frac{\\sin(kx)}{k^2}\\right]_0^\\pi = -\\frac{\\pi (-1)^k}{k} $$\nNow we can evaluate $L_n$.\nFor $n=1$:\n$$ L_1 = \\frac{1}{2} \\int_0^\\pi x (\\sin(2x) - \\sin(0)) \\,dx = \\frac{1}{2} J_2 = \\frac{1}{2} \\left(-\\frac{\\pi (-1)^2}{2}\\right) = -\\frac{\\pi}{4} $$\nFor $n \\ge 2$:\n$$ L_n = \\frac{1}{2} (J_{n+1} - J_{n-1}) = \\frac{1}{2} \\left( -\\frac{\\pi (-1)^{n+1}}{n+1} - \\left(-\\frac{\\pi (-1)^{n-1}}{n-1}\\right) \\right) $$\n$$ L_n = \\frac{\\pi}{2} \\left( \\frac{(-1)^n}{n-1} - \\frac{(-1)^{n+1}}{n+1} \\right) = \\frac{\\pi}{2} (-1)^n \\left( \\frac{1}{n-1} + \\frac{1}{n+1} \\right) = \\frac{\\pi}{2} (-1)^n \\frac{2n}{n^2-1} = \\frac{\\pi n (-1)^n}{n^2-1} $$\n\n**Step 3: Sum the resulting series**\n\nNow we substitute these integral values back into the expression for $I(a)$:\n$$ I(a) = \\frac{1}{1 - a^2} \\left( L_0 + 2 \\sum_{n=1}^\\infty a^n L_n \\right) $$\n$$ I(a) = \\frac{1}{1 - a^2} \\left( \\pi + 2 \\left( a L_1 + \\sum_{n=2}^\\infty a^n L_n \\right) \\right) $$\n$$ I(a) = \\frac{\\pi}{1 - a^2} \\left( 1 + 2 \\left( a \\left(-\\frac{1}{4}\\right) + \\sum_{n=2}^\\infty a^n \\frac{n (-1)^n}{n^2-1} \\right) \\right) $$\n$$ I(a) = \\frac{\\pi}{1 - a^2} \\left( 1 - \\frac{a}{2} + 2 \\sum_{n=2}^\\infty \\frac{n (-a)^n}{n^2-1} \\right) $$\nLet's evaluate the sum $S = \\sum_{n=2}^\\infty \\frac{n (-a)^n}{n^2-1}$. Using partial fraction decomposition:\n$$ \\frac{n}{n^2-1} = \\frac{n}{(n-1)(n+1)} = \\frac{1}{2} \\left( \\frac{1}{n-1} + \\frac{1}{n+1} \\right) $$\n$$ S = \\frac{1}{2} \\sum_{n=2}^\\infty (-a)^n \\left( \\frac{1}{n-1} + \\frac{1}{n+1} \\right) = \\frac{1}{2} \\left( \\sum_{n=2}^\\infty \\frac{(-a)^n}{n-1} + \\sum_{n=2}^\\infty \\frac{(-a)^n}{n+1} \\right) $$\nFor the first sum, let $k = n-1$:\n$$ \\sum_{n=2}^\\infty \\frac{(-a)^n}{n-1} = \\sum_{k=1}^\\infty \\frac{(-a)^{k+1}}{k} = -a \\sum_{k=1}^\\infty \\frac{(-a)^k}{k} = -a (-\\ln(1+a)) = a \\ln(1+a) $$\nFor the second sum, let $k = n+1$:\n$$ \\sum_{n=2}^\\infty \\frac{(-a)^n}{n+1} = \\sum_{k=3}^\\infty \\frac{(-a)^{k-1}}{k} = -\\frac{1}{a} \\sum_{k=3}^\\infty \\frac{(-a)^k}{k} $$\nThe series for $\\ln(1+a)$ is $\\sum_{k=1}^\\infty \\frac{(-1)^{k-1}a^k}{k} = a - \\frac{a^2}{2} + \\frac{a^3}{3} - \\dots$. Thus, $\\sum_{k=1}^\\infty \\frac{(-a)^k}{k} = -\\ln(1+a)$.\n$$ \\sum_{k=3}^\\infty \\frac{(-a)^k}{k} = \\left(\\sum_{k=1}^\\infty \\frac{(-a)^k}{k}\\right) - (-a) - \\frac{(-a)^2}{2} = -\\ln(1+a) + a - \\frac{a^2}{2} $$\nSo the second sum is:\n$$ -\\frac{1}{a} \\left(-\\ln(1+a) + a - \\frac{a^2}{2}\\right) = \\frac{\\ln(1+a)}{a} - 1 + \\frac{a}{2} $$\nCombining these two sums:\n$$ S = \\frac{1}{2} \\left( a \\ln(1+a) + \\frac{\\ln(1+a)}{a} - 1 + \\frac{a}{2} \\right) $$\n\n**Step 4: Combine all terms and simplify**\n\nNow we substitute the value of the sum S back into the expression for $I(a)$:\n$$ I(a) = \\frac{\\pi}{1 - a^2} \\left( 1 - \\frac{a}{2} + 2S \\right) $$\n$$ I(a) = \\frac{\\pi}{1 - a^2} \\left( 1 - \\frac{a}{2} + a \\ln(1+a) + \\frac{\\ln(1+a)}{a} - 1 + \\frac{a}{2} \\right) $$\n$$ I(a) = \\frac{\\pi}{1 - a^2} \\left( \\left(a + \\frac{1}{a}\\right) \\ln(1+a) \\right) $$\n$$ I(a) = \\frac{\\pi}{1 - a^2} \\left( \\frac{a^2+1}{a} \\ln(1+a) \\right) $$\nThis expression doesn't seem to simplify further. Let's re-check the $L_n$ formula.\n\nAh, there is a sign error in the $L_n$ derivation.\n$$ L_n = \\frac{\\pi}{2} \\left( \\frac{(-1)^{n+1}}{n+1} + \\frac{(-1)^{n-1}}{n-1} \\right) = \\frac{\\pi}{2} (-1)^{n-1} \\left( \\frac{1}{n-1} + \\frac{1}{n+1} \\right) = \\frac{\\pi n (-1)^{n-1}}{n^2-1} $$\nLet's restart Step 3 with the correct $L_n$.\nThe sum is inside $I(a) = \\frac{\\pi}{1 - a^2} \\left( 1 - \\frac{a}{2} + 2 \\sum_{n=2}^\\infty a^n \\frac{n (-1)^{n-1}}{n^2-1} \\right)$.\nThe sum is $S' = \\sum_{n=2}^\\infty \\frac{n (-1)^{n-1} a^n}{n^2-1} = -\\sum_{n=2}^\\infty \\frac{n (-a)^n}{n^2-1} = -S$.\nSo we need to compute $1 - a/2 - 2S$.\n$$ 1 - \\frac{a}{2} - 2S = 1 - \\frac{a}{2} - \\left( a \\ln(1+a) + \\frac{\\ln(1+a)}{a} - 1 + \\frac{a}{2} \\right) $$\n$$ = 1 - \\frac{a}{2} - a \\ln(1+a) - \\frac{\\ln(1+a)}{a} + 1 - \\frac{a}{2} = 2 - a - \\left(a + \\frac{1}{a}\\right) \\ln(1+a) $$\nThis seems overly complicated. There must be a simplification that was missed. Let's look at the sum differently.\n$$2 \\sum_{n=2}^\\infty a^n L_n = \\pi \\sum_{n=2}^\\infty a^n \\frac{n (-1)^{n-1}}{n^2-1} = -\\pi a \\sum_{n=2}^\\infty \\frac{n (-a)^{n-1}}{n^2-1} = -\\pi a \\frac{d}{da} \\sum_{n=2}^\\infty \\frac{(-a)^n}{n^2-1}$$\nThis is also complicated. Let's trust the algebraic path but be extremely careful.\n\nLet's re-sum the series $S$.\n$S = \\frac{1}{2} \\left( a \\ln(1+a) + \\frac{\\ln(1+a)}{a} - 1 + \\frac{a}{2} \\right)$.\n$I(a) = \\frac{\\pi}{1 - a^2} \\left( 1 - \\frac{a}{2} - 2S \\right)$.\n$1 - a/2 - 2S = 1 - a/2 - \\left( a\\ln(1+a) + \\frac{\\ln(1+a)}{a} - 1 + \\frac{a}{2} \\right) = 2 - a - \\left( a + \\frac{1}{a} \\right) \\ln(1+a)$.\n$I(a) = \\frac{\\pi}{1-a^2} \\left( 2-a - \\frac{a^2+1}{a} \\ln(1+a) \\right)$. This looks incorrect.\n\nThere must be a cancellation. Let's re-examine my partial fraction of $L_n$ summation.\nSum = $2 \\sum a^n L_n = 2aL_1 + 2\\sum_{n=2}^\\infty a^n L_n = 2a(-\\pi/4) + 2\\pi\\sum_{n=2}^\\infty a^n \\frac{n(-1)^{n-1}}{n^2-1}$\n$ = -\\frac{\\pi a}{2} - 2\\pi \\sum_{n=2}^\\infty \\frac{n(-a)^n}{n^2-1} = -\\frac{\\pi a}{2} - 2\\pi S$.\n$I(a) = \\frac{1}{1-a^2} \\left( \\pi - \\frac{\\pi a}{2} - 2\\pi S \\right) = \\frac{\\pi}{1-a^2} \\left( 1 - \\frac{a}{2} - 2S \\right)$.\n$1 - \\frac{a}{2} - 2S = 1 - \\frac{a}{2} - \\left( a\\ln(1+a) + \\frac{\\ln(1+a)}{a} - 1 + \\frac{a}{2} \\right)$\n$= 2 - a - \\left(a+\\frac{1}{a}\\right)\\ln(1+a)$.\nThis calculation seems correct. Let's rethink the problem from the top. Maybe there is a property I missed. Let $f(a) = I(a)$. Let's check my sanity check again.\nIf $a \\to 0$, $I(a) \\to \\pi$.\nMy result: $\\frac{\\pi}{1-a^2} \\left( 2-a - \\frac{a^2+1}{a} \\ln(1+a) \\right)$\n$\\ln(1+a) \\approx a - a^2/2 + a^3/3$.\n$\\frac{a^2+1}{a} \\ln(1+a) \\approx \\frac{a^2+1}{a} (a-a^2/2) = (a+1/a)(a-a^2/2) = a^2 - a^3/2 + 1 - a/2$.\nSo inside the parenthesis: $2-a - (1-a/2+a^2) = 1 - a/2 - a^2$.\n$I(a) \\approx \\pi (1+a^2)(1-a/2-a^2) \\approx \\pi(1-a/2)$. This is not $\\pi$. The calculation is wrong.\n\nLet's re-calculate $S$.\n$S = \\frac{1}{2} \\left( \\sum_{n=2}^\\infty \\frac{(-a)^n}{n-1} + \\sum_{n=2}^\\infty \\frac{(-a)^n}{n+1} \\right)$\nFirst sum: $a\\ln(1+a)$.\nSecond sum: $\\frac{\\ln(1+a)}{a}-1+\\frac{a}{2}$.\n$S = \\frac{1}{2} \\left( a\\ln(1+a) + \\frac{\\ln(1+a)}{a} - 1 + \\frac{a}{2} \\right) = \\frac{a^2+1}{2a}\\ln(1+a) - \\frac{1}{2} + \\frac{a}{4}$.\nNow, $1 - \\frac{a}{2} - 2S = 1 - \\frac{a}{2} - \\left( \\frac{a^2+1}{a}\\ln(1+a) - 1 + \\frac{a}{2} \\right)$\n$= 2 - a - \\frac{a^2+1}{a}\\ln(1+a)$. Still the same.\n\nWhat if we write $L_n$ as $\\frac{\\pi(-1)^{n-1}}{2}(\\frac{1}{n-1}+\\frac{1}{n+1})$?\n$2 \\sum_{n=2}^\\infty a^n L_n = \\pi \\sum_{n=2}^\\infty (-1)^{n-1} a^n (\\frac{1}{n-1}+\\frac{1}{n+1})$\n$= \\pi \\left( \\sum_{n=2}^\\infty \\frac{(-1)^{n-1}a^n}{n-1} + \\sum_{n=2}^\\infty \\frac{(-1)^{n-1}a^n}{n+1} \\right)$\nFirst sum: $k=n-1$. $\\sum_{k=1}^\\infty \\frac{(-1)^k a^{k+1}}{k} = -a \\sum \\frac{(-a)^k}{k} = -a(-\\ln(1+a)) = a\\ln(1+a)$.\nSecond sum: $k=n+1$. $\\sum_{k=3}^\\infty \\frac{(-1)^{k-2} a^{k-1}}{k} = \\sum \\frac{(-1)^k a^{k-1}}{k} = -\\frac{1}{a} \\sum \\frac{(-a)^k}{k}$.\nThe sum is from $k=3$.\n$-\\frac{1}{a}(\\sum_{k=1}^\\infty \\frac{(-a)^k}{k} - (-a) - \\frac{(-a)^2}{2}) = -\\frac{1}{a}(-\\ln(1+a)+a-a^2/2) = \\frac{\\ln(1+a)}{a}-1+a/2$.\nSo the sum term is $\\pi(a\\ln(1+a) + \\frac{\\ln(1+a)}{a}-1+a/2)$.\n$I(a) = \\frac{1}{1-a^2} \\left( \\pi + 2a(-\\pi/4) + \\pi(\\dots) \\right) = \\frac{\\pi}{1-a^2} \\left( 1-a/2 + a\\ln(1+a) + \\frac{\\ln(1+a)}{a}-1+a/2 \\right)$\n$= \\frac{\\pi}{1-a^2} \\left( (a+1/a)\\ln(1+a) \\right) = \\frac{\\pi}{1-a^2}\\frac{a^2+1}{a}\\ln(1+a)$.\nLet's re-run the sanity check on *this* result.\nFor $a\\to 0$, $\\ln(1+a) \\approx a-a^2/2$.\n$I(a) \\approx \\pi(1+a^2) \\frac{a^2+1}{a} (a-a^2/2) \\approx \\pi \\frac{(1+a^2)^2}{a}(a-a^2/2) = \\pi (1+2a^2)(1-a/2) \\approx \\pi(1-a/2)$. Still wrong.\n\nLet's look at the Poisson sum again.\n$\\frac{1}{1-2a\\cos x+a^2} = \\sum_{k=0}^\\infty U_k(\\cos x) (2a)^k$. No.\nThe identity is $\\frac{1}{1 - 2a\\cos x + a^2} = \\sum_{n=0}^\\infty a^n \\frac{\\sin((n+1)x)}{\\sin x}$.\n$I(a) = \\int_0^\\pi x \\sin x \\sum_{n=0}^\\infty a^n \\frac{\\sin((n+1)x)}{\\sin x} dx = \\sum_{n=0}^\\infty a^n \\int_0^\\pi x \\sin((n+1)x) dx = \\sum_{n=0}^\\infty a^n J_{n+1}$.\n$J_{n+1} = -\\frac{\\pi(-1)^{n+1}}{n+1} = \\frac{\\pi(-1)^n}{n+1}$.\n$I(a) = \\sum_{n=0}^\\infty a^n \\frac{\\pi(-1)^n}{n+1} = \\pi \\sum_{n=0}^\\infty \\frac{(-a)^n}{n+1}$.\nLet $k=n+1$.\n$I(a) = \\pi \\sum_{k=1}^\\infty \\frac{(-a)^{k-1}}{k} = -\\frac{\\pi}{a} \\sum_{k=1}^\\infty \\frac{(-a)^k}{k} = -\\frac{\\pi}{a} (-\\ln(1+a)) = \\frac{\\pi}{a} \\ln(1+a)$.\n\nThis line of reasoning is much simpler and gives a result that passes the sanity check:\n$a\\to 0$, $I(a) \\to \\frac{\\pi}{a}(a - a^2/2 + \\dots) \\to \\pi$.\nThe solution is correct. The first method with the cosine expansion must have a subtle error. The second method using the generating function for $\\sin((n+1)x)/\\sin x$ is much cleaner. I will write the final answer using the cleaner method.\n\nFinal check of generating function identity:\nLet $S = \\sum_{n=0}^\\infty a^n \\sin((n+1)x)$. This is $\\text{Im}(\\sum_{n=0}^\\infty a^n e^{i(n+1)x}) = \\text{Im}(e^{ix} \\sum_{n=0}^\\infty (ae^{ix})^n)$.\n$= \\text{Im}(e^{ix} \\frac{1}{1-ae^{ix}}) = \\text{Im}(\\frac{e^{ix}(1-ae^{-ix})}{(1-ae^{ix})(1-ae^{-ix})}) = \\text{Im}(\\frac{e^{ix}-a}{1-2a\\cos x + a^2})$.\nThe imaginary part is $\\frac{\\sin x}{1-2a\\cos x + a^2}$.\nSo $\\sum_{n=0}^\\infty a^n \\sin((n+1)x) = \\frac{\\sin x}{1-2a \\cos x + a^2}$. Correct.\nThus $\\frac{1}{1-2a \\cos x + a^2} = \\sum_{n=0}^\\infty a^n \\frac{\\sin((n+1)x)}{\\sin x}$. This is the correct expansion.\n\nFinal Answer:\n$$ I(a) = \\frac{\\pi}{a} \\ln(1+a) $$"
    },
    {
        "question": "### A Special Case of the Baker-Campbell-Hausdorff Formula\n\nFor scalars $x$ and $y$, the exponential function satisfies the identity $e^x e^y = e^{x+y}$. However, for square matrices, this property holds only if the matrices commute. For general non-commuting matrices, the Baker-Campbell-Hausdorff (BCH) formula provides a more complex expression for the product of matrix exponentials. This problem explores a special, but important, case where the full BCH formula simplifies to a clean, closed-form expression.\n\n**Definitions and Context**\n\n-   Let $A$ and $B$ be $n \\times n$ matrices with entries in the complex numbers $\\mathbb{C}$.\n-   The **matrix exponential** of a matrix $X$ is defined by the power series:\n    $$ e^X = \\sum_{k=0}^{\\infty} \\frac{X^k}{k!} = I + X + \\frac{X^2}{2!} + \\frac{X^3}{3!} + \\dots $$\n    This series converges for any square matrix $X$.\n-   The **commutator** of two matrices $X$ and $Y$ is defined as:\n    $$ [X, Y] = XY - YX $$\n\n**Problem Statement**\n\nLet $A$ and $B$ be two $n \\times n$ matrices with complex entries. Let $C$ be their commutator, $C = [A, B]$.\n\nAssume that the commutator $C$ commutes with both of the original matrices $A$ and $B$. That is, assume:\n$$ [A, C] = 0 \\quad \\text{and} \\quad [B, C] = 0 $$\n\n**Goal:** Prove the following matrix identity:\n$$ e^A e^B = e^{A+B} e^{\\frac{1}{2}[A, B]} $$",
        "answer": "The goal is to prove the identity $e^A e^B = e^{A+B} e^{\\frac{1}{2}C}$ under the conditions $[A, C] = 0$ and $[B, C] = 0$, where $C = [A, B]$. The proof proceeds by defining a one-parameter function and solving a matrix differential equation that it satisfies.\n\n### Step 1: Define an Auxiliary Function and its Derivative\n\nConsider the function $F: \\mathbb{R} \\to M_n(\\mathbb{C})$ defined as:\n$$ F(t) = e^{tA} e^{tB} $$\nOur ultimate goal is to find a closed-form expression for $F(1)$. We start by differentiating $F(t)$ with respect to $t$ using the product rule:\n$$ \\frac{d}{dt} F(t) = \\left(\\frac{d}{dt} e^{tA}\\right) e^{tB} + e^{tA} \\left(\\frac{d}{dt} e^{tB}\\right) $$\nThe derivative of a matrix exponential is $\\frac{d}{dt}e^{tX} = X e^{tX} = e^{tX} X$. Using this, we get:\n$$ F'(t) = A e^{tA} e^{tB} + e^{tA} (B e^{tB}) $$\nWe can factor out $F(t) = e^{tA}e^{tB}$ from the first term, but not the second. To handle the second term, we rewrite it as:\n$$ F'(t) = A (e^{tA} e^{tB}) + (e^{tA} B e^{-tA}) (e^{tA} e^{tB}) $$\n$$ F'(t) = \\left( A + e^{tA} B e^{-tA} \\right) F(t) $$\nThis is a matrix differential equation for $F(t)$.\n\n### Step 2: Simplify the Operator using the Commutator Conditions\n\nThe term $e^{tA} B e^{-tA}$ can be simplified using the adjoint representation of Lie theory. Its series expansion is given by:\n$$ e^{tA} B e^{-tA} = \\text{Ad}_{e^{tA}}(B) = e^{\\text{ad}_{tA}}(B) = \\sum_{k=0}^{\\infty} \\frac{t^k}{k!} (\\text{ad}_A)^k(B) $$\nwhere $(\\text{ad}_A)(B) = [A, B]$. Let's compute the first few terms:\n-   $(\\text{ad}_A)^0(B) = B$\n-   $(\\text{ad}_A)^1(B) = [A, B] = C$\n-   $(\\text{ad}_A)^2(B) = [A, [A, B]] = [A, C]$\n\nAccording to the problem's assumption, $[A, C] = 0$. This implies that all higher-order nested commutators are also zero: $(\\text{ad}_A)^k(B) = 0$ for all $k \\ge 2$. Therefore, the infinite series for $e^{tA} B e^{-tA}$ truncates after the first-order term:\n$$ e^{tA} B e^{-tA} = B + t[A, B] = B + tC $$\nSubstituting this back into the differential equation for $F(t)$:\n$$ F'(t) = (A + B + tC) F(t) $$\n\n### Step 3: Solve the Matrix Differential Equation\n\nWe have the linear first-order matrix differential equation $F'(t) = M(t)F(t)$ where $M(t) = A + B + tC$.\nThe solution to such an equation is not generally $F(t) = e^{\\int_0^t M(s) ds} F(0)$. This simple form holds only if the matrix $M(t)$ commutes with itself at different times, i.e., $[M(t_1), M(t_2)] = 0$ for all $t_1, t_2$. Let's check this condition.\n$$ [M(t_1), M(t_2)] = [A+B+t_1 C, A+B+t_2 C] $$\nSince $A, B, C$ are matrices, we expand the commutator using its bilinearity:\n$$ [A+B, A+B] + t_2[A+B, C] + t_1[C, A+B] + t_1 t_2 [C, C] $$\nThis simplifies to:\n$$ 0 + t_2[A+B, C] - t_1[A+B, C] + 0 = (t_2 - t_1)[A+B, C] $$\nFor this to be zero for all $t_1, t_2$, we need $[A+B, C] = 0$. Let's verify this using the problem's assumptions:\n$$ [A+B, C] = [A, C] + [B, C] = 0 + 0 = 0 $$\nThe condition holds. Thus, we can integrate the operator $M(t)$ to find the solution. The initial condition is $F(0) = e^{0A}e^{0B} = I \\cdot I = I$.\nThe solution is:\n$$ F(t) = \\exp\\left(\\int_0^t M(s) \\,ds\\right) F(0) = \\exp\\left(\\int_0^t (A+B+sC) \\,ds\\right) \\cdot I $$\n$$ F(t) = \\exp\\left( (A+B)t + \\frac{t^2}{2}C \\right) $$\n\n### Step 4: Obtain the Final Identity\n\nWe are interested in the case $t=1$:\n$$ F(1) = e^A e^B = \\exp\\left( A+B+\\frac{1}{2}C \\right) $$\nThe final step is to split the exponential on the right-hand side. The identity $e^{X+Y} = e^X e^Y$ holds if and only if $X$ and $Y$ commute. Let $X = A+B$ and $Y = \\frac{1}{2}C$. We must check if $[X, Y] = 0$.\n$$ [X, Y] = \\left[ A+B, \\frac{1}{2}C \\right] = \\frac{1}{2}[A+B, C] $$\nAs shown in Step 3, we already established that $[A+B, C] = 0$ from the given hypotheses. Thus, $X$ and $Y$ commute. We can therefore split the exponential:\n$$ \\exp\\left( (A+B) + \\frac{1}{2}C \\right) = e^{A+B} e^{\\frac{1}{2}C} $$\nSubstituting $C = [A, B]$, we arrive at the desired identity:\n$$ e^A e^B = e^{A+B} e^{\\frac{1}{2}[A, B]} $$\nThis completes the proof."
    },
    {
        "question": "### A Quartic Diophantine Equation\n\nA **Diophantine equation** is a polynomial equation for which only integer solutions are sought. This problem involves a specific equation with two variables where the goal is to find all integer points on the curve it defines.\n\nConsider the equation:\n$$ y^2 = x^4 + 2x^3 + 2x^2 + 2x + 5 $$\n\n**Goal:** Find all pairs of integers $(x, y)$ that satisfy this equation.",
        "answer": "The problem is to find all integer solutions $(x, y)$ to the equation $y^2 = x^4 + 2x^3 + 2x^2 + 2x + 5$.\n\nThe main strategy for solving this type of Diophantine equation is to bound the right-hand side, which is a polynomial in $x$, between two consecutive perfect squares. If a polynomial lies strictly between two consecutive squares for a certain range of $x$, it cannot be a square itself, thus eliminating that range from the search for solutions.\n\nLet $P(x) = x^4 + 2x^3 + 2x^2 + 2x + 5$. We are looking for integer values of $x$ for which $P(x)$ is a perfect square.\n\n**Step 1: Find a lower bound for $P(x)$ with a perfect square.**\n\nLet's try to express $P(x)$ in terms of a nearby polynomial square. A natural candidate would be the square of a quadratic polynomial whose first two terms match those of $P(x)$ after being squared.\nConsider the square of $(x^2 + x)$:\n$$ (x^2 + x)^2 = x^4 + 2x^3 + x^2 $$\nNow, let's compare $P(x)$ with this square:\n$$ P(x) - (x^2 + x)^2 = (x^4 + 2x^3 + 2x^2 + 2x + 5) - (x^4 + 2x^3 + x^2) $$\n$$ P(x) - (x^2 + x)^2 = x^2 + 2x + 5 $$\nWe can rewrite the resulting quadratic by completing the square:\n$$ x^2 + 2x + 5 = (x^2 + 2x + 1) + 4 = (x+1)^2 + 4 $$\nSince $(x+1)^2 \\ge 0$ for any integer $x$, we have $(x+1)^2 + 4 > 0$.\nTherefore, $P(x) - (x^2 + x)^2 > 0$ for all integers $x$. This gives us the strict inequality:\n$$ y^2 = P(x) > (x^2 + x)^2 $$\n\n**Step 2: Find an upper bound for $P(x)$ with the next consecutive perfect square.**\n\nThe next consecutive integer square after $(x^2 + x)^2$ is $(x^2 + x + 1)^2$. Let's compute this square:\n$$ (x^2 + x + 1)^2 = ((x^2 + x) + 1)^2 = (x^2 + x)^2 + 2(x^2 + x) + 1 $$\n$$ (x^2 + x + 1)^2 = (x^4 + 2x^3 + x^2) + (2x^2 + 2x) + 1 $$\n$$ (x^2 + x + 1)^2 = x^4 + 2x^3 + 3x^2 + 2x + 1 $$\nNow, let's compare $P(x)$ with this new square:\n$$ (x^2 + x + 1)^2 - P(x) = (x^4 + 2x^3 + 3x^2 + 2x + 1) - (x^4 + 2x^3 + 2x^2 + 2x + 5) $$\n$$ (x^2 + x + 1)^2 - P(x) = x^2 - 4 $$\n\n**Step 3: Combine the bounds to restrict the possible values of $x$.**\n\nFrom Step 1, we know $y^2 > (x^2 + x)^2$.\nIf we can show that $y^2 < (x^2 + x + 1)^2$ for a range of $x$, then for that range we have:\n$$ (x^2 + x)^2 < y^2 < (x^2 + x + 1)^2 $$\nThis expression means $y^2$ is strictly between two consecutive integer squares. Since there is no integer $y$ that can satisfy such a condition, there can be no solutions in that range of $x$.\n\nThe condition for this is $(x^2 + x + 1)^2 - P(x) > 0$, which simplifies to:\n$$ x^2 - 4 > 0 $$\nThis inequality holds when $x^2 > 4$, which is true for $x > 2$ or $x < -2$.\n\nSo, for any integer $x$ in the set $\\{\\dots, -4, -3\\} \\cup \\{3, 4, \\dots\\}$, there are no integer solutions $(x, y)$. This leaves only a small, finite number of cases for $x$ to check.\n\n**Step 4: Perform case analysis for the remaining values of $x$.**\n\nThe only integer values of $x$ not excluded by our analysis are $x \\in \\{-2, -1, 0, 1, 2\\}$. We test each one:\n\n*   **Case $x = -2$:**\n    $y^2 = (-2)^4 + 2(-2)^3 + 2(-2)^2 + 2(-2) + 5 = 16 - 16 + 8 - 4 + 5 = 9$\n    $y^2 = 9 \\implies y = \\pm 3$.\n    This gives two solutions: $(-2, 3)$ and $(-2, -3)$.\n\n*   **Case $x = -1$:**\n    $y^2 = (-1)^4 + 2(-1)^3 + 2(-1)^2 + 2(-1) + 5 = 1 - 2 + 2 - 2 + 5 = 4$\n    $y^2 = 4 \\implies y = \\pm 2$.\n    This gives two solutions: $(-1, 2)$ and $(-1, -2)$.\n\n*   **Case $x = 0$:**\n    $y^2 = (0)^4 + 2(0)^3 + 2(0)^2 + 2(0) + 5 = 5$\n    $y^2 = 5$ has no integer solutions for $y$.\n\n*   **Case $x = 1$:**\n    $y^2 = (1)^4 + 2(1)^3 + 2(1)^2 + 2(1) + 5 = 1 + 2 + 2 + 2 + 5 = 12$\n    $y^2 = 12$ has no integer solutions for $y$.\n\n*   **Case $x = 2$:**\n    $y^2 = (2)^4 + 2(2)^3 + 2(2)^2 + 2(2) + 5 = 16 + 16 + 8 + 4 + 5 = 49$\n    $y^2 = 49 \\implies y = \\pm 7$.\n    This gives two solutions: $(2, 7)$ and $(2, -7)$.\n\n**Conclusion**\n\nWe have exhausted all possible integer values for $x$. The only solutions occur when $x \\in \\{-2, -1, 2\\}$. The complete set of integer solutions $(x, y)$ is:\n$$ \\{(-2, 3), (-2, -3), (-1, 2), (-1, -2), (2, 7), (2, -7)\\} $$"
    },
    {
        "question": "### Norm of the Volterra Operator\n\n**Background and Definitions**\n\n-   Let $H$ be the Hilbert space $L^2[0, 1]$ of real-valued, square-integrable functions on the interval $[0, 1]$.\n-   The inner product on $H$ is defined as $\\langle f, g \\rangle = \\int_0^1 f(x)g(x) \\,dx$.\n-   The norm on $H$ is the one induced by the inner product: $\\|f\\|_2 = \\sqrt{\\langle f, f \\rangle}$.\n-   Consider the linear operator $T: H \\to H$ known as the **Volterra integration operator**, which is defined by:\n    $$(Tf)(x) = \\int_0^x f(y) \\,dy$$\n-   It can be shown that $T$ is a bounded operator, meaning there exists a constant $C$ such that $\\|Tf\\|_2 \\le C \\|f\\|_2$ for all $f \\in H$.\n-   The **operator norm** of $T$, denoted $\\|T\\|$, is the smallest such constant $C$. Equivalently, it is defined as:\n    $$\\|T\\| = \\sup_{\\|f\\|_2=1} \\|Tf\\|_2$$\n\n**Goal:** Determine the exact value of $\\|T\\|$.",
        "answer": "The final answer is that the norm of the Volterra operator $T$ on $L^2[0,1]$ is **$2/\\pi$**.\n\nHere is the detailed derivation.\n\n### Strategy\n\nThe strategy is to relate the norm of $T$ to the spectrum of the related operator $S = T^*T$, where $T^*$ is the Hilbert adjoint of $T$. For any bounded operator $T$ on a Hilbert space, we have the identity $\\|T\\|^2 = \\|T^*T\\|$. Since $T^*T$ is self-adjoint (and in this case, compact and positive), its norm is equal to its largest eigenvalue. Our goal is therefore to find the largest eigenvalue of the operator $S = T^*T$.\n\n### Step 1: Find the Adjoint Operator $T^*$\n\nThe adjoint operator $T^*$ is defined by the relation $\\langle Tf, g \\rangle = \\langle f, T^*g \\rangle$ for all $f, g \\in H$. Let's compute the left-hand side:\n$$ \\langle Tf, g \\rangle = \\int_0^1 (Tf)(x) g(x) \\,dx = \\int_0^1 \\left( \\int_0^x f(y) \\,dy \\right) g(x) \\,dx $$\nThis is an integral over the triangle in the $xy$-plane defined by $0 \\le y \\le x \\le 1$. We can change the order of integration (by Fubini's theorem) to be over the same region but with $y$ as the outer variable: $0 \\le y \\le 1$ and $y \\le x \\le 1$.\n$$ \\langle Tf, g \\rangle = \\int_0^1 \\int_y^1 g(x) f(y) \\,dx \\,dy = \\int_0^1 f(y) \\left( \\int_y^1 g(x) \\,dx \\right) \\,dy $$\nThis expression has the form $\\langle f, T^*g \\rangle = \\int_0^1 f(y) (T^*g)(y) \\,dy$. By comparing the two forms, we can identify the adjoint operator:\n$$ (T^*g)(y) = \\int_y^1 g(x) \\,dx $$\n\n### Step 2: Formulate the Eigenvalue Problem for $S = T^*T$\n\nNow we construct the operator $S = T^*T$ and set up its eigenvalue problem, $Sf = \\lambda f$, for some eigenvalue $\\lambda$ and eigenfunction $f$.\n$$ (Sf)(y) = (T^*Tf)(y) = \\int_y^1 (Tf)(x) \\,dx = \\int_y^1 \\left( \\int_0^x f(z) \\,dz \\right) \\,dx $$\nThe eigenvalue equation is:\n$$ \\lambda f(y) = \\int_y^1 \\int_0^x f(z) \\,dz \\,dx $$\n\n### Step 3: Convert the Integral Equation to a Differential Equation\n\nThis is the key step. We can convert the integral equation into an ordinary differential equation (ODE) by repeated differentiation with respect to $y$, using the Fundamental Theorem of Calculus. (Note: The eigenfunctions of this compact operator can be shown to be sufficiently smooth for this to be valid.)\n\nDifferentiating the eigenvalue equation once with respect to $y$:\n$$ \\lambda f'(y) = -\\int_0^y f(z) \\,dz $$\nDifferentiating a second time with respect to $y$:\n$$ \\lambda f''(y) = -f(y) $$\nThis gives the familiar second-order ODE:\n$$ f''(y) + \\frac{1}{\\lambda} f(y) = 0 $$\n\n### Step 4: Determine the Boundary Conditions\n\nWe need two boundary conditions to solve this ODE. We can extract them from the integral equations derived in the previous step.\n1.  From $\\lambda f'(y) = -\\int_0^y f(z) \\,dz$, we can set $y=0$:\n    $$ \\lambda f'(0) = -\\int_0^0 f(z) \\,dz = 0 $$\n    Since we are seeking non-zero eigenvalues $\\lambda$, this implies our first boundary condition is $f'(0) = 0$.\n\n2.  From the original eigenvalue equation $\\lambda f(y) = \\int_y^1 \\int_0^x f(z) \\,dz \\,dx$, we can set $y=1$:\n    $$ \\lambda f(1) = \\int_1^1 \\left( \\int_0^x f(z) \\,dz \\right) \\,dx = 0 $$\n    Again, for $\\lambda \\ne 0$, this implies our second boundary condition is $f(1) = 0$.\n\n### Step 5: Solve the Boundary Value Problem\n\nWe now solve the boundary value problem:\n$$ f'' + k^2 f = 0, \\quad \\text{where } k^2 = 1/\\lambda $$\n$$ f'(0) = 0, \\quad f(1) = 0 $$\nThe general solution to the ODE is $f(y) = A \\cos(ky) + B \\sin(ky)$.\nThe derivative is $f'(y) = -Ak \\sin(ky) + Bk \\cos(ky)$.\n\nApplying the first boundary condition:\n$f'(0) = -Ak \\sin(0) + Bk \\cos(0) = Bk = 0$.\nSince we require $k \\ne 0$ (as $\\lambda \\ne 0$), this implies $B=0$. The solution must have the form $f(y) = A \\cos(ky)$.\n\nApplying the second boundary condition:\n$f(1) = A \\cos(k) = 0$.\nFor a non-trivial solution, we need $A \\ne 0$, which implies $\\cos(k) = 0$. This occurs when $k$ is an odd multiple of $\\pi/2$:\n$$ k_n = \\frac{(2n+1)\\pi}{2} \\quad \\text{for } n = 0, 1, 2, \\dots $$\n\n### Step 6: Find the Eigenvalues and the Norm of $T$\n\nThe eigenvalues $\\lambda$ are related to $k$ by $\\lambda = 1/k^2$. So the eigenvalues of $S = T^*T$ are:\n$$ \\lambda_n = \\frac{1}{k_n^2} = \\frac{4}{(2n+1)^2\\pi^2} $$\nThe set of eigenvalues is $\\{\\frac{4}{\\pi^2}, \\frac{4}{9\\pi^2}, \\frac{4}{25\\pi^2}, \\dots \\}$.\n\nThe norm of the self-adjoint operator $S=T^*T$ is its largest eigenvalue (in absolute value), which is the one corresponding to $n=0$:\n$$ \\|S\\| = \\|T^*T\\| = \\lambda_0 = \\frac{4}{\\pi^2} $$\nFinally, the norm of the operator $T$ is the square root of this value:\n$$ \\|T\\| = \\sqrt{\\|T^*T\\|} = \\sqrt{\\frac{4}{\\pi^2}} = \\frac{2}{\\pi} $$"
    },
    {
        "question": "### The Maximum Number of Edges in a Triangle-Free Graph\n\n**Definitions and Context**\n\nIn graph theory, a **simple graph** $G = (V, E)$ consists of a set of vertices $V$ and a set of edges $E$. Each edge is a two-element subset of $V$, representing an undirected connection between two distinct vertices. We denote the number of vertices as $n = |V|$ and the number of edges as $m = |E|$.\n\nA **triangle** in a graph is a set of three distinct vertices $\\{u, v, w\\}$ such that $\\{u, v\\}$, $\\{v, w\\}$, and $\\{w, u\\}$ are all edges in $E$. A graph is called **triangle-free** if it contains no triangles.\n\nFor a vertex $v \\in V$, its **degree**, denoted $\\deg(v)$, is the number of edges connected to it.\n\nFor example, a cycle graph on 5 vertices ($C_5$) is triangle-free. It has $n=5$ vertices and $m=5$ edges. The formula below gives an upper bound of $\\lfloor 5^2/4 \\rfloor = \\lfloor 6.25 \\rfloor = 6$. A cycle on 6 vertices ($C_6$) has $n=6, m=6$, while the bound is $\\lfloor 6^2/4 \\rfloor = 9$.\n\nThis problem asks for a proof of the tightest possible upper bound on the number of edges a triangle-free graph can have, a result known as Mantel's Theorem (1907), which is a special case of Turan's Theorem.\n\n**Goal**\n\nLet $G=(V, E)$ be any simple graph with $n$ vertices. If $G$ is triangle-free, prove that the number of edges $m$ satisfies the following inequality:\n\n$$ m \\le \\left\\lfloor \\frac{n^2}{4} \\right\\rfloor $$",
        "answer": "This theorem can be proven in several ways. We will present a proof that uses a local property of triangle-free graphs and an application of the Cauchy-Schwarz inequality.\n\n**Step 1: A Local Property of Edges in a Triangle-Free Graph**\n\nLet $G=(V,E)$ be a triangle-free graph with $n$ vertices. Consider any edge $\\{u, v\\} \\in E$. Let $N(u)$ and $N(v)$ be the set of neighbors of $u$ and $v$, respectively. By definition, $\\deg(u) = |N(u)|$ and $\\deg(v) = |N(v)|$.\n\nSince the graph is triangle-free, there is no vertex $w$ that is adjacent to both $u$ and $v$. If such a $w$ existed, then $\\{u, v, w\\}$ would form a triangle. This directly implies that the neighbor sets are disjoint: $N(u) \\cap N(v) = \\emptyset$.\n\nThe union of the sets $\\{u\\}$, $\\{v\\}$, $N(u) \\setminus \\{v\\}$, and $N(v) \\setminus \\{u\\}$ is a collection of distinct vertices within $V$. Therefore, the total number of vertices in this union must be less than or equal to $n$.\n\n$|\\{u\\}| + |\\{v\\}| + |N(u) \\setminus \\{v\\}| + |N(v) \\setminus \\{u\\}| \\le n$\n$1 + 1 + (\\deg(u) - 1) + (\\deg(v) - 1) \\le n$\n$\\deg(u) + \\deg(v) \\le n$\n\nThis establishes a crucial local property: for any edge $\\{u, v\\}$ in a triangle-free graph, the sum of the degrees of its endpoints is at most $n$.\n\n**Step 2: Summing Over All Edges**\n\nWe now sum this inequality over all edges in the graph. Let $m = |E|$.\n\n$$ \\sum_{\\{u,v\\} \\in E} (\\deg(u) + \\deg(v)) \\le \\sum_{\\{u,v\\} \\in E} n = m \\cdot n $$\n\nThe sum on the left can be rewritten. When we sum over all edges, the degree of a specific vertex $w$, $\\deg(w)$, is included in the sum exactly once for each edge incident to $w$. Since there are $\\deg(w)$ such edges, the term $\\deg(w)$ appears $\\deg(w)$ times in the sum. Therefore, the sum is equivalent to the sum of the squares of the degrees of all vertices.\n\n$$ \\sum_{w \\in V} [\\deg(w)]^2 = \\sum_{\\{u,v\\} \\in E} (\\deg(u) + \\deg(v)) $$\n\nCombining these results, we get an inequality relating the sum of squared degrees to the number of edges and vertices:\n\n$$ \\sum_{w \\in V} [\\deg(w)]^2 \\le m \\cdot n $$\n\n**Step 3: Applying the Cauchy-Schwarz Inequality**\n\nThe \"handshaking lemma\" states that the sum of the degrees of all vertices is twice the number of edges: $\\sum_{w \\in V} \\deg(w) = 2m$.\n\nWe now use the Cauchy-Schwarz inequality. For two vectors $\\mathbf{x} = (x_1, \\dots, x_n)$ and $\\mathbf{y} = (y_1, \\dots, y_n)$, the inequality states $(\\sum x_i y_i)^2 \\le (\\sum x_i^2)(\\sum y_i^2)$. Let $\\mathbf{x}$ be the vector of degrees $(\\deg(v_1), \\dots, \\deg(v_n))$ and $\\mathbf{y}$ be the vector of all ones $(1, 1, \\dots, 1)$.\n\n$$ \\left( \\sum_{w \\in V} \\deg(w) \\cdot 1 \\right)^2 \\le \\left( \\sum_{w \\in V} [\\deg(w)]^2 \\right) \\left( \\sum_{w \\in V} 1^2 \\right) $$\n\nSubstituting the known quantities:\n- $\\sum \\deg(w) = 2m$\n- $\\sum 1^2 = n$\n\nThis gives:\n$$ (2m)^2 \\le \\left( \\sum_{w \\in V} [\\deg(w)]^2 \\right) \\cdot n $$\n\n**Step 4: Final Derivation**\n\nWe now have two key inequalities:\n1. $\\sum_{w \\in V} [\\deg(w)]^2 \\le mn$\n2. $(2m)^2 \\le n \\cdot \\sum_{w \\in V} [\\deg(w)]^2$\n\nSubstitute the first inequality into the second:\n$$ (2m)^2 \\le n \\cdot (mn) $$\n$$ 4m^2 \\le m n^2 $$\n\nIf $m=0$, the inequality $0 \\le \\lfloor n^2/4 \\rfloor$ is trivially true. If $m > 0$, we can divide by $m$:\n$$ 4m \\le n^2 $$\n$$ m \\le \\frac{n^2}{4} $$\n\nSince $m$ must be an integer, we can take the floor of the right-hand side:\n$$ m \\le \\left\\lfloor \\frac{n^2}{4} \\right\\rfloor $$\n\nThis completes the proof.\n\n**Tightness of the Bound**\n\nThe bound is tight, meaning there exists a triangle-free graph for every $n$ that has exactly $\\lfloor n^2/4 \\rfloor$ edges. This is achieved by a **complete bipartite graph** $K_{\\lfloor n/2 \\rfloor, \\lceil n/2 \\rceil}$. In this graph, the vertex set $V$ is partitioned into two disjoint sets, $V_1$ and $V_2$, of size $\\lfloor n/2 \\rfloor$ and $\\lceil n/2 \\rceil$. Edges exist between every vertex in $V_1$ and every vertex in $V_2$, but there are no edges within $V_1$ or $V_2$.\n\nThis graph is triangle-free because any path of length two (e.g., $u-v-w$) must have its endpoints $u$ and $w$ in the same partition, so there is no edge $\\{u, w\\}$. The number of edges is $|V_1| \\cdot |V_2| = \\lfloor n/2 \\rfloor \\cdot \\lceil n/2 \\rceil$.\n\n- If $n=2k$, the sizes are $k$ and $k$, and the number of edges is $k \\cdot k = k^2 = n^2/4$.\n- If $n=2k+1$, the sizes are $k$ and $k+1$, and the number of edges is $k(k+1) = (n-1)/2 \\cdot (n+1)/2 = (n^2-1)/4 = \\lfloor n^2/4 \\rfloor$.\n\nThus, the bound is the best possible."
    },
    {
        "question": "### The Impossibility of the Canonical Commutation Relation\n\n**Background and Definitions**\n\nThis problem explores a fundamental result in operator theory, which has profound implications in the mathematical formulation of quantum mechanics.\n\n-   A **Banach algebra** is a complex Banach space (a complete normed vector space) that is also an associative algebra with a submultiplicative norm: $\\|xy\\| \\le \\|x\\|\\|y\\|$. The set of all bounded linear operators on a Hilbert space, denoted $\\mathcal{B}(H)$, is a primary example of a Banach algebra.\n-   The **spectrum** of an element $x$ in a Banach algebra, denoted $\\sigma(x)$, is the set of all complex numbers $\\lambda$ for which the element $x - \\lambda I$ is not invertible. Here, $I$ is the multiplicative identity element.\n-   The **commutator** of two operators $A$ and $B$ is defined as $[A, B] = AB - BA$.\n\nYou may assume the following three standard theorems from operator theory and Banach algebra theory without proof:\n\n1.  **Spectrum Properties:** For any element $x$ in a Banach algebra, its spectrum $\\sigma(x)$ is a non-empty and compact (i.e., closed and bounded) subset of the complex plane $\\mathbb{C}$.\n2.  **Spectral Shift Theorem:** For any element $x$ and any scalar $\\lambda \\in \\mathbb{C}$, the spectrum of $x + \\lambda I$ is given by $\\sigma(x + \\lambda I) = \\sigma(x) + \\lambda$, which is the set $\\{s + \\lambda \\mid s \\in \\sigma(x)\\}$.\n3.  **Cyclic Property of Spectra:** For any two elements $a$ and $b$ in a Banach algebra, the non-zero parts of their spectra are identical: $\\sigma(ab) \\setminus \\{0\\} = \\sigma(ba) \\setminus \\{0\\}$.\n\n**Problem Statement**\n\nLet $H$ be a Hilbert space over the complex numbers. Let $A$ and $B$ be bounded linear operators mapping $H$ to itself. Let $I$ be the identity operator on $H$.\n\n**Goal:** Prove that the following relation is impossible:\n$$ AB - BA = I $$",
        "answer": "This is a proof by contradiction. We assume that there exist two bounded linear operators, $A$ and $B$, on a Hilbert space $H$ that satisfy the canonical commutation relation $AB - BA = I$.\n\n1.  **Rewrite the Equation:**\n    Let's rewrite the given equation. We can express $AB$ in terms of $BA$:\n    $$ AB = BA + I $$\n\n2.  **Apply Spectral Theory:**\n    The core of the proof lies in analyzing the spectra of the operators involved. Let $\\sigma(X)$ denote the spectrum of an operator $X$. The set of bounded linear operators on a Hilbert space forms a Banach algebra, so we can apply the properties mentioned in the problem statement.\n\n3.  **Relate the Spectra of AB and BA:**\n    Using the cyclic property of spectra (Assumed Property 3), we know that:\n    $$ \\sigma(AB) \\setminus \\{0\\} = \\sigma(BA) \\setminus \\{0\\} $$\n\n4.  **Incorporate the Commutation Relation:**\n    Substituting $AB = BA + I$ into the spectral relation from the previous step, we get:\n    $$ \\sigma(BA + I) \\setminus \\{0\\} = \\sigma(BA) \\setminus \\{0\\} $$\n\n5.  **Use the Spectral Shift Theorem:**\n    According to the spectral shift theorem (Assumed Property 2), we have $\\sigma(BA + I) = \\sigma(BA) + 1$. Let's denote the spectrum of $BA$ by $S$, so $S = \\sigma(BA)$. The relation becomes:\n    $$ (S + 1) \\setminus \\{0\\} = S \\setminus \\{0\\} $$\n    where $S+1 = \\{\\lambda+1 \\mid \\lambda \\in S\\}$.\n\n6.  **Analyze the Consequence of the Spectral Relation:**\n    This equality of sets has two important implications:\n    *   **Forward implication:** If $\\lambda \\in S$ and $\\lambda+1 \\neq 0$ (i.e., $\\lambda \\neq -1$), then $\\lambda+1$ must be in the set on the right, which means $\\lambda+1 \\in S \\setminus \\{0\\}$. This implies $\\lambda+1 \\in S$.\n    *   **Backward implication:** If $\\mu \\in S \\setminus \\{0\\}$, then $\\mu$ must be in the set on the left. This means $\\mu \\in (S+1) \\setminus \\{0\\}$, which implies $\\mu \\in S+1$. By definition, this means $\\mu-1 \\in S$.\n\n7.  **Use the Properties of the Spectrum to Find a Contradiction:**\n    From Assumed Property 1, we know that $S = \\sigma(BA)$ is a **non-empty** and **compact** (closed and bounded) subset of $\\mathbb{C}$.\n\n    *   Since $S$ is a non-empty compact set in $\\mathbb{C}$, there must exist an element $\\lambda_{max} \\in S$ that has the largest real part. That is, $\\text{Re}(\\lambda_{max}) \\ge \\text{Re}(\\lambda)$ for all $\\lambda \\in S$.\n\n    *   Now, let's use the forward implication from step 6 on this element $\\lambda_{max}$. If $\\lambda_{max} \\neq -1$, then $\\lambda_{max}+1$ must also be in $S$. However, $\\text{Re}(\\lambda_{max}+1) = \\text{Re}(\\lambda_{max}) + 1$, which is strictly greater than $\\text{Re}(\\lambda_{max})$. This contradicts the fact that $\\lambda_{max}$ has the largest real part in $S$.\n\n    *   The only way to avoid this contradiction is if the condition \"$\\lambda_{max} \\neq -1$\" is false. Therefore, it must be that $\\lambda_{max} = -1$. This means that for every element $\\lambda \\in S$, we must have $\\text{Re}(\\lambda) \\le -1$.\n\n    *   This conclusion has an important consequence: $0$ cannot be in the spectrum $S$.\n\n    *   Now we use the backward implication from step 6. Since $0 \\notin S$, the condition \"If $\\mu \\in S \\setminus \\{0\\}$\" simplifies to \"If $\\mu \\in S$\". So, for any $\\mu \\in S$, we must have $\\mu-1 \\in S$.\n\n    *   We know $S$ is non-empty. In fact, we deduced that $-1 \\in S$. Applying the rule we just found, since $-1 \\in S$, it follows that $-1-1 = -2 \\in S$. Since $-2 \\in S$, it follows that $-3 \\in S$. By induction, the infinite set of points $\\{-1, -2, -3, \\dots, -n, \\dots\\}$ must be a subset of $S$.\n\n    *   This infinite set is not bounded, as the points extend to negative infinity along the real axis. However, the spectrum $S$ must be a bounded set because it is compact. An unbounded set cannot be a subset of a bounded set.\n\n8.  **Conclusion:**\n    We have reached a contradiction. The initial assumption\u2014that bounded operators $A$ and $B$ satisfying $AB - BA = I$ exist\u2014must be false. Therefore, no such operators can exist."
    },
    {
        "question": "### Euler's Reflection Formula for the Gamma Function\n\n**Background and Definitions**\n\nThe Gamma function, denoted $\\Gamma(z)$, is a central function in mathematical analysis that extends the factorial function to complex numbers. For a complex number $z$ with a positive real part ($\\text{Re}(z) > 0$), it is defined by the convergent improper integral:\n\n$$ \\Gamma(z) = \\int_0^\\infty t^{z-1} e^{-t} \\,dt $$\n\nThis function can be analytically continued to the entire complex plane, except for simple poles at the non-positive integers ($0, -1, -2, \\dots$). It satisfies the property $\\Gamma(z+1) = z\\Gamma(z)$, which for positive integers implies $\\Gamma(n+1) = n!$.\n\nA fundamental identity connecting the Gamma function to trigonometry is known as Euler's reflection formula. This problem asks for a proof of this formula.\n\n**Goal**\n\nProve the following identity for all non-integer complex numbers $z$:\n\n$$ \\Gamma(z) \\Gamma(1-z) = \\frac{\\pi}{\\sin(\\pi z)} $$\n\n*You may establish the identity for a specific domain, such as $0 < \\text{Re}(z) < 1$, and then briefly justify its extension to all non-integer complex numbers.*",
        "answer": "The proof proceeds in two main stages. First, we express the product $\\Gamma(z)\\Gamma(1-z)$ as a single integral using a change of variables in a double integral. Second, we evaluate this resulting integral using methods from complex analysis.\n\n### Step 1: Transforming the Product into a Single Integral\n\nWe begin by considering the product for complex numbers $z$ in the strip $0 < \\text{Re}(z) < 1$. In this domain, both $\\Gamma(z)$ and $\\Gamma(1-z)$ are well-defined by their integral representations.\n\n1.  **Write the product as a double integral:**\n    Using the definition of the Gamma function, we have:\n    $$ \\Gamma(z)\\Gamma(1-z) = \\left( \\int_0^\\infty x^{z-1} e^{-x} \\,dx \\right) \\left( \\int_0^\\infty y^{(1-z)-1} e^{-y} \\,dy \\right) $$\n    $$ \\Gamma(z)\\Gamma(1-z) = \\int_0^\\infty \\int_0^\\infty x^{z-1} y^{-z} e^{-(x+y)} \\,dx \\,dy $$\n    This double integral is over the first quadrant of the $xy$-plane.\n\n2.  **Perform a change of variables:**\n    To simplify the integral, we introduce a change of variables. Let $u = x+y$ and $v = x/y$. This implies $x = yv$, so $u = yv+y = y(1+v)$.\n    From this we solve for $x$ and $y$ in terms of $u$ and $v$:\n    $$ y = \\frac{u}{1+v} \\quad \\text{and} \\quad x = \\frac{uv}{1+v} $$\n    The original domain of integration is $(x, y) \\in (0, \\infty) \\times (0, \\infty)$, which corresponds to $(u, v) \\in (0, \\infty) \\times (0, \\infty)$ in the new coordinate system.\n\n3.  **Calculate the Jacobian of the transformation:**\n    The Jacobian determinant of the transformation from $(u,v)$ to $(x,y)$ is:\n    $$ J = \\det \\begin{pmatrix} \\frac{\\partial x}{\\partial u} & \\frac{\\partial x}{\\partial v} \\\\ \\frac{\\partial y}{\\partial u} & \\frac{\\partial y}{\\partial v} \\end{pmatrix} = \\det \\begin{pmatrix} \\frac{v}{1+v} & \\frac{u}{(1+v)^2} \\\\ \\frac{1}{1+v} & -\\frac{u}{(1+v)^2} \\end{pmatrix} = \\frac{-uv}{(1+v)^3} - \\frac{u}{(1+v)^3} = \\frac{-u(v+1)}{(v+1)^3} = \\frac{-u}{(1+v)^2} $$\n    The differential area element transforms as $dx\\,dy = |J|\\,du\\,dv = \\frac{u}{(1+v)^2}\\,du\\,dv$.\n\n4.  **Substitute into the integral:**\n    We substitute the expressions for $x, y,$ and $dx\\,dy$ into the integral:\n    $$ \\Gamma(z)\\Gamma(1-z) = \\int_0^\\infty \\int_0^\\infty \\left(\\frac{uv}{1+v}\\right)^{z-1} \\left(\\frac{u}{1+v}\\right)^{-z} e^{-u} \\frac{u}{(1+v)^2} \\,du \\,dv $$\n    Now, simplify the integrand by combining terms with like bases:\n    -   Powers of $u$: $u^{z-1} \\cdot u^{-z} \\cdot u^1 = u^{z-1-z+1} = u^0 = 1$.\n    -   Powers of $v$: $v^{z-1}$.\n    -   Powers of $(1+v)$: $(1+v)^{-(z-1)} \\cdot (1+v)^{z} \\cdot (1+v)^{-2} = (1+v)^{-z+1+z-2} = (1+v)^{-1}$.\n    \n    The simplified integrand is $\\frac{v^{z-1}}{1+v} e^{-u}$. The double integral becomes:\n    $$ \\Gamma(z)\\Gamma(1-z) = \\int_0^\\infty \\int_0^\\infty \\frac{v^{z-1}}{1+v} e^{-u} \\,du \\,dv $$\n\n5.  **Separate the integrals:**\n    The integral is separable with respect to $u$ and $v$:\n    $$ \\Gamma(z)\\Gamma(1-z) = \\left( \\int_0^\\infty e^{-u} \\,du \\right) \\left( \\int_0^\\infty \\frac{v^{z-1}}{1+v} \\,dv \\right) $$\n    The first integral is standard: $\\int_0^\\infty e^{-u} \\,du = [-e^{-u}]_0^\\infty = 1$.\n    Therefore, we have reduced the problem to evaluating a single integral:\n    $$ \\Gamma(z)\\Gamma(1-z) = \\int_0^\\infty \\frac{v^{z-1}}{1+v} \\,dv $$\n\n### Step 2: Evaluating the Integral using Contour Integration\n\nWe evaluate the integral $I = \\int_0^\\infty \\frac{v^{z-1}}{1+v} \\,dv$ for $0 < \\text{Re}(z) < 1$.\n\n1.  **Set up the complex integral:**\n    Consider the complex function $f(w) = \\frac{w^{z-1}}{1+w}$. The term $w^{z-1}$ is multi-valued. We define a branch by taking the logarithm as $\\text{Log}(w) = \\ln|w| + i\\arg(w)$ with the argument in the range $0 \\le \\arg(w) < 2\\pi$. This places a branch cut along the positive real axis.\n\n2.  **Choose the contour:**\n    We use a \"keyhole\" contour $C$ that avoids the branch cut. The contour consists of four parts:\n    -   $L_1$: A line segment from $\\epsilon$ to $R$ just above the positive real axis (where $\\arg(w)=0$).\n    -   $C_R$: A large circle of radius $R$ traversed counter-clockwise.\n    -   $L_2$: A line segment from $R$ to $\\epsilon$ just below the positive real axis (where $\\arg(w) \\to 2\\pi$).\n    -   $C_\\epsilon$: A small circle of radius $\\epsilon$ around the origin, traversed clockwise.\n\n3.  **Apply the Residue Theorem:**\n    The function $f(w)$ has a single simple pole inside the contour at $w = -1 = e^{i\\pi}$. The residue at this pole is:\n    $$ \\text{Res}(f, -1) = \\lim_{w \\to -1} (w+1)f(w) = (-1)^{z-1} = (e^{i\\pi})^{z-1} = e^{i\\pi(z-1)} $$\n    By the Residue Theorem, the integral around the closed contour is:\n    $$ \\oint_C f(w) \\,dw = 2\\pi i \\cdot \\text{Res}(f, -1) = 2\\pi i e^{i\\pi(z-1)} $$\n\n4.  **Evaluate the integrals along the contour segments:**\n    -   **On $C_R$**: For large $|w|=R$, $|f(w)| \\approx \\frac{R^{\\text{Re}(z)-1}}{R} = R^{\\text{Re}(z)-2}$. Since $\\text{Re}(z) < 1$, the exponent is negative. The length of the path is $2\\pi R$. The integral's magnitude is bounded by $2\\pi R \\cdot R^{\\text{Re}(z)-2} = 2\\pi R^{\\text{Re}(z)-1}$, which vanishes as $R \\to \\infty$.\n    -   **On $C_\\epsilon$**: For small $|w|=\\epsilon$, $|f(w)| \\approx \\epsilon^{\\text{Re}(z)-1}$. The path length is $2\\pi \\epsilon$. The integral's magnitude is bounded by $2\\pi \\epsilon \\cdot \\epsilon^{\\text{Re}(z)-1} = 2\\pi \\epsilon^{\\text{Re}(z)}$, which vanishes as $\\epsilon \\to 0$ since $\\text{Re}(z) > 0$.\n    -   **On $L_1$**: As $\\epsilon \\to 0$ and $R \\to \\infty$, this integral becomes our target integral $I$. Here $w = v$ (with $\\arg(w)=0$), so $w^{z-1} = v^{z-1}$.\n        $$ \\int_{L_1} f(w) \\,dw \\to \\int_0^\\infty \\frac{v^{z-1}}{1+v} \\,dv = I $$\n    -   **On $L_2$**: The path is from $R$ to $\\epsilon$. Here $w=v$, but its argument is $2\\pi$. So, $w^{z-1} = e^{(z-1)(\\ln v + i2\\pi)} = v^{z-1}e^{i2\\pi (z-1)}$.\n        $$ \\int_{L_2} f(w) \\,dw \\to \\int_\\infty^0 \\frac{v^{z-1}e^{i2\\pi(z-1)}}{1+v} \\,dv = -e^{i2\\pi(z-1)} \\int_0^\\infty \\frac{v^{z-1}}{1+v} \\,dv = -e^{i2\\pi z} I $$\n        (since $e^{-i2\\pi}=1$).\n\n5.  **Combine the results:**\n    Summing the integrals over the four parts gives the total contour integral:\n    $$ \\oint_C f(w) \\,dw = I + 0 - e^{i2\\pi z}I + 0 = I(1 - e^{i2\\pi z}) $$\n    Equating this with the result from the Residue Theorem:\n    $$ I(1 - e^{i2\\pi z}) = 2\\pi i e^{i\\pi(z-1)} = -2\\pi i e^{i\\pi z} $$\n    Solving for $I$:\n    $$ I = \\frac{-2\\pi i e^{i\\pi z}}{1 - e^{i2\\pi z}} = \\frac{-2\\pi i e^{i\\pi z}}{-e^{i\\pi z}(e^{i\\pi z} - e^{-i\\pi z})} = \\frac{2\\pi i}{e^{i\\pi z} - e^{-i\\pi z}} $$\n    Recalling Euler's formula for sine, $\\sin(\\theta) = \\frac{e^{i\\theta} - e^{-i\\theta}}{2i}$, we get:\n    $$ I = \\pi \\left( \\frac{2i}{e^{i\\pi z} - e^{-i\\pi z}} \\right) = \\frac{\\pi}{\\sin(\\pi z)} $$\n\n### Conclusion\n\nWe have shown that for $0 < \\text{Re}(z) < 1$:\n$$ \\Gamma(z)\\Gamma(1-z) = I = \\frac{\\pi}{\\sin(\\pi z)} $$\nThis proves the identity in the specified strip.\n\n**Extension by Analytic Continuation:**\nThe function $\\Gamma(z)\\Gamma(1-z)$ is analytic on the complex plane except for poles at the integers $\\mathbb{Z}$. The function $\\frac{\\pi}{\\sin(\\pi z)}$ is also analytic on the complex plane except for poles at the integers $\\mathbb{Z}$ (where $\\sin(\\pi z) = 0$). Since these two analytic functions agree on the continuous set of points in the interval $(0, 1)$ on the real axis, by the principle of analytic continuation, they must be equal on their entire common domain of analyticity, which is $\\mathbb{C} \\setminus \\mathbb{Z}$.\n\nThis completes the proof."
    },
    {
        "question": "### The Asymptotic Variance of the Prime Divisor Function\n\n**Background and Definitions**\n\nIn number theory, an **arithmetic function** is a function defined on the set of positive integers $\\mathbb{Z}^+$. A simple but fundamental example is the function $\\omega(n)$, which counts the number of distinct prime factors of an integer $n$.\n\nFor example:\n-   $\\omega(1) = 0$\n-   $\\omega(12) = \\omega(2^2 \\cdot 3^1) = 2$ (the distinct prime factors are 2 and 3)\n-   $\\omega(30) = \\omega(2 \\cdot 3 \\cdot 5) = 3$\n-   $\\omega(7) = 1$\n\nA celebrated result by G. H. Hardy and S. Ramanujan (1917) states that the \"normal order\" of $\\omega(n)$ is $\\ln(\\ln n)$. This means that for almost all integers $n$, the value of $\\omega(n)$ is very close to $\\ln(\\ln n)$. A more precise way to state this is to examine the statistical properties of $\\omega(n)$ for integers up to a large number $N$.\n\nThe average value of $\\omega(n)$ for $n \\in \\{1, \\dots, N\\}$ can be shown to be approximately $\\ln(\\ln N)$. This problem asks you to go one step further and analyze the variance, which measures how much the values of $\\omega(n)$ tend to deviate from this average. This result is a cornerstone of probabilistic number theory and is known as the Turan-Kubilius inequality (for the specific function $\\omega(n)$).\n\n**Given Information**\n\nYou may use the following standard results from analytic number theory without proof:\n1.  **Mertens' First Theorem:** For a large real number $x$,\n    $$ \\sum_{p \\le x} \\frac{\\ln p}{p} = \\ln x + O(1) $$\n    where the sum is over all prime numbers $p \\le x$.\n2.  **Mertens' Second Theorem:** For a large real number $x$,\n    $$ \\sum_{p \\le x} \\frac{1}{p} = \\ln(\\ln x) + M + o(1) $$\n    for some constant $M$ (the Meissel-Mertens constant). For the purpose of this problem, the simpler asymptotic form $\\sum_{p \\le x} 1/p = \\ln(\\ln x) + O(1)$ is sufficient.\n3.  **Prime Number Theorem (Weak Form):** The number of primes less than or equal to $x$, denoted $\\pi(x)$, satisfies\n    $$ \\pi(x) = O\\left(\\frac{x}{\\ln x}\\right) $$\n\n**Goal**\n\nProve that the sum of the squared differences between $\\omega(n)$ and its average value $\\ln(\\ln N)$ has the following asymptotic behavior:\n$$ \\sum_{n=1}^N \\left(\\omega(n) - \\ln(\\ln N)\\right)^2 \\sim N \\ln(\\ln N) $$\nThe notation $f(N) \\sim g(N)$ means that $\\lim_{N \\to \\infty} \\frac{f(N)}{g(N)} = 1$.\n\n\nLet $\\omega(n)$ be the number of distinct prime factors of the integer $n$. Prove the following asymptotic relation:\n$$ \\sum_{n=1}^N \\left(\\omega(n) - \\ln(\\ln N)\\right)^2 \\sim N \\ln(\\ln N) $$\nYou may use the three theorems provided in the background section.",
        "answer": "This is a classic result established by P\u00e1l Tur\u00e1n, which provides a simple proof of the Hardy-Ramanujan theorem. The core of the proof involves expanding the sum and evaluating the first and second moments of $\\omega(n)$ by cleverly swapping the order of summations.\n\nLet $L = \\ln(\\ln N)$. We want to evaluate the sum\n$$ S = \\sum_{n=1}^N (\\omega(n) - L)^2 $$\nExpanding the square, we get:\n$$ S = \\sum_{n=1}^N \\omega(n)^2 - 2L \\sum_{n=1}^N \\omega(n) + \\sum_{n=1}^N L^2 = \\left( \\sum_{n=1}^N \\omega(n)^2 \\right) - 2L \\left( \\sum_{n=1}^N \\omega(n) \\right) + N L^2 $$\nWe will evaluate the two sums involving $\\omega(n)$ separately.\n\n### Step 1: The First Moment (Average Value)\n\nLet $M_1 = \\sum_{n=1}^N \\omega(n)$. We can rewrite $\\omega(n)$ as $\\sum_{p|n} 1$, where the sum is over all distinct prime factors $p$ of $n$.\n$$ M_1 = \\sum_{n=1}^N \\sum_{p|n} 1 $$\nWe swap the order of summation. A prime $p$ can be a factor of $n$ only if $p \\le n \\le N$.\n$$ M_1 = \\sum_{p \\le N} \\sum_{\\substack{n=1 \\\\ p|n}}^N 1 $$\nThe inner sum counts the number of integers $n \\in \\{1, \\dots, N\\}$ that are divisible by $p$. This is exactly $\\lfloor N/p \\rfloor$.\n$$ M_1 = \\sum_{p \\le N} \\left\\lfloor \\frac{N}{p} \\right\\rfloor $$\nWe can write $\\lfloor x \\rfloor = x - \\{x\\}$, where $\\{x\\}$ is the fractional part, $0 \\le \\{x\\} < 1$.\n$$ M_1 = \\sum_{p \\le N} \\left(\\frac{N}{p} - \\left\\{\\frac{N}{p}\\right\\}\\right) = N \\sum_{p \\le N} \\frac{1}{p} - \\sum_{p \\le N} \\left\\{\\frac{N}{p}\\right\\} $$\nUsing Mertens' Second Theorem, $\\sum_{p \\le N} 1/p = \\ln(\\ln N) + O(1) = L + O(1)$.\nThe error term is bounded by $\\sum_{p \\le N} 1 = \\pi(N)$, which is $O(N/\\ln N)$ by the Prime Number Theorem.\n$$ M_1 = N(L + O(1)) - O\\left(\\frac{N}{\\ln N}\\right) = N L + O(N) $$\nSo, the first moment is asymptotically $N \\ln(\\ln N)$.\n\n### Step 2: The Second Moment\n\nLet $M_2 = \\sum_{n=1}^N \\omega(n)^2$. We have:\n$$ \\omega(n)^2 = \\left(\\sum_{p|n} 1\\right)^2 = \\left(\\sum_{p|n} 1\\right)\\left(\\sum_{q|n} 1\\right) = \\sum_{p, q \\text{ prime, } p|n, q|n} 1 $$\nwhere $p$ and $q$ are prime divisors of $n$. We can split this sum based on whether $p=q$ or $p \\ne q$.\n$$ \\omega(n)^2 = \\sum_{p|n} 1 + \\sum_{p \\ne q, \\ p|n, q|n} 1 = \\omega(n) + \\sum_{\\substack{p \\ne q \\\\ pq | n}} 1 $$\nNow, we sum over $n$ from 1 to $N$:\n$$ M_2 = \\sum_{n=1}^N \\omega(n) + \\sum_{n=1}^N \\sum_{\\substack{p \\ne q \\\\ pq|n}} 1 $$\nThe first term is just $M_1 = NL + O(N)$. For the second term, we swap the order of summation:\n$$ \\sum_{n=1}^N \\sum_{\\substack{p \\ne q \\\\ pq|n}} 1 = \\sum_{p \\ne q, \\ pq \\le N} \\sum_{\\substack{n=1 \\\\ pq|n}}^N 1 = \\sum_{\\substack{p \\ne q \\\\ pq \\le N}} \\left\\lfloor \\frac{N}{pq} \\right\\rfloor $$\nAgain using $\\lfloor x \\rfloor = x - \\{x\\}$:\n$$ \\sum_{\\substack{p \\ne q \\\\ pq \\le N}} \\left(\\frac{N}{pq} - \\left\\{\\frac{N}{pq}\\right\\}\\right) = N \\sum_{\\substack{p \\ne q \\\\ pq \\le N}} \\frac{1}{pq} - \\sum_{\\substack{p \\ne q \\\\ pq \\le N}} \\left\\{\\frac{N}{pq}\\right\\} $$\nThe error term is bounded by the number of pairs $(p,q)$ with $p \\ne q$ and $pq \\le N$. This sum is $O(N)$. So, the term is $N \\sum_{p\\ne q, pq \\le N} \\frac{1}{pq} + O(N)$.\nThus,\n$$ M_2 = (NL + O(N)) + N \\sum_{\\substack{p \\ne q \\\\ pq \\le N}} \\frac{1}{pq} + O(N) $$\n\n### Step 3: Analyzing the Sum over Prime Pairs\n\nThe crucial step is to relate $\\sum_{p \\ne q, pq \\le N} 1/(pq)$ to $(\\sum_{p \\le N} 1/p)^2$.\nLet $E_N = \\sum_{p \\le N} 1/p$. Then $E_N^2 = (\\sum_{p \\le N} 1/p)(\\sum_{q \\le N} 1/q) = \\sum_{p,q \\le N} \\frac{1}{pq}$.\nWe can split this into $p=q$ and $p \\ne q$:\n$$ E_N^2 = \\sum_{p \\le N} \\frac{1}{p^2} + \\sum_{p \\ne q, p,q \\le N} \\frac{1}{pq} $$\nThe sum we have is $S_{pq} = \\sum_{p \\ne q, pq \\le N} 1/(pq)$. The difference between $\\sum_{p \\ne q, p,q \\le N} 1/(pq)$ and $S_{pq}$ is the sum over pairs where both $p, q \\le N$ but their product $pq > N$.\n$$ \\sum_{p \\ne q, p,q \\le N} \\frac{1}{pq} - S_{pq} = \\sum_{\\substack{p \\ne q, p,q \\le N \\\\ pq > N}} \\frac{1}{pq} $$\nThis remainder term $R_N = \\sum_{p \\ne q, p,q \\le N, pq > N} 1/(pq)$ must be small. For $pq > N$, at least one of the primes must be greater than $\\sqrt{N}$. We can bound this sum:\n$$ R_N \\le 2 \\sum_{p \\le N} \\frac{1}{p} \\sum_{q > N/p, q \\le N} \\frac{1}{q} $$\nUsing mertens' theorems, the inner sum is $\\ln(\\ln N) - \\ln(\\ln(N/p)) + o(1) = -\\ln(1 - \\frac{\\ln p}{\\ln N}) + o(1)$. For large $N$, this is approximately $\\frac{\\ln p}{\\ln N}$.\nA careful analysis shows that $R_N = O(1)$. A simpler observation is that $S_{pq} \\le E_N^2$ and also $(\\sum_{p \\le \\sqrt{N}} 1/p)^2 - \\sum_{p\\le\\sqrt{N}} 1/p^2 \\le S_{pq}$. Both bounds lead to $S_{pq} = (\\ln\\ln N)^2 + O(\\ln\\ln N)$.\nLet's use the relation `S_{pq} = E_N^2 - \\sum_{p \\le N} 1/p^2 - R_N`. The sum $\\sum 1/p^2$ converges (it's less than $\\sum 1/n^2 = \\pi^2/6$), so it's $O(1)$. And as reasoned, $R_N=O(1)$.\nSo, `S_{pq} = E_N^2 - O(1) = (L+O(1))^2 - O(1) = L^2 + O(L)`.\nPlugging this back into $M_2$:\n$$ M_2 = NL + O(N) + N(L^2 + O(L)) = NL^2 + N(L+O(L)) + O(N) = NL^2 + O(N\\ln\\ln N) $$\n\n### Step 4: Final Assembly\n\nNow we substitute our expressions for $M_1$ and $M_2$ into the expanded formula for $S$.\n$$ S = M_2 - 2LM_1 + NL^2 $$\n$$ S = [NL^2 + NL + O(N)] - 2L[NL + O(N)] + NL^2 $$\nWait, the `O(L)` term in the estimate for `S_pq` is too coarse. Let's refine the calculation.\nLet's go back to the expression for S:\n$$ S = \\sum_{n=1}^N \\omega(n)^2 - 2L \\sum_{n=1}^N \\omega(n) + N L^2 $$\n$$ \\sum \\omega(n) = M_1 = \\sum_{p \\le N} \\lfloor N/p \\rfloor $$\n$$ \\sum \\omega(n)^2 = M_2 = \\sum_{p \\le N} \\lfloor N/p \\rfloor + \\sum_{p \\ne q, pq \\le N} \\lfloor N/pq \\rfloor $$\nLet $E_N' = \\sum_{p\\le N} 1/p$. Note $L = \\ln\\ln N = E_N' + O(1)$.\n$$ S = \\left( \\sum_{p \\le N} \\lfloor \\frac{N}{p} \\rfloor + \\sum_{p \\ne q, pq \\le N} \\lfloor \\frac{N}{pq} \\rfloor \\right) - 2L \\left( \\sum_{p \\le N} \\lfloor \\frac{N}{p} \\rfloor \\right) + N L^2 $$\nApproximating $\\lfloor x \\rfloor$ by $x$ with an error $O(1)$:\n$$ \\sum \\lfloor N/p \\rfloor \\approx N E_N' $$\n$$ \\sum \\lfloor N/pq \\rfloor \\approx N \\sum_{pq \\le N, p \\ne q} 1/(pq) \\approx N (E_N'^2 - O(1)) $$\n$$ S \\approx (N E_N' + N E_N'^2) - 2L (N E_N') + N L^2 $$\nSince $L \\approx E_N'$, this is approximately:\n$ S \\approx (N L + N L^2) - 2L(NL) + NL^2 = NL $.\nLet's make this rigorous. Let $E_N = \\sum_{p\\le N} 1/p$.\n$$ S = \\sum_{n=1}^N (\\omega(n) - E_N)^2 + \\sum_{n=1}^N (E_N - L)(2\\omega(n) - E_N - L) $$\nThe second term is $(E_N-L)(2M_1 - N(E_N+L)) = O(1)(2NL - N(2L+O(1))) = O(N)$. So we can prove the result for $E_N$ instead of $L$. Let's prove $\\sum(\\omega(n)-E_N)^2 \\sim N E_N$.\n\n$$ \\sum_{n=1}^N (\\omega(n) - E_N)^2 = M_2 - 2E_N M_1 + N E_N^2 $$\n$$ M_1 = N E_N + O(N/\\ln N) $$\n$$ M_2 = M_1 + \\sum_{p \\ne q, pq \\le N} \\lfloor N/pq \\rfloor = M_1 + N \\sum_{p \\ne q, pq \\le N} \\frac{1}{pq} + O(N) $$\n$$ \\sum_{n=1}^N (\\omega(n) - E_N)^2 = (M_1 + N S_{pq} + O(N)) - 2E_N(N E_N + O(\\frac{N}{\\ln N})) + N E_N^2 $$\nwhere $S_{pq} = \\sum_{p \\ne q, pq \\le N} 1/(pq)$.\n$$ = (N E_N + O(N/\\ln N) + N S_{pq} + O(N)) - 2N E_N^2 - O(\\frac{N E_N}{\\ln N}) + N E_N^2 $$\n$$ = N E_N + N(S_{pq} - E_N^2) + O(N) $$\nWe have established that $E_N^2 = S_{pq} + \\sum_{p\\le N} 1/p^2 + R_N$, where $R_N = \\sum_{p\\ne q, p,q\\le N, pq > N} 1/(pq) = O(1)$ and $\\sum 1/p^2 = O(1)$.\nTherefore, $S_{pq} - E_N^2 = - O(1)$.\n$$ \\sum_{n=1}^N (\\omega(n) - E_N)^2 = N E_N - N \\cdot O(1) + O(N) = N E_N + O(N) $$\nSince $E_N = \\sum_{p\\le N} 1/p = \\ln(\\ln N) + O(1)$, this gives:\n$$ \\sum_{n=1}^N (\\omega(n) - \\ln(\\ln N))^2 = N(\\ln(\\ln N) + O(1)) + O(N) = N \\ln(\\ln N) + O(N) $$\nTo find the limit of the ratio:\n$$ \\lim_{N \\to \\infty} \\frac{\\sum_{n=1}^N (\\omega(n) - \\ln(\\ln N))^2}{N \\ln(\\ln N)} = \\lim_{N \\to \\infty} \\frac{N \\ln(\\ln N) + O(N)}{N \\ln(\\ln N)} = \\lim_{N \\to \\infty} \\left(1 + \\frac{O(N)}{N \\ln(\\ln N)}\\right) = 1 $$\nThis proves the asymptotic relation:\n$$ \\sum_{n=1}^N \\left(\\omega(n) - \\ln(\\ln N)\\right)^2 \\sim N \\ln(\\ln N) $$"
    },
    {
        "question": "### The Isoperimetric Inequality: A Fourier Series Proof\n\n**Background and Definitions**\n\nIn geometry, the **isoperimetric problem** seeks to determine the shape of a closed curve of a given length that encloses the maximum possible area. The solution is a circle, a fact known since antiquity but rigorously proven much later. The mathematical statement of this is the **isoperimetric inequality**. This problem asks for a rigorous proof of this inequality using tools from Fourier analysis.\n\n- A **regular simple closed curve** in the plane is a non-self-intersecting loop that can be described by a continuously differentiable parameterization $(x(t), y(t))$ with a non-vanishing velocity vector.\n- Such a curve $\\mathcal{C}$ can be parameterized by its **arc length** $s$, yielding a parameterization $(x(s), y(s))$ for $s \\in [0, L]$, where $L$ is the total length of the curve. By definition, this parameterization satisfies $(x'(s))^2 + (y'(s))^2 = 1$ for all $s \\in [0, L]$. Since the curve is closed, we have $(x(0), y(0)) = (x(L), y(L))$.\n- The **area** $A$ enclosed by a counter-clockwise oriented curve $\\mathcal{C}$ can be calculated using Green's Theorem:\n$$ A = \\frac{1}{2} \\int_0^L \\left(x(s)y'(s) - y(s)x'(s)\\right) \\,ds $$\n- For the purpose of this problem, you may use the following standard results from **Fourier analysis** without proof. Let $f(s)$ be a complex-valued, continuously differentiable function on $[0, L]$ with $f(0) = f(L)$.\n    1.  **Fourier Series Expansion:** The function can be represented by its Fourier series:\n        $$ f(s) = \\sum_{n=-\\infty}^{\\infty} c_n e^{2\\pi i n s/L} \\quad \\text{where} \\quad c_n = \\frac{1}{L} \\int_0^L f(s) e^{-2\\pi i n s/L} \\,ds $$\n    2.  **Parseval's Identity:** This identity relates the integral of the squared magnitude of a function to the sum of the squared magnitudes of its Fourier coefficients:\n        $$ \\frac{1}{L} \\int_0^L |f(s)|^2 \\,ds = \\sum_{n=-\\infty}^{\\infty} |c_n|^2 $$\n\n**Goal**\n\n1.  Prove the **isoperimetric inequality**: For any regular simple closed curve of length $L$ enclosing an area $A$, the following inequality holds:\n    $$ 4\\pi A \\le L^2 $$\n2.  Prove that equality holds if and only if the curve is a circle.",
        "answer": "This problem can be solved elegantly by representing the curve in the complex plane and applying Fourier analysis. The proof consists of three main parts: translating the geometric properties into the language of Fourier coefficients, deriving the inequality, and analyzing the case of equality.\n\n### Step 1: Complex Representation and Fourier Series\n\nLet the curve $\\mathcal{C}$ be parameterized by arc length $s \\in [0, L]$. We can represent the points on the curve as complex numbers $z(s) = x(s) + i y(s)$. Since the curve is closed and regular, $z(s)$ is a continuously differentiable function with $z(0) = z(L)$. We can therefore consider $z(s)$ to be a periodic function on $\\mathbb{R}$ with period $L$.\n\nThe derivative is $z'(s) = x'(s) + i y'(s)$. The arc length condition, $(x'(s))^2 + (y'(s))^2 = 1$, can be written in complex form as $|z'(s)|^2 = 1$.\n\nThe area formula $A = \\frac{1}{2} \\int_0^L (xy' - yx') \\,ds$ can also be expressed in terms of $z(s)$. Note that the integrand is the imaginary part of $\\bar{z}(s) z'(s)$:\n$$ \\bar{z}(s) z'(s) = (x(s) - iy(s))(x'(s) + iy'(s)) = (x(s)x'(s) + y(s)y'(s)) + i(x(s)y'(s) - y(s)x'(s)) $$\nThus, we have:\n$$ A = \\frac{1}{2} \\int_0^L \\text{Im}(\\bar{z}(s) z'(s)) \\,ds $$\nSince $\\text{Im}(w) = (w - \\bar{w})/(2i)$, we can write:\n$$ A = \\frac{1}{4i} \\int_0^L (\\bar{z}(s)z'(s) - z(s)\\bar{z}'(s)) \\,ds $$\nNow, let's expand $z(s)$ in a Fourier series:\n$$ z(s) = \\sum_{n=-\\infty}^{\\infty} c_n e^{i \\omega_n s} \\quad \\text{where } \\omega_n = \\frac{2\\pi n}{L} $$\nThe derivative $z'(s)$ has the Fourier series:\n$$ z'(s) = \\sum_{n=-\\infty}^{\\infty} i \\omega_n c_n e^{i \\omega_n s} $$\n\n### Step 2: Applying Parseval's Identity\n\nWe apply Parseval's identity to the derivative $z'(s)$. The Fourier coefficients of $z'(s)$ are $d_n = i \\omega_n c_n$. The identity states:\n$$ \\frac{1}{L} \\int_0^L |z'(s)|^2 \\,ds = \\sum_{n=-\\infty}^{\\infty} |d_n|^2 $$\nUsing the arc length condition, the left side is $\\frac{1}{L} \\int_0^L 1 \\,ds = 1$. So we have:\n$$ 1 = \\sum_{n=-\\infty}^{\\infty} |i \\omega_n c_n|^2 = \\sum_{n=-\\infty}^{\\infty} \\omega_n^2 |c_n|^2 = \\sum_{n=-\\infty}^{\\infty} \\left(\\frac{2\\pi n}{L}\\right)^2 |c_n|^2 $$\nMultiplying by $L^2/(4\\pi^2)$ gives a key relationship for the length $L$:\n$$ \\frac{L^2}{4\\pi^2} = \\sum_{n=-\\infty}^{\\infty} n^2 |c_n|^2 $$\n\nNext, we express the area $A$ in terms of the coefficients. Using the orthogonality of the complex exponentials, i.e., $\\int_0^L e^{i\\omega_n s} e^{-i\\omega_m s} \\,ds = L \\delta_{nm}$:\n\\begin{align*}\n\\int_0^L \\bar{z}(s)z'(s) \\,ds &= \\int_0^L \\left(\\sum_{m=-\\infty}^{\\infty} \\bar{c}_m e^{-i\\omega_m s}\\right) \\left(\\sum_{n=-\\infty}^{\\infty} i\\omega_n c_n e^{i\\omega_n s}\\right) \\,ds \\\\\n&= \\sum_{m,n} \\bar{c}_m (i\\omega_n c_n) \\int_0^L e^{i(\\omega_n - \\omega_m)s} \\,ds \\\\\n&= \\sum_{n=-\\infty}^{\\infty} \\bar{c}_n (i\\omega_n c_n) L = iL \\sum_{n=-\\infty}^{\\infty} \\omega_n |c_n|^2 \\\\\n&= iL \\sum_{n=-\\infty}^{\\infty} \\frac{2\\pi n}{L} |c_n|^2 = 2\\pi i \\sum_{n=-\\infty}^{\\infty} n |c_n|^2\n\\end{align*}\nThe integral is purely imaginary, so $\\text{Im}(\\int_0^L \\bar{z}z'\\,ds) = \\int_0^L \\bar{z}z'\\,ds / i$.\nThe area is:\n$$ A = \\frac{1}{2} \\text{Im}\\left(2\\pi i \\sum_{n=-\\infty}^{\\infty} n |c_n|^2\\right) = \\pi \\sum_{n=-\\infty}^{\\infty} n |c_n|^2 $$\n\n### Step 3: Proving the Inequality and Finding the Condition for Equality\n\nWe now have expressions for both $L^2$ and $A$ in terms of the Fourier coefficients $\\{c_n\\}$:\n1.  $L^2 = 4\\pi^2 \\sum_{n=-\\infty}^{\\infty} n^2 |c_n|^2$\n2.  $A = \\pi \\sum_{n=-\\infty}^{\\infty} n |c_n|^2$\n\nTo prove the inequality $4\\pi A \\le L^2$, we examine the difference $L^2 - 4\\pi A$:\n\\begin{align*}\nL^2 - 4\\pi A &= 4\\pi^2 \\sum_{n=-\\infty}^{\\infty} n^2 |c_n|^2 - 4\\pi \\left(\\pi \\sum_{n=-\\infty}^{\\infty} n |c_n|^2\\right) \\\\\n&= 4\\pi^2 \\left( \\sum_{n=-\\infty}^{\\infty} n^2 |c_n|^2 - \\sum_{n=-\\infty}^{\\infty} n |c_n|^2 \\right) \\\\\n&= 4\\pi^2 \\sum_{n=-\\infty}^{\\infty} (n^2 - n) |c_n|^2\n\\end{align*}\nFor any integer $n$, the term $n^2 - n = n(n-1)$ is non-negative.\n- If $n \\ge 1$, $n-1 \\ge 0$, so $n(n-1) \\ge 0$.\n- If $n = 0$, $n(n-1) = 0$.\n- If $n \\le -1$, then $n < 0$ and $n-1 < 0$, so their product $n(n-1)$ is positive.\nSince $|c_n|^2 \\ge 0$ for all $n$, every term in the summation is non-negative. Therefore, the sum itself is non-negative:\n$$ L^2 - 4\\pi A = 4\\pi^2 \\sum_{n=-\\infty}^{\\infty} (n^2 - n) |c_n|^2 \\ge 0 $$\nThis proves the isoperimetric inequality, $L^2 \\ge 4\\pi A$.\n\n### Part 2: The Case of Equality\n\nEquality holds if and only if $L^2 - 4\\pi A = 0$. This requires every term in the sum to be zero:\n$$ (n^2 - n) |c_n|^2 = 0 \\quad \\text{for all } n \\in \\mathbb{Z} $$\nSince $n^2 - n$ is zero only for $n=0$ and $n=1$, this condition implies that $|c_n|^2$ must be zero for all integers $n$ except possibly $n=0$ and $n=1$. This means the Fourier series for $z(s)$ must truncate to:\n$$ z(s) = c_0 e^{i\\omega_0 s} + c_1 e^{i\\omega_1 s} = c_0 + c_1 e^{2\\pi i s/L} $$\nThis is the parameterization of a circle.\n- $c_0$ is a complex constant representing the center of the circle.\n- The term $c_1 e^{2\\pi i s/L}$ traces a circle as $s$ varies from $0$ to $L$.\n\nTo confirm that it is indeed a circle of length $L$, we check the arc length condition using the coefficients. The only non-zero coefficient with $n \\ne 0$ is $c_1$. From the Parseval relation for $L^2$:\n$$ \\frac{L^2}{4\\pi^2} = \\sum_{n=-\\infty}^{\\infty} n^2 |c_n|^2 = 0^2|c_0|^2 + 1^2|c_1|^2 = |c_1|^2 $$\nThis gives $|c_1| = L/(2\\pi)$. The radius of the circle parameterized by $z(s)$ is $R = |c_1|$. Thus, $R = L/(2\\pi)$, which means $L=2\\pi R$. The area enclosed is $A = \\pi R^2 = \\pi(L/(2\\pi))^2 = L^2/(4\\pi)$. For this curve, $4\\pi A = L^2$, so equality holds.\n\nNote: We could also have the case where the only non-zero coefficients are $c_0$ and $c_{-1}$. This would correspond to $z(s) = c_0 + c_{-1}e^{-2\\pi i s/L}$, which is the same circle traversed in the opposite (clockwise) direction. In this case, the area calculation would yield $A = \\pi \\sum n|c_n|^2 = \\pi(-1)|c_{-1}|^2 = -L^2/(4\\pi)$. Since area is physically non-negative, this corresponds to our choice of orientation. The magnitude of the area is the same, and the inequality $|4\\pi A| \\le L^2$ still holds with equality.\n\nTherefore, equality in the isoperimetric inequality holds if and only if the curve is a circle."
    },
    {
        "question": "### The Galois Group of a Quintic Polynomial\n\n**Background and Definitions**\n\n-   Let $P(x)$ be a polynomial with coefficients in a field $F$. The **splitting field** of $P(x)$ over $F$ is the smallest field extension of $F$ that contains all the roots of $P(x)$.\n-   The **Galois group** of $P(x)$ over $F$, denoted $\\text{Gal}(P/F)$, is the group of all automorphisms of the splitting field of $P(x)$ that fix every element of $F$. The Galois group can be viewed as a group of permutations of the roots of $P(x)$, and as such, it is isomorphic to a subgroup of the symmetric group $S_n$, where $n$ is the degree of $P(x)$.\n-   A finite group $G$ is called **solvable** if it has a subnormal series (a sequence of subgroups) $\\{e\\} = G_0 \\triangleleft G_1 \\triangleleft \\dots \\triangleleft G_k = G$ such that each factor group (or quotient group) $G_{i+1}/G_i$ is abelian.\n-   A polynomial is said to be **solvable by radicals** if its roots can be expressed in terms of its coefficients using only the field operations (addition, subtraction, multiplication, division) and the extraction of $n$-th roots.\n-   **Galois's Theorem:** One of the most profound results in algebra, this theorem states that a polynomial is solvable by radicals over a field of characteristic zero if and only if its Galois group is a solvable group.\n-   It is a standard result of group theory that the symmetric group $S_n$ is not a solvable group for any $n \\ge 5$.\n\n**Problem Statement**\n\nConsider the polynomial with rational coefficients:\n$$ P(x) = x^5 - 6x + 3 $$\n\n**Goal**\n\nProve that the Galois group of $P(x)$ over the field of rational numbers $\\mathbb{Q}$ is isomorphic to the symmetric group $S_5$. As a consequence of this result, show that the equation $P(x) = 0$ is not solvable by radicals.",
        "answer": "The proof proceeds in several distinct steps: first establishing the irreducibility of the polynomial, then determining the nature of its roots using calculus, and finally using group-theoretic arguments to identify the Galois group.\n\n### Step 1: Prove that $P(x)$ is irreducible over $\\mathbb{Q}$\n\nWe can use Eisenstein's Criterion for irreducibility. Let the prime be $p=3$. The polynomial is $P(x) = x^5 - 6x + 3$. The integer coefficients are $a_5=1, a_4=0, a_3=0, a_2=0, a_1=-6, a_0=3$.\n\n1.  The prime $p=3$ does not divide the leading coefficient, $a_5 = 1$.\n2.  The prime $p=3$ divides all other coefficients: $a_4=0$, $a_3=0$, $a_2=0$, $a_1=-6$, and $a_0=3$.\n3.  The square of the prime, $p^2=9$, does not divide the constant term, $a_0=3$.\n\nAll conditions of Eisenstein's Criterion are met. Therefore, $P(x)$ is irreducible over the field of rational numbers $\\mathbb{Q}$.\n\n### Step 2: Determine the number of real roots of $P(x)$\n\nWe can use calculus to analyze the real roots of $P(x)$. Let $f(x) = x^5 - 6x + 3$.\nThe derivative is $f'(x) = 5x^4 - 6$.\n\nThe critical points of the function occur where $f'(x) = 0$, which gives $5x^4 = 6$, or $x^4 = 6/5$. This equation has two real solutions:\n$c_1 = -\\left(\\frac{6}{5}\\right)^{1/4}$ and $c_2 = +\\left(\\frac{6}{5}\\right)^{1/4}$.\n\nWe evaluate the function at these two critical points to find the local extrema:\n-   **Local Minimum:** At $x = c_2 = (6/5)^{1/4}$.\n    $f(c_2) = c_2^5 - 6c_2 + 3 = c_2(c_2^4) - 6c_2 + 3 = c_2(6/5) - 6c_2 + 3 = 3 - \\frac{24}{5}c_2$.\n    To determine the sign, we note that $c_2 = (1.2)^{1/4}$. Since $1^4 = 1$ and $1.1^4 \\approx 1.46$, we have $1 < c_2 < 1.1$. In particular, $c_2 > 1$.\n    Therefore, $f(c_2) = 3 - \\frac{24}{5}c_2 < 3 - \\frac{24}{5}(1) = 3 - 4.8 = -1.8$.\n    The local minimum is negative.\n\n-   **Local Maximum:** At $x = c_1 = -(6/5)^{1/4} = -c_2$.\n    $f(c_1) = (-c_2)^5 - 6(-c_2) + 3 = -c_2^5 + 6c_2 + 3 = -(c_2(6/5)) + 6c_2 + 3 = 3 + \\frac{24}{5}c_2$.\n    Since $c_2 > 0$, this value is clearly positive. The local maximum is positive.\n\nThe function $f(x)$ is continuous. It has a positive local maximum and a negative local minimum. This implies that the graph of $f(x)$ must cross the x-axis three times.\nAlso, $\\lim_{x \\to -\\infty} f(x) = -\\infty$ and $\\lim_{x \\to \\infty} f(x) = \\infty$, which is consistent with this shape.\nThus, $P(x)$ has exactly **three real roots**.\n\n### Step 3: Deduce the structure of the Galois group\n\nLet $G = \\text{Gal}(P/ \\mathbb{Q})$.\n1.  Since $P(x)$ is irreducible over $\\mathbb{Q}$ and has degree 5, its Galois group $G$ is a subgroup of $S_5$. Furthermore, the order of $G$ must be divisible by the degree of the polynomial, so $|G|$ is a multiple of 5.\n2.  By Cauchy's theorem from group theory, if a prime $p$ divides the order of a group, then the group contains an element of order $p$. Thus, $G$ must contain an element of order 5. The only elements of order 5 in $S_5$ are 5-cycles. So, **$G$ contains a 5-cycle**.\n3.  By the Fundamental Theorem of Algebra, $P(x)$ has 5 roots in the complex plane $\\mathbb{C}$. From Step 2, we know exactly three of these roots are real numbers. The remaining two roots must be non-real. Since the polynomial has real coefficients, these two non-real roots must be a **complex conjugate pair**, say $r_1 = a+bi$ and $r_2 = a-bi$ with $b \\neq 0$.\n4.  The splitting field of $P(x)$ is closed under complex conjugation. The map $\\sigma: z \\mapsto \\bar{z}$ (complex conjugation) is an automorphism of this splitting field that fixes the base field $\\mathbb{Q}$. Thus, $\\sigma$ is an element of the Galois group $G$.\n5.  When viewed as a permutation of the roots, $\\sigma$ fixes the three real roots and swaps the two complex conjugate roots $(r_1 \\leftrightarrow r_2)$. This permutation is a **transposition**.\n\n### Step 4: Identify the Galois group as $S_5$\n\nWe have established that the Galois group $G$ is a subgroup of $S_5$ that contains both a 5-cycle and a transposition. A standard theorem in group theory states:\n\n> **Theorem:** If $p$ is a prime number, any subgroup of $S_p$ that contains a $p$-cycle and a transposition must be the entire group $S_p$.\n\nApplying this theorem with $p=5$, we conclude that $G$ must be $S_5$.\n\n**Conclusion:** The Galois group of $P(x) = x^5 - 6x + 3$ over $\\mathbb{Q}$ is isomorphic to $S_5$.\n\n### Step 5: Consequence for solvability by radicals\n\n1.  The Galois group of $P(x)$ is $S_5$.\n2.  The symmetric group $S_5$ is not a solvable group.\n3.  According to Galois's Theorem, a polynomial is solvable by radicals if and only if its Galois group is solvable.\n4.  Since the Galois group of $P(x)$ is not solvable, the polynomial **is not solvable by radicals**. This means there is no general formula for the roots of $x^5 - 6x + 3 = 0$ using only arithmetic operations and root extractions."
    },
    {
        "question": "### Brouwer's Fixed-Point Theorem from Sperner's Lemma\n\n**Background and Definitions**\n\nThis problem guides you through a celebrated proof in topology, which uses a combinatorial result about labeled graphs to establish a deep property of continuous functions. The goal is to prove **Brouwer's Fixed-Point Theorem** in two dimensions.\n\n1.  **Continuous Functions and Fixed Points:**\n    -   Let $T$ be a closed triangle in the plane $\\mathbb{R}^2$. A function $f: T \\to T$ is **continuous** if, intuitively, it maps nearby points in the domain to nearby points in the codomain.\n    -   A **fixed point** of a function $f$ is a point $x \\in T$ such that $f(x) = x$. Brouwer's theorem states that such a point always exists for any continuous function from a set like $T$ to itself.\n\n2.  **Triangulation:**\n    -   A **triangulation** of a triangle $T$ is a subdivision of $T$ into a finite number of smaller triangles, called *sub-simplices*. This subdivision must satisfy the condition that any two sub-simplices are either disjoint, share exactly one common vertex, or share exactly one common edge.\n\n3.  **Sperner's Lemma:**\n    -   Let the vertices of the main triangle $T$ be $V_1, V_2, V_3$. Consider a triangulation of $T$. We assign a label from the set $\\{1, 2, 3\\}$ to every vertex of the triangulation.\n    -   This labeling is called a **Sperner labeling** if it satisfies the following two conditions:\n        a.  The main vertices of $T$ are labeled distinctly: $V_1$ is labeled 1, $V_2$ is labeled 2, and $V_3$ is labeled 3.\n        b.  Any vertex lying on an edge of the main triangle $T$ must be labeled with one of the labels of the endpoints of that edge. For example, any vertex on the edge connecting $V_1$ and $V_2$ must be labeled either 1 or 2. Vertices in the interior of $T$ can have any label.\n    -   A sub-simplex (a small triangle in the triangulation) is called **trichromatic** if its three vertices are labeled with all three distinct labels: 1, 2, and 3.\n\n**Goal**\n\nYou are to prove the following two statements in order:\n\n1.  **Prove Sperner's Lemma:** Show that every Sperner-labeled triangulation of a triangle contains an odd number of trichromatic sub-simplices. (Proving it contains at least one is sufficient).\n\n2.  **Prove Brouwer's Fixed-Point Theorem for a Triangle:** Using Sperner's Lemma, prove that any continuous function $f: T \\to T$ must have at least one fixed point. You may assume that a triangle is a compact set and that continuous functions on compact sets are uniformly continuous.",
        "answer": "This problem is solved in two parts, as requested.\n\n### Part 1: Proof of Sperner's Lemma\n\nThe lemma states that for any Sperner labeling of a triangulation of a triangle $T$, there must be an odd number of trichromatic sub-simplices.\n\nWe will use a graph-theoretic counting argument.\n\n1.  **Define \"Doors\":**\n    Consider all the line segments that form the edges of the sub-simplices in the triangulation. We call an edge a **(1,2)-door** if its two endpoints are labeled 1 and 2.\n\n2.  **Count Doors in Sub-simplices:**\n    Let's examine how many (1,2)-doors a single sub-simplex can have. Let its vertices be labeled $L_1, L_2, L_3$.\n    -   If the labels are `{1, 1, 1}`, `{2, 2, 2}`, `{3, 3, 3}`, `{1, 1, 3}`, `{1, 3, 3}`, `{2, 2, 3}`, or `{2, 3, 3}`, there are **zero** (1,2)-doors.\n    -   If the labels are `{1, 1, 2}` or `{1, 2, 2}`, there is exactly **one** (1,2)-door.\n    -   If the labels are `{1, 2, 3}` (i.e., the simplex is trichromatic), there is exactly **one** (1,2)-door.\n\n    Notice that a sub-simplex has an odd number of (1,2)-doors (namely, one) if and only if its labels include both 1 and 2. This is true for triangles of type `{1,1,2}`, `{1,2,2}`, and `{1,2,3}`.\n\n3.  **The Double-Counting Argument:**\n    Let's count the total number of (1,2)-doors in two ways.\n    -   Let $N$ be the total number of (1,2)-doors in the entire triangulation.\n    -   An edge can either be in the interior of the main triangle $T$ or on its boundary.\n        -   If a (1,2)-door is in the **interior**, it is the common edge of two adjacent sub-simplices. Thus, when we sum the number of doors per simplex, this interior door is counted twice.\n        -   If a (1,2)-door is on the **boundary** of $T$, it is part of only one sub-simplex. It is counted once.\n\n    Let $S$ be the set of all sub-simplices. Let $d(\\Delta)$ be the number of (1,2)-doors of a sub-simplex $\\Delta \\in S$. The total sum is:\n    $$ \\sum_{\\Delta \\in S} d(\\Delta) = 2 \\times (\\text{number of interior (1,2)-doors}) + (\\text{number of boundary (1,2)-doors}) $$\n    Taking this equation modulo 2, we get:\n    $$ \\sum_{\\Delta \\in S} d(\\Delta) \\equiv (\\text{number of boundary (1,2)-doors}) \\pmod 2 $$\n\n4.  **Count Doors on the Boundary:**\n    By the rules of Sperner labeling, a vertex on the edge connecting $V_1$ and $V_3$ can only be labeled 1 or 3. It cannot be 2. So, there are no (1,2)-doors on the edge $V_1V_3$.\n    Similarly, there are no (1,2)-doors on the edge $V_2V_3$.\n    We only need to consider the main edge connecting $V_1$ (label 1) and $V_2$ (label 2). Let the vertices along this edge be $v_0, v_1, \\dots, v_k$, where $v_0=V_1$ and $v_k=V_2$. The label of $v_0$ is 1 and the label of $v_k$ is 2. The labels of the intermediate vertices can only be 1 or 2.\n    For there to be a change of label from 1 to 2 along this path, there must be an odd number of segments where the label switches (e.g., from 1 to 2 or 2 to 1). Each such switch corresponds to a (1,2)-door on the boundary. Therefore, the number of (1,2)-doors on the boundary of $T$ is **odd**.\n\n5.  **Conclusion:**\n    From step 3, we have $\\sum_{\\Delta \\in S} d(\\Delta) \\equiv (\\text{odd number}) \\pmod 2$.\n    This means the sum of the number of doors for all sub-simplices must be odd.\n    From step 2, we know that $d(\\Delta)$ is either 0 or 1. So, $\\sum d(\\Delta)$ is just the number of sub-simplices with one door.\n    Therefore, the number of sub-simplices with one (1,2)-door must be odd. These are the sub-simplices with vertex labels of type `{1,1,2}`, `{1,2,2}`, or `{1,2,3}`.\n\n    Now, let's look at the trichromatic sub-simplices (`{1,2,3}`). Let $K$ be the number of such triangles.\n    The number of (1,2)-doors is $K + (\\# \\text{of } \\{1,1,2\\} \\text{ type}) + (\\# \\text{of } \\{1,2,2\\} \\text{ type})$, which we know is odd.\n    This clever argument, however, doesn't isolate $K$. The standard path-following argument is more direct. Let's re-present with that logic.\n    Let $T_{123}$ be the number of trichromatic `(1,2,3)` triangles. The number of (1,2)-doors on the boundary of $T$ is odd. Each path starting from a boundary (1,2)-door must end somewhere. A path consists of moving from a sub-simplex to an adjacent one by crossing a (1,2)-door.\n    - A simplex of type `{1,1,2}` or `{1,2,2}` has two (1,2)-doors, so a path entering it can always exit. These are \"corridor\" rooms.\n    - A simplex of type `{1,2,3}` has one (1,2)-door. A path entering it terminates. These are \"end\" rooms.\n    - A path cannot loop, and it cannot exit the main triangle except through another (1,2)-door on the $V_1V_2$ edge.\n    Since there is an odd number of (1,2)-doors on the boundary, at least one path starting from the boundary cannot be paired up with another path ending on the boundary. This path must therefore terminate inside the triangulation. The only place it can terminate is in a trichromatic sub-simplex.\n\n    A more robust counting argument: Let $T_{123}$ be the number of trichromatic triangles and $T_{112}$ (resp. $T_{122}$) be the number of triangles with vertices {1,1,2} (resp. {1,2,2}). We showed the number of (1,2) edges on the boundary is odd. Let this number be $B_{12}$. The total count of (1,2) \"door sightings\" is $\\sum_{\\Delta} d(\\Delta) = T_{123} + T_{112} + T_{122}$. This is also equal to $2 I_{12} + B_{12}$, where $I_{12}$ is the number of interior (1,2) edges. Modulo 2, we get $T_{123} + T_{112} + T_{122} \\equiv B_{12} \\equiv 1 \\pmod 2$.\n    This argument as written seems incomplete. The standard proof proves that the number of `{1,2,3}` triangles has the same parity as the number of `{1,2}` edges on the boundary. The number of such boundary edges is odd, so the number of `{1,2,3}` triangles is odd, and therefore never zero.\n\n### Part 2: Proof of Brouwer's Fixed-Point Theorem\n\nWe want to prove that any continuous function $f: T \\to T$ has a fixed point. We will argue by contradiction.\n\n1.  **Assume No Fixed Point:**\n    Suppose there exists a continuous function $f: T \\to T$ such that $f(x) \\neq x$ for all $x \\in T$.\n\n2.  **Barycentric Coordinates and Labeling:**\n    Let the vertices of $T$ be $V_1, V_2, V_3$. Any point $x \\in T$ has a unique representation as a **barycentric combination**:\n    $x = c_1(x)V_1 + c_2(x)V_2 + c_3(x)V_3$, where the coefficients $c_i(x) \\ge 0$ and $\\sum_{i=1}^3 c_i(x) = 1$.\n    Similarly, the point $f(x) \\in T$ can be written as:\n    $f(x) = c_1(f(x))V_1 + c_2(f(x))V_2 + c_3(f(x))V_3$. For brevity, let's write $c_i = c_i(x)$ and $c_i' = c_i(f(x))$.\n\n    We define a labeling function $L(x)$ for each point $x \\in T$. Under our assumption that $f(x) \\ne x$, the coordinate vectors $(c_1, c_2, c_3)$ and $(c_1', c_2', c_3')$ must be different. Since $\\sum c_i = \\sum c_i'$, it cannot be that $c_i' \\ge c_i$ for all $i$. Thus, there must be at least one index $i$ for which $c_i' < c_i$.\n\n    We define the label of $x$ to be the *smallest* such index:\n    $$L(x) = \\min \\{ i \\in \\{1,2,3\\} \\mid c_i(f(x)) < c_i(x) \\}$$\n\n3.  **Verify it is a Sperner Labeling:**\n    Let's check the two conditions for any triangulation of $T$.\n    a.  **Vertices of T:**\n        -   For $x = V_1$, its coordinates are $(1, 0, 0)$. So $c_1=1, c_2=0, c_3=0$. For $f(V_1)$, let the coordinates be $(c_1', c_2', c_3')$. Since $f(V_1) \\in T$, we know $c_2' \\ge 0 = c_2$ and $c_3' \\ge 0 = c_3$. For the label to be well-defined, we must have $c_1' < c_1$. Thus, $L(V_1)=1$.\n        -   Similarly, $L(V_2)=2$ and $L(V_3)=3$. Condition (a) is satisfied.\n    b.  **Edges of T:**\n        -   Consider a point $x$ on the edge connecting $V_1$ and $V_2$. Its barycentric coordinates are $(c_1, c_2, 0)$. For $f(x)$, we have coordinates $(c_1', c_2', c_3')$. Since $f(x) \\in T$, we know $c_3' \\ge 0 = c_3$. So the inequality $c_i' < c_i$ cannot hold for $i=3$.\n        -   Therefore, $L(x)$ must be in $\\{1, 2\\}$. Condition (b) is satisfied for the edge $V_1V_2$, and by symmetry for the other edges.\n\n4.  **The Limit Argument:**\n    -   Consider a sequence of triangulations of $T$, indexed by $n=1, 2, 3, \\dots$, such that the maximum edge length (mesh size) of the $n$-th triangulation approaches zero as $n \\to \\infty$.\n    -   For each triangulation $T_n$, we have a Sperner labeling defined above. By Sperner's Lemma, each $T_n$ must contain at least one trichromatic sub-simplex, which we call $\\Delta_n$.\n    -   Let the vertices of $\\Delta_n$ be $x_n^{(1)}, x_n^{(2)}, x_n^{(3)}$, with labels $L(x_n^{(1)})=1$, $L(x_n^{(2)})=2$, and $L(x_n^{(3)})=3$.\n    -   The set $T$ is compact. The sequence of points $(x_n^{(1)})_{n\\ge1}$ lies in $T$ and thus has a convergent subsequence that converges to some point $x^* \\in T$. By relabeling the sequence if necessary, we can assume $x_n^{(1)} \\to x^*$.\n    -   Because the mesh size of the triangulations goes to zero, the other vertices of $\\Delta_n$ must also converge to the same point: $x_n^{(2)} \\to x^*$ and $x_n^{(3)} \\to x^*$.\n\n5.  **Reaching a Contradiction:**\n    By the definition of our labeling, we have the following inequalities for all $n$:\n    -   $L(x_n^{(1)})=1 \\implies c_1(f(x_n^{(1)})) < c_1(x_n^{(1)})$\n    -   $L(x_n^{(2)})=2 \\implies c_2(f(x_n^{(2)})) < c_2(x_n^{(2)})$\n    -   $L(x_n^{(3)})=3 \\implies c_3(f(x_n^{(3)})) < c_3(x_n^{(3)})$\n\n    Now we take the limit as $n \\to \\infty$. The function $f$ is continuous, and the barycentric coordinate functions $c_i(\\cdot)$ are also continuous. Therefore, we can take the limit inside the functions:\n    -   $\\lim_{n\\to\\infty} c_1(f(x_n^{(1)})) = c_1(f(\\lim_{n\\to\\infty} x_n^{(1)})) = c_1(f(x^*))$\n    -   $\\lim_{n\\to\\infty} c_1(x_n^{(1)}) = c_1(\\lim_{n\\to\\infty} x_n^{(1)}) = c_1(x^*)$\n\n    Passing the inequalities to the limit (which turns strict inequalities into non-strict ones):\n    -   $c_1(f(x^*)) \\le c_1(x^*)$\n    -   $c_2(f(x^*)) \\le c_2(x^*)$\n    -   $c_3(f(x^*)) \\le c_3(x^*)$\n\n    Let's sum these three inequalities:\n    $$ c_1(f(x^*)) + c_2(f(x^*)) + c_3(f(x^*)) \\le c_1(x^*) + c_2(x^*) + c_3(x^*) $$\n    By the definition of barycentric coordinates, the sum on each side is exactly 1:\n    $$ 1 \\le 1 $$\n    This equality can only hold if all three of the contributing inequalities are actually equalities:\n    -   $c_1(f(x^*)) = c_1(x^*)$\n    -   $c_2(f(x^*)) = c_2(x^*)$\n    -   $c_3(f(x^*)) = c_3(x^*)$\n\n    Since the barycentric coordinates of $f(x^*)$ and $x^*$ are identical, the points must be the same: $f(x^*) = x^*$.\n    This shows that $x^*$ is a fixed point of $f$. This contradicts our initial assumption that $f$ has no fixed points. Therefore, the assumption must be false, and a fixed point must exist.\n\nThis completes the proof. The theorem for a closed disk $D^2$ follows because a closed triangle is homeomorphic to (can be continuously deformed into) a closed disk."
    },
    {
        "question": "### The Asymptotic Density of Square-Free Integers\n\n**Background and Definitions**\n\nIn number theory, we often study the properties of sets of integers. A fundamental question one can ask about a set of positive integers is how \"large\" it is. The **asymptotic density** is a way to measure this for infinite sets.\n\n-   An integer $n$ is called **square-free** if its prime factorization contains no repeated prime factors. Equivalently, there is no prime $p$ such that $p^2$ divides $n$. For example, $10 = 2 \\cdot 5$ is square-free, but $12 = 2^2 \\cdot 3$ is not.\n-   The **asymptotic density** of a set of positive integers $A \\subseteq \\mathbb{Z}^+$ is defined as the limit, if it exists:\n    $$ D(A) = \\lim_{N \\to \\infty} \\frac{|A \\cap \\{1, 2, \\dots, N\\}|}{N} $$\n    This value represents the \"probability\" that a randomly chosen large integer belongs to the set $A$.\n\nThis problem asks for the asymptotic density of the set of square-free numbers. To aid in this, we introduce the **Mobius function**, $\\mu(n)$, which is an important multiplicative function in number theory defined as follows:\n-   $\\mu(1) = 1$.\n-   $\\mu(n) = (-1)^k$ if $n$ is the product of $k$ distinct prime numbers (i.e., $n$ is square-free with $k$ prime factors).\n-   $\\mu(n) = 0$ if $n$ has a squared prime factor (i.e., $n$ is not square-free).\n\n**Goal**\n\nDetermine the exact value of the asymptotic density of the set of square-free integers.\n\n**Given Information**\n\nYou may use the following facts without proof:\n1.  **Characteristic Function for Square-Free Integers:** For any positive integer $n$, the following identity holds:\n    $$ \\sum_{d^2 | n} \\mu(d) = \\begin{cases} 1 & \\text{if } n \\text{ is square-free} \\\\ 0 & \\text{if } n \\text{ is not square-free} \\end{cases} $$\n    The sum is over all integers $d$ such that $d^2$ is a divisor of $n$.\n2.  **The Basel Problem:** The sum of the reciprocals of the squares of the positive integers is given by:\n    $$ \\sum_{n=1}^\\infty \\frac{1}{n^2} = \\frac{\\pi^2}{6} $$\n\n\nLet $S$ be the set of all square-free positive integers. Find the asymptotic density of $S$. Express your answer as a closed-form constant.",
        "answer": "Let $S$ be the set of square-free integers. We want to compute its asymptotic density, $D(S)$. Let $Q(N)$ be the number of square-free integers in the set $\\{1, 2, \\dots, N\\}$. By definition, the asymptotic density is:\n$$ D(S) = \\lim_{N \\to \\infty} \\frac{Q(N)}{N} $$\n\n**Step 1: Express Q(N) using the Mobius function**\n\nUsing the provided characteristic function for square-free numbers, we can write $Q(N)$ as a sum over all integers from 1 to $N$:\n$$ Q(N) = \\sum_{n=1}^N \\left( \\sum_{d^2 | n} \\mu(d) \\right) $$\n\n**Step 2: Swap the order of summation**\n\nThis double summation can be reordered. The sum is over pairs $(n, d)$ such that $1 \\le n \\le N$ and $d^2 | n$. Instead of summing over $n$ first, we can sum over all possible values of $d$ first.\nFor $d^2$ to be a divisor of some $n \\le N$, we must have $d^2 \\le N$, which implies $d \\le \\sqrt{N}$. For a fixed $d$, the integers $n \\le N$ that are divisible by $d^2$ are $d^2, 2d^2, 3d^2, \\dots, k d^2$ where $k d^2 \\le N$. The number of such integers is $\\lfloor N/d^2 \\rfloor$.\n\nSwapping the order of summation gives:\n$$ Q(N) = \\sum_{d=1}^{\\lfloor\\sqrt{N}\\rfloor} \\sum_{\\substack{n=1 \\\\ d^2 | n}}^N \\mu(d) $$\nFor each $d$, the inner sum adds $\\mu(d)$ for each multiple of $d^2$ up to $N$. There are $\\lfloor N/d^2 \\rfloor$ such multiples.\n$$ Q(N) = \\sum_{d=1}^{\\lfloor\\sqrt{N}\\rfloor} \\mu(d) \\left\\lfloor \\frac{N}{d^2} \\right\\rfloor $$\n\n**Step 3: Prepare for the limit**\n\nTo find the density, we divide by $N$:\n$$ \\frac{Q(N)}{N} = \\frac{1}{N} \\sum_{d=1}^{\\lfloor\\sqrt{N}\\rfloor} \\mu(d) \\left\\lfloor \\frac{N}{d^2} \\right\\rfloor $$\nWe can write $\\lfloor x \\rfloor = x - \\{x\\}$, where $\\{x\\}$ is the fractional part of $x$ satisfying $0 \\le \\{x\\} < 1$.\n$$ \\frac{Q(N)}{N} = \\frac{1}{N} \\sum_{d=1}^{\\lfloor\\sqrt{N}\\rfloor} \\mu(d) \\left( \\frac{N}{d^2} - \\left\\{\\frac{N}{d^2}\\right\\} \\right) $$\n$$ \\frac{Q(N)}{N} = \\sum_{d=1}^{\\lfloor\\sqrt{N}\\rfloor} \\frac{\\mu(d)}{d^2} - \\frac{1}{N} \\sum_{d=1}^{\\lfloor\\sqrt{N}\\rfloor} \\mu(d) \\left\\{\\frac{N}{d^2}\\right\\} $$\n\n**Step 4: Analyze the error term**\n\nLet's analyze the second term, which we'll call the remainder term $R_N$:\n$$ R_N = \\frac{1}{N} \\sum_{d=1}^{\\lfloor\\sqrt{N}\\rfloor} \\mu(d) \\left\\{\\frac{N}{d^2}\\right\\} $$\nWe take the absolute value and use the facts that $|\\mu(d)| \\le 1$ and $0 \\le \\{x\\} < 1$:\n$$ |R_N| = \\left| \\frac{1}{N} \\sum_{d=1}^{\\lfloor\\sqrt{N}\\rfloor} \\mu(d) \\left\\{\\frac{N}{d^2}\\right\\} \\right| \\le \\frac{1}{N} \\sum_{d=1}^{\\lfloor\\sqrt{N}\\rfloor} |\\mu(d)| \\cdot \\left|\\left\\{\\frac{N}{d^2}\\right\\}\\right| < \\frac{1}{N} \\sum_{d=1}^{\\lfloor\\sqrt{N}\\rfloor} 1 \\cdot 1 $$\n$$ |R_N| < \\frac{\\lfloor\\sqrt{N}\\rfloor}{N} \\le \\frac{\\sqrt{N}}{N} = \\frac{1}{\\sqrt{N}} $$\nAs $N \\to \\infty$, the term $1/\\sqrt{N}$ goes to 0, which means $\\lim_{N \\to \\infty} R_N = 0$.\n\n**Step 5: Compute the limit**\n\nNow we can compute the limit of the main expression:\n$$ D(S) = \\lim_{N \\to \\infty} \\frac{Q(N)}{N} = \\lim_{N \\to \\infty} \\left( \\sum_{d=1}^{\\lfloor\\sqrt{N}\\rfloor} \\frac{\\mu(d)}{d^2} - R_N \\right) $$\nSince $\\lim_{N\\to\\infty} R_N = 0$, this simplifies to:\n$$ D(S) = \\lim_{N \\to \\infty} \\sum_{d=1}^{\\lfloor\\sqrt{N}\\rfloor} \\frac{\\mu(d)}{d^2} = \\sum_{d=1}^{\\infty} \\frac{\\mu(d)}{d^2} $$\n\n**Step 6: Relate the sum to the Riemann Zeta Function**\n\nThis infinite sum is a famous Dirichlet series. The Riemann zeta function $\\zeta(s)$ and the Mobius function $\\mu(n)$ are related by the identity:\n$$ \\frac{1}{\\zeta(s)} = \\sum_{n=1}^\\infty \\frac{\\mu(n)}{n^s} \\quad \\text{for } \\text{Re}(s) > 1 $$\nOur sum is precisely this series evaluated at $s=2$. Therefore:\n$$ D(S) = \\sum_{d=1}^\\infty \\frac{\\mu(d)}{d^2} = \\frac{1}{\\zeta(2)} $$\n\n**Step 7: Final Result**\n\nUsing the given value from the Basel problem, $\\zeta(2) = \\sum_{n=1}^\\infty \\frac{1}{n^2} = \\frac{\\pi^2}{6}$.\nSubstituting this into our expression gives the final answer for the asymptotic density of square-free integers:\n$$ D(S) = \\frac{1}{\\pi^2/6} = \\frac{6}{\\pi^2} $$"
    },
    {
        "question": "### Polya's Random Walk Theorem in Two Dimensions\n\n**Background and Definitions**\n\nA **simple symmetric random walk** on the integer lattice $\\mathbb{Z}^d$ describes a path that starts at the origin and, at each time step, moves to one of the $2d$ nearest neighboring points with equal probability.\n\n-   Let $P_n \\in \\mathbb{Z}^d$ be the position of the particle after $n$ steps. $P_0 = \\mathbf{0}$.\n-   The walk is called **recurrent** if the particle returns to the origin with probability 1. It is **transient** if there is a non-zero probability that the particle never returns to the origin.\n-   A key criterion for recurrence is the divergence of the sum of the probabilities of being at the origin. Let $p_n(\\mathbf{0}) = \\mathbb{P}(P_n = \\mathbf{0})$ be the probability that the particle is at the origin after $n$ steps. The walk is recurrent if and only if the expected number of returns to the origin is infinite, which is equivalent to the condition:\n    $$ \\sum_{n=1}^\\infty p_n(\\mathbf{0}) = \\infty $$\n\n**Given Information**\n\nYou may use without proof the following asymptotic formula for factorials, known as **Stirling's approximation**:\n$$ n! \\sim \\sqrt{2\\pi n} \\left(\\frac{n}{e}\\right)^n $$\nThis implies the following approximation for the central binomial coefficient:\n$$ \\binom{2n}{n} \\sim \\frac{4^n}{\\sqrt{\\pi n}} \\quad \\text{as } n \\to \\infty $$\n\n**Goal**\n\nProve that the simple symmetric random walk on the 2-dimensional integer lattice $\\mathbb{Z}^2$ is **recurrent**.",
        "answer": "To prove that the simple symmetric random walk on $\\mathbb{Z}^2$ is recurrent, we must show that the sum of probabilities of being at the origin diverges: $\\sum_{n=1}^\\infty p_n(\\mathbf{0}) = \\infty$.\n\n### Step 1: Conditions for Returning to the Origin\n\nLet the position of the particle after $n$ steps be $P_n = (x_n, y_n)$. The particle starts at $P_0 = (0,0)$. At each step, the particle moves one unit in one of the four cardinal directions (Up, Down, Left, Right) with equal probability $1/4$.\n\nFor the particle to be at the origin after $n$ steps, i.e., $P_n = (0,0)$, it must have taken an even number of steps. If $n$ is odd, it is impossible for the particle to be at an even-coordinate point like $(0,0)$. Thus, $p_n(\\mathbf{0}) = 0$ for all odd $n$. We only need to consider steps of the form $n=2k$.\n\nLet $U, D, L, R$ be the number of steps taken Up, Down, Left, and Right, respectively, after $2k$ steps. The total number of steps is $U+D+L+R = 2k$.\nFor the final position to be $(0,0)$, the number of Up steps must equal the number of Down steps, and the number of Left steps must equal the number of Right steps.\n-   $U - D = 0 \\implies U = D$\n-   $L - R = 0 \\implies L = R$\n\nLet $U = D = i$ and $L = R = j$. Substituting this into the total steps equation gives $2i + 2j = 2k$, which simplifies to $i+j=k$. Here, $i$ can range from $0$ to $k$.\n\n### Step 2: Calculating the Probability $p_{2k}(\\mathbf{0})$\n\nThe total number of distinct paths of length $2k$ is $4^{2k}$, since there are 4 choices at each of the $2k$ steps.\n\nFor a fixed pair of integers $(i, j)$ such that $i+j=k$, the number of paths consisting of $i$ Up, $i$ Down, $j$ Left, and $j$ Right steps is given by the multinomial coefficient:\n$$ \\binom{2k}{i, i, j, j} = \\frac{(2k)!}{i! \\, i! \\, j! \\, j!} $$\n\nTo find the total probability of being at the origin after $2k$ steps, we must sum over all possible values of $i$ (from $0$ to $k$), with $j=k-i$:\n$$ p_{2k}(\\mathbf{0}) = \\sum_{i=0}^k \\frac{(2k)!}{(i!)^2 ((k-i)!)^2} \\left(\\frac{1}{4}\\right)^{2k} $$\n\n### Step 3: Simplifying the Expression\n\nWe can rewrite the sum by factoring out terms and recognizing binomial coefficients.\n$$ p_{2k}(\\mathbf{0}) = \\frac{1}{4^{2k}} \\sum_{i=0}^k \\frac{(2k)!}{(i!)^2 ((k-i)!)^2} $$\nLet's manipulate the fraction:\n$$ \\frac{(2k)!}{(i!)^2 ((k-i)!)^2} = \\frac{(2k)!}{(k!)^2} \\cdot \\frac{(k!)^2}{(i!)^2 ((k-i)!)^2} = \\binom{2k}{k} \\left( \\frac{k!}{i!(k-i)!} \\right)^2 = \\binom{2k}{k} \\binom{k}{i}^2 $$\nSubstituting this back into the expression for $p_{2k}(\\mathbf{0})$:\n$$ p_{2k}(\\mathbf{0}) = \\frac{1}{4^{2k}} \\binom{2k}{k} \\sum_{i=0}^k \\binom{k}{i}^2 $$\nWe now use the well-known combinatorial identity (a special case of Vandermonde's identity):\n$$ \\sum_{i=0}^k \\binom{k}{i}^2 = \\binom{2k}{k} $$\nThis gives a remarkably simple closed form for the probability:\n$$ p_{2k}(\\mathbf{0}) = \\frac{1}{4^{2k}} \\binom{2k}{k} \\binom{2k}{k} = \\left( \\frac{1}{2^{2k}} \\binom{2k}{k} \\right)^2 $$\n\n### Step 4: Asymptotic Analysis\n\nWe now analyze the behavior of $p_{2k}(\\mathbf{0})$ as $k \\to \\infty$. We use the given Stirling's approximation for the central binomial coefficient:\n$$ \\binom{2k}{k} \\sim \\frac{4^k}{\\sqrt{\\pi k}} $$\nSubstituting this into our expression for $p_{2k}(\\mathbf{0})$:\n$$ p_{2k}(\\mathbf{0}) \\sim \\left( \\frac{1}{4^k} \\cdot \\frac{4^k}{\\sqrt{\\pi k}} \\right)^2 = \\left( \\frac{1}{\\sqrt{\\pi k}} \\right)^2 = \\frac{1}{\\pi k} $$\n\n### Step 5: Testing for Recurrence\n\nThe walk is recurrent if the sum $\\sum_{n=1}^\\infty p_n(\\mathbf{0})$ diverges. As $p_n(\\mathbf{0})=0$ for odd $n$, this is equivalent to the divergence of the sum over even terms:\n$$ \\sum_{k=1}^\\infty p_{2k}(\\mathbf{0}) $$\nWe can use the Limit Comparison Test. Let $a_k = p_{2k}(\\mathbf{0})$ and $b_k = \\frac{1}{k}$. We have shown that:\n$$ \\lim_{k \\to \\infty} \\frac{a_k}{b_k} = \\lim_{k \\to \\infty} \\frac{1/(\\pi k)}{1/k} = \\frac{1}{\\pi} $$\nSince this limit is a finite positive constant, the series $\\sum a_k$ and $\\sum b_k$ either both converge or both diverge.\n\nThe series $\\sum_{k=1}^\\infty b_k = \\sum_{k=1}^\\infty \\frac{1}{k}$ is the harmonic series, which is known to diverge.\n\nTherefore, the series $\\sum_{k=1}^\\infty p_{2k}(\\mathbf{0})$ also diverges.\n\n### Conclusion\n\nSince $\\sum_{n=1}^\\infty p_n(\\mathbf{0}) = \\sum_{k=1}^\\infty p_{2k}(\\mathbf{0}) = \\infty$, the expected number of returns to the origin is infinite. By the given criterion, the simple symmetric random walk on the 2-dimensional lattice $\\mathbb{Z}^2$ is recurrent."
    },
    {
        "question": "### Jensen's Formula: Connecting Zeros and Averages of an Analytic Function\n\n**Background and Definitions**\n\nIn complex analysis, there is a beautiful relationship between the magnitude of an analytic function on a circle and the locations of its zeros inside the circle. This relationship is captured by Jensen's formula.\n\n-   An **analytic function** $f(z)$ in a domain $\\Omega \\subseteq \\mathbb{C}$ is a complex-valued function that is complex differentiable at every point in $\\Omega$.\n-   A **zero** of an analytic function $f(z)$ is a point $a$ such that $f(a)=0$. Zeros are counted with multiplicity.\n-   Let $R > 0$ be a real number. The **closed disk** of radius $R$ centered at the origin is the set $\\overline{D_R} = \\{z \\in \\mathbb{C} : |z| \\le R\\}$.\n-   It is a standard result that if a function $g(z)$ is analytic and has no zeros in a simply connected domain (like a disk), then its real part, $\\log|g(z)|$, is a **harmonic function**. Harmonic functions satisfy the **mean value property**: the value at the center of a disk is equal to the average of its values on the boundary circle.\n\n**Problem Statement**\n\nLet $f(z)$ be an analytic function in a region that contains the closed disk $\\overline{D_R}$. Let $a_1, a_2, \\dots, a_n$ be the zeros of $f(z)$ inside the open disk $D_R = \\{z \\in \\mathbb{C} : |z| < R\\}$, repeated according to their multiplicity.\n\nAssume that $f(z)$ has no zeros on the boundary circle $|z|=R$ and that $f(0) \\ne 0$.\n\n**Goal**\n\nProve **Jensen's formula**:\n$$ \\log|f(0)| + \\sum_{k=1}^n \\log\\left(\\frac{R}{|a_k|}\\right) = \\frac{1}{2\\pi} \\int_0^{2\\pi} \\log|f(Re^{i\\theta})| \\,d\\theta $$",
        "answer": "The proof of Jensen's formula involves constructing a new function that has the same magnitude as the original function on the boundary circle but has no zeros inside the disk. We can then apply the mean value property for harmonic functions to the logarithm of this new function.\n\n### Step 1: Constructing a Zero-Free Function\n\nThe core idea is to \"remove\" the zeros of $f(z)$ from the interior of the disk $D_R$ without changing the magnitude of the function on the boundary circle $|z|=R$. This is achieved by multiplying $f(z)$ by a carefully chosen product of functions called **Blaschke factors**.\n\nFor each zero $a_k$ of $f(z)$ in $D_R$, consider the function:\n$$ \\phi_k(z) = \\frac{R(z - a_k)}{R^2 - \\bar{a}_k z} $$\nThis function maps the zero $a_k$ to the origin. Let's examine its magnitude on the circle $|z|=R$. For $z$ on this circle, we have $z\\bar{z} = |z|^2 = R^2$, so $\\bar{z} = R^2/z$.\n$$ |\\phi_k(z)| = \\left| \\frac{R(z - a_k)}{R^2 - \\bar{a}_k z} \\right| = \\left| \\frac{R(z - a_k)}{z\\bar{z} - \\bar{a}_k z} \\right| = \\left| \\frac{R(z - a_k)}{z(\\bar{z} - \\bar{a}_k)} \\right| = \\frac{R}{|z|} \\left| \\frac{z-a_k}{\\overline{z-a_k}} \\right| $$\nSince $|w| = |\\bar{w}|$, we have $|z-a_k| = |\\overline{z-a_k}|$. Also, $|z|=R$. Thus, for $|z|=R$:\n$$ |\\phi_k(z)| = \\frac{R}{R} \\cdot 1 = 1 $$\nThis factor has a magnitude of 1 on the boundary circle. To cancel the zeros of $f(z)$, we should divide by these factors.\n\nLet's define a new function $g(z)$ as follows:\n$$ g(z) = f(z) \\prod_{k=1}^n \\frac{R^2 - \\bar{a}_k z}{R(z - a_k)} $$\nThis is $f(z)$ divided by the product of the Blaschke factors $\\phi_k(z)$.\n\n### Step 2: Properties of the New Function `g(z)`\n\n1.  **Analyticity:** The function $f(z)$ is analytic in $\\overline{D_R}$ and has zeros at $a_1, \\dots, a_n$. Each term in the product introduces a simple pole at $z=a_k$. Since the zeros of $f(z)$ match these poles (including multiplicity), the singularities are removable. Therefore, $g(z)$ is analytic throughout the disk $D_R$.\n\n2.  **Zero-Free:** By construction, the zeros of the denominator cancel the zeros of $f(z)$. Thus, $g(z)$ has no zeros inside the disk $D_R$.\n\n3.  **Magnitude on the Boundary:** For any $z$ on the circle $|z|=R$, we have shown that the magnitude of each factor $\\frac{R(z-a_k)}{R^2 - \\bar{a}_k z}$ is 1. Therefore, the magnitude of its reciprocal is also 1.\n    $$ |g(z)| = |f(z)| \\prod_{k=1}^n \\left| \\frac{R^2 - \\bar{a}_k z}{R(z - a_k)} \\right| = |f(z)| \\cdot 1 = |f(z)| $$\n    So, on the boundary circle $|z|=R$, the functions $g(z)$ and $f(z)$ have the same magnitude.\n\n### Step 3: Applying the Mean Value Property\n\nSince $g(z)$ is analytic and non-zero in the disk $D_R$ (and on its boundary, as $f(z)$ has no zeros there), the function $\\log|g(z)|$ is well-defined and **harmonic** inside the disk.\n\nBy the **mean value property for harmonic functions**, the value of $\\log|g(z)|$ at the center of the disk ($z=0$) is the average of its values on the boundary circle $|z|=R$.\n$$ \\log|g(0)| = \\frac{1}{2\\pi} \\int_0^{2\\pi} \\log|g(Re^{i\\theta})| \\,d\\theta $$\n\n### Step 4: Final Calculation\n\nWe now substitute our expressions for $g(0)$ and $g(Re^{i\\theta})$ into this equation.\n\nFirst, the right-hand side (RHS):\nSince $|g(z)| = |f(z)|$ on the circle $|z|=R$, we have:\n$$ \\text{RHS} = \\frac{1}{2\\pi} \\int_0^{2\\pi} \\log|f(Re^{i\\theta})| \\,d\\theta $$\nThis is exactly the integral term in Jensen's formula.\n\nNext, the left-hand side (LHS), $\\log|g(0)|$. We evaluate $g(z)$ at $z=0$:\n$$ g(0) = f(0) \\prod_{k=1}^n \\frac{R^2 - \\bar{a}_k (0)}{R(0 - a_k)} = f(0) \\prod_{k=1}^n \\frac{R^2}{-R a_k} = f(0) \\prod_{k=1}^n \\left(-\\frac{R}{a_k}\\right) $$\nNow, we take the magnitude. We are given that $|a_k| < R$.\n$$ |g(0)| = |f(0)| \\prod_{k=1}^n \\left| -\\frac{R}{a_k} \\right| = |f(0)| \\prod_{k=1}^n \\frac{R}{|a_k|} $$\nFinally, we take the natural logarithm:\n$$ \\log|g(0)| = \\log \\left( |f(0)| \\prod_{k=1}^n \\frac{R}{|a_k|} \\right) = \\log|f(0)| + \\sum_{k=1}^n \\log\\left(\\frac{R}{|a_k|}\\right) $$\nThis is the expression on the left side of Jensen's formula.\n\nBy equating the LHS and RHS from the mean value property, we arrive at the desired result:\n$$ \\log|f(0)| + \\sum_{k=1}^n \\log\\left(\\frac{R}{|a_k|}\\right) = \\frac{1}{2\\pi} \\int_0^{2\\pi} \\log|f(Re^{i\\theta})| \\,d\\theta $$\nThis completes the proof."
    },
    {
        "question": "### Lagrange's Four-Square Theorem via the Geometry of Numbers\n\n**Background and Definitions**\n\nIn number theory, **Lagrange's four-square theorem**, sometimes called Bachet's conjecture, states that every natural number can be represented as the sum of four integer squares. For example, $31 = 5^2 + 2^2 + 1^2 + 1^2$. This problem asks for a proof of this theorem using tools from the geometry of numbers.\n\nTo make the problem self-contained, we introduce the following concepts and state a major theorem you may use without proof.\n\n1.  **Lattice:** A **lattice** $L$ in $\\mathbb{R}^n$ is a set of points of the form $L = \\{\\sum_{i=1}^n c_i \\mathbf{b}_i \\mid c_i \\in \\mathbb{Z}\\}$, where $\\{\\mathbf{b}_1, \\dots, \\mathbf{b}_n\\}$ is a set of linearly independent vectors in $\\mathbb{R}^n$ called the **basis** of the lattice.\n    -   The **fundamental parallelepiped** of the lattice with respect to this basis is the set $P = \\{\\sum_{i=1}^n x_i \\mathbf{b}_i \\mid 0 \\le x_i < 1\\}$.\n    -   The **volume of the fundamental parallelepiped** is given by $|\\det(B)|$, where $B$ is the matrix whose columns are the basis vectors $\\mathbf{b}_i$. This volume is an invariant of the lattice and does not depend on the choice of basis.\n\n2.  **Convex and Centrally Symmetric Sets:**\n    -   A set $K \\subseteq \\mathbb{R}^n$ is **convex** if for any two points $\\mathbf{x}, \\mathbf{y} \\in K$, the line segment connecting them, $\\{\\lambda \\mathbf{x} + (1-\\lambda)\\mathbf{y} \\mid 0 \\le \\lambda \\le 1\\}$, is entirely contained in $K$.\n    -   A set $K \\subseteq \\mathbb{R}^n$ is **centrally symmetric** if for any point $\\mathbf{x} \\in K$, the point $-\\mathbf{x}$ is also in $K$.\n\n**Given Information**\n\nYou may use the following theorems without proof:\n\n1.  **Minkowski's Convex Body Theorem:** Let $L$ be a lattice in $\\mathbb{R}^n$ and let $K$ be a convex, centrally symmetric subset of $\\mathbb{R}^n$. If the volume of $K$ is strictly greater than $2^n$ times the volume of the fundamental parallelepiped of $L$, then $K$ must contain at least one point of $L$ other than the origin $\\mathbf{0}$.\n    $$ \\text{vol}(K) > 2^n \\text{vol}(L) \\implies K \\cap (L \\setminus \\{\\mathbf{0}\\}) \\neq \\emptyset $$\n\n2.  **Euler's Four-Square Identity:** The product of two numbers that are each a sum of four squares is itself a sum of four squares. Specifically, for any integers $a_i, b_i$:\n    $$ (a_1^2+a_2^2+a_3^2+a_4^2)(b_1^2+b_2^2+b_3^2+b_4^2) = \\\\ (a_1b_1 - a_2b_2 - a_3b_3 - a_4b_4)^2 + (a_1b_2 + a_2b_1 + a_3b_4 - a_4b_3)^2 + \\\\ (a_1b_3 - a_2b_4 + a_3b_1 + a_4b_2)^2 + (a_1b_4 + a_2b_3 - a_3b_2 + a_4b_1)^2 $$\n\n**Goal**\n\nProve **Lagrange's four-square theorem**: Every positive integer can be written as the sum of four integer squares.",
        "answer": "The proof proceeds in several steps. First, we reduce the problem to proving it for prime numbers. Then, we use number theory to establish a key congruence. Finally, we construct a specific lattice and a convex set in $\\mathbb{R}^4$ to apply Minkowski's theorem, which proves the result for primes.\n\n### Step 1: Reduction to Prime Numbers\n\nBy the fundamental theorem of arithmetic, any positive integer $n > 1$ can be written as a product of prime numbers. Euler's four-square identity shows that if two numbers, $N_1$ and $N_2$, are sums of four squares, then their product $N_1 N_2$ is also a sum of four squares.\n\nBy induction, this means that if we can prove that every prime number is a sum of four squares, then every positive integer (which is a product of primes) must also be a sum of four squares. The number 1 is trivially a sum of four squares: $1 = 1^2 + 0^2 + 0^2 + 0^2$.\n\nTherefore, the main goal is to prove that any prime number $p$ can be written as the sum of four integer squares.\n\n### Step 2: The Case for the Prime $p=2$\n\nThe smallest prime, 2, can be written as a sum of four squares: $2 = 1^2 + 1^2 + 0^2 + 0^2$.\nSo we only need to consider odd primes $p$.\n\n### Step 3: A Key Congruence Lemma\n\n**Lemma:** For any odd prime $p$, there exist integers $x$ and $y$ such that $x^2 + y^2 + 1 \\equiv 0 \\pmod p$.\n\n**Proof of Lemma:**\nConsider the following two sets of residues modulo $p$:\n1.  $S_1 = \\{x^2 \\pmod p \\mid x \\in \\{0, 1, \\dots, p-1\\}\\}$\n2.  $S_2 = \\{-1 - y^2 \\pmod p \\mid y \\in \\{0, 1, \\dots, p-1\\}\\}$\n\nThe set of quadratic residues modulo $p$, $S_1$, contains $1 + (p-1)/2 = (p+1)/2$ distinct elements. This is because $x^2 \\equiv z^2 \\pmod p$ if and only if $x \\equiv \\pm z \\pmod p$, so for $x \\in \\{1, \\dots, p-1\\}$, each non-zero square is the image of two elements, plus the element $0^2=0$.\n\nThe set $S_2$ is a translation and reflection of $S_1$, so it also has $(p+1)/2$ distinct elements.\n\nThe total number of elements in $S_1 \\cup S_2$ is $|S_1 \\cup S_2| = |S_1| + |S_2| - |S_1 \\cap S_2|$.\nThe combined number of elements in these two sets is $|S_1| + |S_2| = (p+1)/2 + (p+1)/2 = p+1$.\nSince there are only $p$ distinct residues modulo $p$, by the Pigeonhole Principle, the two sets must have at least one element in common. That is, $S_1 \\cap S_2$ is non-empty.\n\nThus, there must exist some $x_0, y_0$ such that $x_0^2 \\equiv -1 - y_0^2 \\pmod p$. Rearranging gives $x_0^2 + y_0^2 + 1 \\equiv 0 \\pmod p$. This proves the lemma.\n\n### Step 4: Constructing the Lattice and Applying Minkowski's Theorem\n\nWe now use the result of the lemma to prove that any odd prime $p$ is a sum of four squares. Let $x_0, y_0$ be integers satisfying $x_0^2 + y_0^2 + 1 \\equiv 0 \\pmod p$.\n\nConsider the following set of vectors in $\\mathbb{R}^4$:\n$L = \\{ (v_1, v_2, v_3, v_4) \\in \\mathbb{Z}^4 \\mid v_3 \\equiv x_0 v_1 + y_0 v_2 \\pmod p, \\text{ and } v_4 \\equiv y_0 v_1 - x_0 v_2 \\pmod p \\}$\n\nThis set $L$ forms a lattice in $\\mathbb{R}^4$. A basis for this lattice is given by the vectors:\n$\\mathbf{b}_1 = (p, 0, 0, 0)$\n$\\mathbf{b}_2 = (0, p, 0, 0)$\n$\\mathbf{b}_3 = (x_0, y_0, 1, 0)$\n$\\mathbf{b}_4 = (y_0, -x_0, 0, 1)$\n\nThe volume of the fundamental parallelepiped of $L$ is the absolute value of the determinant of the matrix formed by these basis vectors:\n$$ \\text{vol}(L) = \\left| \\det \\begin{pmatrix} p & 0 & x_0 & y_0 \\\\ 0 & p & y_0 & -x_0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{pmatrix} \\right| = p \\cdot p \\cdot 1 \\cdot 1 = p^2 $$\n\nNow, let $K$ be an open ball in $\\mathbb{R}^4$ centered at the origin with radius $r$:\n$K = \\{ \\mathbf{v} \\in \\mathbb{R}^4 \\mid v_1^2+v_2^2+v_3^2+v_4^2 < r^2 \\}$\n$K$ is a convex and centrally symmetric set. The volume of a 4-dimensional ball of radius $r$ is $\\frac{1}{2}\\pi^2 r^4$.\n\nWe want to apply Minkowski's theorem, so we need to choose $r$ such that $\\text{vol}(K) > 2^4 \\text{vol}(L)$.\nLet's set $r^2 = 2p$. Then the condition becomes:\n$$ \\text{vol}(K) = \\frac{1}{2}\\pi^2 (r^2)^2 = \\frac{1}{2}\\pi^2 (2p)^2 = 2\\pi^2 p^2 $$\nWe must check if $2\\pi^2 p^2 > 16 p^2$. This simplifies to $\\pi^2 > 8$. Since $\\pi \\approx 3.14159$, $\\pi^2 \\approx 9.87$, so the inequality holds.\n\nBy Minkowski's theorem, since $\\text{vol}(K) > 16p^2$, there must exist a non-zero vector $\\mathbf{v} = (v_1, v_2, v_3, v_4)$ that is in both the lattice $L$ and the ball $K$.\n\n### Step 5: Final Conclusion\n\nThis non-zero vector $\\mathbf{v} \\in L \\cap K$ has the following properties:\n1.  Since $\\mathbf{v} \\in \\mathbb{Z}^4$ and is non-zero, the sum of its squared components, $N = v_1^2+v_2^2+v_3^2+v_4^2$, is a positive integer.\n2.  Since $\\mathbf{v} \\in K$, we have $v_1^2+v_2^2+v_3^2+v_4^2 < r^2 = 2p$. So $0 < N < 2p$.\n3.  Since $\\mathbf{v} \\in L$, its components satisfy the congruences $v_3 \\equiv x_0 v_1 + y_0 v_2 \\pmod p$ and $v_4 \\equiv y_0 v_1 - x_0 v_2 \\pmod p$. Let's examine the sum of squares modulo $p$:\n    \\begin{align*} v_1^2+v_2^2+v_3^2+v_4^2 & \\equiv v_1^2+v_2^2 + (x_0 v_1 + y_0 v_2)^2 + (y_0 v_1 - x_0 v_2)^2 \\pmod p \\\\ & \\equiv v_1^2+v_2^2 + (x_0^2v_1^2 + 2x_0y_0v_1v_2 + y_0^2v_2^2) + (y_0^2v_1^2 - 2x_0y_0v_1v_2 + x_0^2v_2^2) \\pmod p \\\\ & \\equiv v_1^2+v_2^2 + (x_0^2+y_0^2)v_1^2 + (x_0^2+y_0^2)v_2^2 \\pmod p \\\\ & \\equiv (1 + x_0^2 + y_0^2)(v_1^2 + v_2^2) \\pmod p \\end{align*}\n    From our lemma in Step 3, we know $1 + x_0^2 + y_0^2 \\equiv 0 \\pmod p$. Therefore,\n    $$ v_1^2+v_2^2+v_3^2+v_4^2 \\equiv 0 \\pmod p $$\n\nSo, we have found an integer $N = v_1^2+v_2^2+v_3^2+v_4^2$ that is a multiple of $p$ and also satisfies $0 < N < 2p$. The only multiple of $p$ in this range is $p$ itself.\n\nTherefore, we must have $v_1^2+v_2^2+v_3^2+v_4^2 = p$.\n\nThis shows that any odd prime $p$ can be written as the sum of four integer squares. Combined with the cases for $n=1, 2$ and the reduction argument in Step 1, this completes the proof of Lagrange's four-square theorem."
    },
    {
        "question": "### The Cauchy Determinant\n\n**Background and Definitions**\n\nIn linear algebra, a **determinant** is a scalar value that can be computed from the elements of a square matrix. It encodes certain properties of the linear transformation described by the matrix. While computing determinants for matrices with arbitrary numerical entries can be tedious, there exist remarkable closed-form formulas for certain structured matrices.\n\nOne such famous case is the **Cauchy matrix**. Let $x_1, \\dots, x_n$ and $y_1, \\dots, y_n$ be two sequences of numbers from a field (e.g., the real or complex numbers). An $n \\times n$ Cauchy matrix, $C$, is defined by its entries:\n$$\n(C)_{ij} = \\frac{1}{x_i + y_j}\n$$\nFor this definition to be valid, we must assume that $x_i + y_j \\neq 0$ for all pairs of indices $1 \\le i, j \\le n$.\n\nFor example, the $2 \\times 2$ Cauchy matrix is:\n$$\nC_2 = \\begin{pmatrix}\n\\frac{1}{x_1+y_1} & \\frac{1}{x_1+y_2} \\\\\n\\frac{1}{x_2+y_1} & \\frac{1}{x_2+y_2}\n\\end{pmatrix}\n$$\nIts determinant is $\\frac{1}{(x_1+y_1)(x_2+y_2)} - \\frac{1}{(x_1+y_2)(x_2+y_1)} = \\frac{(x_2-x_1)(y_2-y_1)}{(x_1+y_1)(x_1+y_2)(x_2+y_1)(x_2+y_2)}$. This suggests a general pattern.\n\n**Goal**\n\nProve **Cauchy's determinant identity**. For an $n \\times n$ Cauchy matrix $C$ as defined above, prove that its determinant is given by the following formula:\n$$\n\\det(C) = \\frac{\\prod_{1 \\le i < j \\le n} (x_j - x_i)(y_j - y_i)}{\\prod_{i=1}^n \\prod_{j=1}^n (x_i + y_j)}\n$$\nFor the formula to be non-singular, you may assume that the elements $x_1, \\dots, x_n$ are distinct and the elements $y_1, \\dots, y_n$ are also distinct.",
        "answer": "The proof proceeds by induction on $n$. However, a direct inductive argument requires establishing a recurrence relation for the determinant. We will derive this recurrence by performing a sequence of row and column operations.\n\nLet $D_n$ be the determinant of the $n \\times n$ Cauchy matrix.\n$$\nD_n = \\det\\left(\\frac{1}{x_i + y_j}\\right)_{1 \\le i,j \\le n}\n$$\n\n**Base Case: n = 1**\nFor $n=1$, the matrix is just $\\left(\\frac{1}{x_1+y_1}\\right)$. The determinant is $D_1 = \\frac{1}{x_1+y_1}$.\nThe formula gives $\\frac{\\text{(empty product)}}{\\prod_{i=1}^1\\prod_{j=1}^1(x_i+y_j)} = \\frac{1}{x_1+y_1}$. The formula holds for $n=1$.\n\n**Inductive Step**\n\nThe core of the proof is to relate $D_n$ to $D_{n-1}$. We achieve this by manipulating the matrix to isolate a block corresponding to the $(n-1) \\times (n-1)$ Cauchy matrix.\n\n**Step 1: Manipulate columns**\n\nSubtract the last column (column $n$) from every other column $j$ for $j=1, \\dots, n-1$. The determinant remains unchanged. The new entries for $j < n$ become:\n$$\n(C')_{ij} = \\frac{1}{x_i + y_j} - \\frac{1}{x_i + y_n} = \\frac{(x_i + y_n) - (x_i + y_j)}{(x_i + y_j)(x_i + y_n)} = \\frac{y_n - y_j}{(x_i + y_j)(x_i + y_n)}\n$$\nThe last column remains $(C)_{in} = \\frac{1}{x_i + y_n}$.\n\nNow, for each column $j=1, \\dots, n-1$, we can factor out the term $(y_n - y_j)$, which is constant with respect to the row index $i$.\n$$\nD_n = \\left(\\prod_{j=1}^{n-1} (y_n - y_j)\\right) \\det(C'')\n$$\nwhere the entries of matrix $C''$ are:\n$$\n(C'')_{ij} = \\frac{1}{(x_i + y_j)(x_i + y_n)} \\quad \\text{for } j < n, \\quad \\text{and} \\quad (C'')_{in} = \\frac{1}{x_i + y_n}\n$$\n\n**Step 2: Manipulate rows**\n\nIn matrix $C''$, we can factor out the term $\\frac{1}{x_i + y_n}$ from each row $i$.\n$$\nD_n = \\left(\\prod_{j=1}^{n-1} (y_n - y_j)\\right) \\left(\\prod_{i=1}^{n} \\frac{1}{x_i + y_n}\\right) \\det(C''')\n$$\nwhere the entries of matrix $C'''$ are:\n$$\n(C''')_{ij} = \\frac{1}{x_i + y_j} \\quad \\text{for } j < n, \\quad \\text{and} \\quad (C''')_{in} = 1\n$$\nThe matrix $C'''$ has a very convenient structure: its last column consists entirely of 1s. We can use this to simplify the matrix further. Subtract the last row (row $n$) from every other row $i$ for $i=1, \\dots, n-1$. This does not change the determinant. The new entries for $i < n$ become:\n$$\n(C'''')_{ij} = (C''')_{ij} - (C''')_{nj} = \\frac{1}{x_i + y_j} - \\frac{1}{x_n + y_j} = \\frac{x_n - x_i}{(x_i + y_j)(x_n + y_j)} \\quad \\text{for } j < n\n$$\n$$\n(C'''')_{in} = (C''')_{in} - (C''')_{nn} = 1 - 1 = 0 \\quad \\text{for } i < n\n$$\nThe last row remains unchanged. The resulting matrix $C''''$ is:\n$$\nC'''' = \\begin{pmatrix}\n\\frac{x_n-x_1}{(x_1+y_1)(x_n+y_1)} & \\cdots & \\frac{x_n-x_1}{(x_1+y_{n-1})(x_n+y_{n-1})} & 0 \\\\\n\\vdots & \\ddots & \\vdots & \\vdots \\\\\n\\frac{x_n-x_{n-1}}{(x_{n-1}+y_1)(x_n+y_1)} & \\cdots & \\frac{x_n-x_{n-1}}{(x_{n-1}+y_{n-1})(x_n+y_{n-1})} & 0 \\\\\n\\frac{1}{x_n+y_1} & \\cdots & \\frac{1}{x_n+y_{n-1}} & 1\n\\end{pmatrix}\n$$\nThe determinant of this matrix can be computed by cofactor expansion along the last column. The only non-zero term comes from the entry $(n,n)$, which is 1. Thus, $\\det(C'''') = \\det(M)$, where $M$ is the $(n-1) \\times (n-1)$ submatrix in the top-left corner.\n$$\n\\det(M) = \\det\\left(\\frac{x_n-x_i}{(x_i+y_j)(x_n+y_j)}\\right)_{1 \\le i,j \\le n-1}\n$$\nNow we factor out terms from this smaller determinant. From each row $i$ (for $i < n$), we factor out $(x_n-x_i)$. From each column $j$ (for $j < n$), we factor out $\\frac{1}{x_n+y_j}$.\n$$\n\\det(M) = \\left(\\prod_{i=1}^{n-1} (x_n-x_i)\\right) \\left(\\prod_{j=1}^{n-1} \\frac{1}{x_n+y_j}\\right) \\det\\left(\\frac{1}{x_i+y_j}\\right)_{1 \\le i,j \\le n-1}\n$$\nThe remaining determinant is exactly $D_{n-1}$.\n\n**Step 3: Assemble the recurrence relation**\n\nCombining all the factors, we get:\n$$\nD_n = \\left(\\prod_{j=1}^{n-1} (y_n - y_j)\\right) \\left(\\prod_{i=1}^{n} \\frac{1}{x_i + y_n}\\right) \\left(\\prod_{i=1}^{n-1} (x_n - x_i)\\right) \\left(\\prod_{j=1}^{n-1} \\frac{1}{x_n+y_j}\\right) D_{n-1}\n$$\nRearranging the terms gives the recurrence relation:\n$$\nD_n = D_{n-1} \\cdot \\frac{\\left(\\prod_{i=1}^{n-1} (x_n - x_i)\\right) \\left(\\prod_{j=1}^{n-1} (y_n - y_j)\\right)}{\\left(\\prod_{i=1}^{n} (x_i + y_n)\\right) \\left(\\prod_{j=1}^{n-1} (x_n + y_j)\\right)}\n$$\nSwitching the sign of the factors $(y_n-y_j)$ to $(y_j-y_n)$ introduces a factor of $(-1)^{n-1}$. Likewise for $(x_n-x_i)$.\n$$\nD_n = D_{n-1} \\cdot \\frac{(-1)^{n-1}\\left(\\prod_{i=1}^{n-1} (x_i - x_n)\\right) (-1)^{n-1}\\left(\\prod_{j=1}^{n-1} (y_j - y_n)\\right)}{\\prod_{j=1}^n (x_n+y_j) \\prod_{i=1}^{n-1} (x_i+y_n)}\n$$\n$$\nD_n = D_{n-1} \\cdot \\frac{\\left(\\prod_{i=1}^{n-1} (x_i - x_n)\\right) \\left(\\prod_{j=1}^{n-1} (y_j - y_n)\\right)}{\\prod_{j=1}^n (x_n+y_j) \\prod_{i=1}^{n-1} (x_i+y_n)}\n$$\nThis recurrence relates $D_n$ to $D_{n-1}$. By applying it repeatedly, we can establish the general formula.\n\nLet $F(n)$ be the formula we want to prove: $F(n) = \\frac{\\prod_{1 \\le i < j \\le n} (x_j - x_i)(y_j - y_i)}{\\prod_{i,j=1}^n (x_i + y_j)}$.\nLet's check if $F(n)$ satisfies the same recurrence:\n$$\n\\frac{F(n)}{F(n-1)} = \\frac{\\frac{\\prod_{1 \\le i < j \\le n} (x_j - x_i)(y_j - y_i)}{\\prod_{i,j=1}^n (x_i + y_j)}}{\\frac{\\prod_{1 \\le i < j \\le n-1} (x_j - x_i)(y_j - y_i)}{\\prod_{i,j=1}^{n-1} (x_i + y_j)}}\n$$\nThe numerator ratio is:\n$$\n\\frac{\\prod_{1 \\le i < j \\le n} (x_j - x_i)}{\\prod_{1 \\le i < j \\le n-1} (x_j - x_i)} = \\prod_{i=1}^{n-1} (x_n - x_i)\n$$\nAnd similarly for the $y$ terms.\nThe denominator ratio is:\n$$\n\\frac{\\prod_{i,j=1}^n (x_i + y_j)}{\\prod_{i,j=1}^{n-1} (x_i + y_j)} = \\left(\\prod_{j=1}^n (x_n + y_j)\\right) \\left(\\prod_{i=1}^{n-1} (x_i + y_n)\\right)\n$$\nCombining these gives:\n$$\n\\frac{F(n)}{F(n-1)} = \\frac{\\left(\\prod_{i=1}^{n-1} (x_n - x_i)\\right) \\left(\\prod_{j=1}^{n-1} (y_n - y_j)\\right)}{\\left(\\prod_{j=1}^n (x_n+y_j)\\right) \\left(\\prod_{i=1}^{n-1} (x_i+y_n)\\right)}\n$$\nThis matches the recurrence relation derived for $D_n$. Since $D_n$ and $F(n)$ satisfy the same recurrence relation and have the same initial value $D_1 = F(1)$, we conclude that $D_n = F(n)$ for all $n \\ge 1$.\n\nThis completes the proof of Cauchy's determinant identity."
    }
]